# HoppingRabbit AI æŠ€æœ¯è®¾è®¡æ–‡æ¡£
## ä¸“ä¸šçº§è§†é¢‘ç¼–è¾‘å¹³å°å®Œæ•´æŠ€æœ¯æ–¹æ¡ˆ

> **Version**: 1.0.0  
> **Author**: æŠ€æœ¯æ¶æ„å›¢é˜Ÿ  
> **Last Updated**: 2026å¹´1æœˆ6æ—¥  
> **Status**: ğŸš§ è®¾è®¡é˜¶æ®µ

---

## ğŸ“‹ æ–‡æ¡£ç›®å½•

### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•´ä½“æ¶æ„è®¾è®¡
1. [ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ](#1-ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ)
2. [æ ¸å¿ƒè®¾è®¡ç†å¿µ](#2-æ ¸å¿ƒè®¾è®¡ç†å¿µ)
3. [æŠ€æœ¯æ ˆé€‰å‹](#3-æŠ€æœ¯æ ˆé€‰å‹)
4. [æ•°æ®æµè®¾è®¡](#4-æ•°æ®æµè®¾è®¡)

### ç¬¬äºŒéƒ¨åˆ†ï¼šåç«¯ API è®¾è®¡
5. [API æ¥å£å…¨æ™¯å›¾](#5-api-æ¥å£å…¨æ™¯å›¾)
6. [é¡¹ç›®ç®¡ç†æ¨¡å—](#6-é¡¹ç›®ç®¡ç†æ¨¡å—)
7. [èµ„æºç®¡ç†æ¨¡å—](#7-èµ„æºç®¡ç†æ¨¡å—)
8. [AI å¤„ç†æ¨¡å—](#8-ai-å¤„ç†æ¨¡å—)
9. [å¯¼å‡ºæ¸²æŸ“æ¨¡å—](#9-å¯¼å‡ºæ¸²æŸ“æ¨¡å—)

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•°æ®åº“ä¸å­˜å‚¨
10. [æ•°æ®åº“è¡¨ç»“æ„è®¾è®¡](#10-æ•°æ®åº“è¡¨ç»“æ„è®¾è®¡)
11. [ç´¢å¼•ä¸æŸ¥è¯¢ä¼˜åŒ–](#11-ç´¢å¼•ä¸æŸ¥è¯¢ä¼˜åŒ–)
12. [å¯¹è±¡å­˜å‚¨ç­–ç•¥](#12-å¯¹è±¡å­˜å‚¨ç­–ç•¥)

### ç¬¬å››éƒ¨åˆ†ï¼šå‰ç«¯çŠ¶æ€ç®¡ç†
13. [å‰ç«¯ Store æ¶æ„](#13-å‰ç«¯-store-æ¶æ„)
14. [API è°ƒç”¨ç‚¹æ˜ å°„](#14-api-è°ƒç”¨ç‚¹æ˜ å°„)
15. [æ¯«ç§’çº§ä¿å­˜æœºåˆ¶](#15-æ¯«ç§’çº§ä¿å­˜æœºåˆ¶)
16. [ç¦»çº¿å®¹ç¾æ–¹æ¡ˆ](#16-ç¦»çº¿å®¹ç¾æ–¹æ¡ˆ)

### ç¬¬äº”éƒ¨åˆ†ï¼šAI æŠ€æœ¯å®ç°
17. [ASR è¯­éŸ³è¯†åˆ«](#17-asr-è¯­éŸ³è¯†åˆ«)
18. [äººå£°ä¼´å¥åˆ†ç¦»](#18-äººå£°ä¼´å¥åˆ†ç¦»)
19. [è¯´è¯äººåˆ†ç¦» (Diarization)](#19-è¯´è¯äººåˆ†ç¦»-diarization)
20. [æ™ºèƒ½å‰ªè¾‘ç®—æ³•](#20-æ™ºèƒ½å‰ªè¾‘ç®—æ³•)

### ç¬¬å…­éƒ¨åˆ†ï¼šæ€§èƒ½ä¼˜åŒ–
21. [ç¼“å­˜ç­–ç•¥è®¾è®¡](#21-ç¼“å­˜ç­–ç•¥è®¾è®¡)
22. [è§†é¢‘ä»£ç†ä¸é¢„è§ˆ](#22-è§†é¢‘ä»£ç†ä¸é¢„è§ˆ)
23. [éŸ³é¢‘æ³¢å½¢ç”Ÿæˆ](#23-éŸ³é¢‘æ³¢å½¢ç”Ÿæˆ)
24. [CDN ä¸åŠ é€Ÿ](#24-cdn-ä¸åŠ é€Ÿ)

### ç¬¬ä¸ƒéƒ¨åˆ†ï¼šéƒ¨ç½²ä¸è¿ç»´
25. [Docker å®¹å™¨åŒ–](#25-docker-å®¹å™¨åŒ–)
26. [Celery ä»»åŠ¡é˜Ÿåˆ—](#26-celery-ä»»åŠ¡é˜Ÿåˆ—)
27. [ç›‘æ§ä¸å‘Šè­¦](#27-ç›‘æ§ä¸å‘Šè­¦)
28. [æ‰©å±•æ€§è®¾è®¡](#28-æ‰©å±•æ€§è®¾è®¡)

### é™„å½•
- [A. é”™è¯¯ç è§„èŒƒ](#é™„å½•a-é”™è¯¯ç è§„èŒƒ)
- [B. WebSocket æ¶ˆæ¯åè®®](#é™„å½•b-websocket-æ¶ˆæ¯åè®®)
- [C. Timeline JSON Schema](#é™„å½•c-timeline-json-schema)
- [D. æ€§èƒ½æŒ‡æ ‡ SLA](#é™„å½•d-æ€§èƒ½æŒ‡æ ‡-sla)

---

## 1. ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### 1.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·æµè§ˆå™¨ (Browser)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Next.js App  â”‚   â”‚ IndexedDB    â”‚   â”‚ LocalStorage (Fallback)    â”‚ â”‚
â”‚  â”‚ (React 18)   â”‚   â”‚ (Timeline)   â”‚   â”‚ (Settings)                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ HTTPS / WebSocket
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Gateway / Load Balancer                        â”‚
â”‚                         (Nginx / Cloudflare)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼              â–¼                â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI    â”‚  â”‚  WebSocket   â”‚  â”‚  Supabase    â”‚  â”‚  Redis       â”‚
â”‚  (Backend)  â”‚  â”‚  Server      â”‚  â”‚  Storage     â”‚  â”‚  (Cache)     â”‚
â”‚  8000       â”‚  â”‚  8001        â”‚  â”‚  (S3-like)   â”‚  â”‚  6379        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Dispatch Tasks
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Celery Task Queue (RabbitMQ)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
          â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CPU Workers    â”‚    â”‚  GPU Workers (CUDA / M-Series Mac)              â”‚
â”‚  - Export       â”‚    â”‚  - Whisper (ASR)                                â”‚
â”‚  - Waveform     â”‚    â”‚  - Demucs (Stem Separation)                     â”‚
â”‚  - Thumbnail    â”‚    â”‚  - Pyannote (Speaker Diarization)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Supabase Postgres  â”‚
          â”‚  (Metadata DB)      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æŠ€æœ¯åˆ†å±‚

| å±‚çº§ | èŒè´£ | å…³é”®æŠ€æœ¯ | å“åº”æ—¶é—´ç›®æ ‡ |
|------|------|----------|-------------|
| **å‘ˆç°å±‚** | UI æ¸²æŸ“ã€æœ¬åœ°çŠ¶æ€ç®¡ç† | Next.js 14, Zustand, Canvas API | < 16ms (60 FPS) |
| **æ¥å£å±‚** | API ç½‘å…³ã€é‰´æƒã€é™æµ | FastAPI, JWT, Rate Limiter | < 100ms (P95) |
| **ä¸šåŠ¡å±‚** | é¡¹ç›®é€»è¾‘ã€ä»»åŠ¡è°ƒåº¦ | Pydantic, Celery | < 200ms (P95) |
| **æ•°æ®å±‚** | æŒä¹…åŒ–å­˜å‚¨ | Supabase Postgres, Redis | < 50ms (P99) |
| **è®¡ç®—å±‚** | AI æ¨ç†ã€è§†é¢‘æ¸²æŸ“ | Whisper, Demucs, FFmpeg | å¼‚æ­¥ï¼ˆç§’çº§åˆ°åˆ†é’Ÿçº§ï¼‰ |

---

## 2. æ ¸å¿ƒè®¾è®¡ç†å¿µ

### 2.1 ç±» CapCut ä½“éªŒçš„å››å¤§æ”¯æŸ±

#### âœ… 1. ä¹è§‚ UI (Optimistic UI)
**åŸåˆ™**: æ‰€æœ‰ç”¨æˆ·æ“ä½œç«‹å³åœ¨å‰ç«¯ç”Ÿæ•ˆï¼Œä¸ç­‰å¾…åç«¯å“åº”ã€‚

**å®ç°æ–¹å¼**:
- ç”¨æˆ·æ‹–åŠ¨ç‰‡æ®µæ—¶ï¼Œç«‹å³æ›´æ–° Zustand Store ä¸­çš„ `clip.start` å±æ€§ã€‚
- åŒæ—¶å‘èµ· API è¯·æ±‚åˆ° `/api/projects/save`ï¼ˆå¸¦é˜²æŠ–ï¼‰ã€‚
- å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œæ˜¾ç¤º"ç¦»çº¿æ¨¡å¼"æ ‡è¯†ï¼Œæ•°æ®å­˜å…¥ IndexedDB å¾…åŒæ­¥é˜Ÿåˆ—ã€‚

**ä»£ç ç¤ºä¾‹**:
```typescript
// å‰ç«¯ï¼šä¹è§‚æ›´æ–°
const moveClip = (clipId: string, newStart: number) => {
  // 1. ç«‹å³æ›´æ–° UIï¼ˆ0ms å»¶è¿Ÿï¼‰
  set(state => ({
    clips: state.clips.map(c => 
      c.id === clipId ? { ...c, start: newStart } : c
    )
  }));
  
  // 2. å¼‚æ­¥åŒæ­¥åˆ°åç«¯ï¼ˆé˜²æŠ– 3 ç§’ï¼‰
  debouncedSaveToBackend();
};
```

#### âœ… 2. å¢é‡åŒæ­¥ (Incremental Sync)
**é—®é¢˜**: å®Œæ•´çš„é¡¹ç›® JSON å¯èƒ½è¾¾åˆ° 10MBï¼ˆåŒ…å«æ•°åƒä¸ªç‰‡æ®µçš„å…ƒæ•°æ®ï¼‰ã€‚

**æ–¹æ¡ˆ**: ä½¿ç”¨ **Operation Log** (ç±»ä¼¼ Git Commits) è®°å½•ç”¨æˆ·æ“ä½œï¼Œåªä¼ è¾“å¢é‡å˜æ›´ã€‚

**æ•°æ®ç»“æ„**:
```typescript
interface Operation {
  id: string;
  type: 'UPDATE_CLIP' | 'DELETE_CLIP' | 'ADD_TRACK' | ...;
  timestamp: number;
  payload: any;
  appliedToServer: boolean;
}
```

#### âœ… 3. å¤šçº§ç¼“å­˜ (Multi-Tier Cache)
```
User Action
   â†“
Memory (Zustand) â”€â”€â”€â”€â”
   â†“                 â”‚ å®¹ç¾å¤‡ä»½
IndexedDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
Backend API
   â†“
PostgreSQL
```

#### âœ… 4. æ¸è¿›å¼å¢å¼º (Progressive Enhancement)
- **ç½‘ç»œè‰¯å¥½**: å®æ—¶åŒæ­¥ + WebSocket é€šçŸ¥ã€‚
- **ç½‘ç»œä¸ç¨³å®š**: è‡ªåŠ¨é™çº§ä¸ºè½®è¯¢æ¨¡å¼ã€‚
- **å®Œå…¨ç¦»çº¿**: æ‰€æœ‰æ“ä½œå­˜å…¥æœ¬åœ°ï¼Œè”ç½‘åæ‰¹é‡ä¸Šä¼ ã€‚

### 2.2 æ€§èƒ½ç›®æ ‡ (Performance SLA)

| æ“ä½œ | ç›®æ ‡å»¶è¿Ÿ | æµ‹é‡æ–¹å¼ |
|------|---------|---------|
| æ‹–åŠ¨ç‰‡æ®µ | < 16ms | `requestAnimationFrame` å¸§ç‡ |
| åˆ‡å‰²è§†é¢‘ | < 50ms | æ“ä½œåˆ° UI æ›´æ–° |
| æ’­æ”¾/æš‚åœ | < 100ms | ç‚¹å‡»åˆ°çŠ¶æ€å˜åŒ– |
| åŠ è½½é¡¹ç›® | < 2s | é¦–å±å¯äº¤äº’æ—¶é—´ (TTI) |
| ASR è½¬å†™ (1åˆ†é’Ÿè§†é¢‘) | < 30s | Celery ä»»åŠ¡å®Œæˆæ—¶é—´ |
| å¯¼å‡ºè§†é¢‘ (1080p, 5åˆ†é’Ÿ) | < 3min | åå°ä»»åŠ¡å®Œæˆæ—¶é—´ |

---

## 3. æŠ€æœ¯æ ˆé€‰å‹

### 3.1 å‰ç«¯æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | ç†ç”± |
|------|---------|------|
| **æ¡†æ¶** | Next.js 14 (App Router) | SSR + é™æ€ä¼˜åŒ–ï¼ŒSEO å‹å¥½ |
| **çŠ¶æ€ç®¡ç†** | Zustand | è½»é‡çº§ï¼Œæ€§èƒ½ä¼˜äº Redux |
| **è§†é¢‘æ’­æ”¾** | Video.js / Custom HTMLVideoElement | ç²¾ç¡®æ§åˆ¶æ’­æ”¾è¿›åº¦ |
| **æ—¶é—´è½´æ¸²æŸ“** | Canvas API + Konva.js | é«˜æ€§èƒ½ç»˜åˆ¶ï¼Œæ”¯æŒæ•°ä¸‡å…ƒç´  |
| **æ‹–æ‹½** | react-dnd / @dnd-kit | è§¦æ‘¸å±å…¼å®¹ï¼Œå¯è®¿é—®æ€§ |
| **æœ¬åœ°å­˜å‚¨** | Dexie.js (IndexedDB å°è£…) | å­˜å‚¨å¤§å‹ JSON å¯¹è±¡ |
| **ç½‘ç»œè¯·æ±‚** | Axios + SWR | è‡ªåŠ¨é‡è¯• + ç¼“å­˜ |

### 3.2 åç«¯æŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | ç†ç”± |
|------|---------|------|
| **Web æ¡†æ¶** | FastAPI 0.110+ | åŸç”Ÿå¼‚æ­¥ï¼Œè‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆ |
| **ä»»åŠ¡é˜Ÿåˆ—** | Celery 5.3 + RabbitMQ | æˆç†Ÿçš„åˆ†å¸ƒå¼ä»»åŠ¡ç³»ç»Ÿ |
| **æ•°æ®åº“** | Supabase Postgres 15 | å¼€æºï¼Œå®æ—¶è®¢é˜…ï¼ŒRLS |
| **ç¼“å­˜** | Redis 7.2 | é«˜æ€§èƒ½ï¼Œæ”¯æŒå¤æ‚æ•°æ®ç»“æ„ |
| **å¯¹è±¡å­˜å‚¨** | Supabase Storage (å…¼å®¹ S3) | å…è´¹é¢åº¦ï¼ŒCDN åŠ é€Ÿ |
| **ASR** | faster-whisper (Large V3) | æ¯” OpenAI Whisper å¿« 4x |
| **éŸ³é¢‘åˆ†ç¦»** | Demucs Hybrid Transformer | SOTA äººå£°åˆ†ç¦»è´¨é‡ |
| **è¯´è¯äººåˆ†ç¦»** | pyannote.audio 3.1 | å·¥ä¸šçº§ Diarization |
| **è§†é¢‘å¤„ç†** | FFmpeg 6.1 | è¡Œä¸šæ ‡å‡† |

### 3.3 åŸºç¡€è®¾æ–½

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | ç†ç”± |
|------|---------|------|
| **å®¹å™¨åŒ–** | Docker + Docker Compose | æœ¬åœ°å¼€å‘ç¯å¢ƒä¸€è‡´æ€§ |
| **åå‘ä»£ç†** | Nginx / Caddy | è‡ªåŠ¨ HTTPSï¼Œé™æµ |
| **ç›‘æ§** | Prometheus + Grafana | å¼€æºï¼Œå¯è§†åŒ– |
| **æ—¥å¿—** | Loki + Promtail | ä¸ Grafana é›†æˆ |
| **CI/CD** | GitHub Actions | å…è´¹ï¼Œæ˜“é…ç½® |

---

## 4. æ•°æ®æµè®¾è®¡

### 4.1 ç”¨æˆ·ä¸Šä¼ è§†é¢‘æµç¨‹

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant F as å‰ç«¯
    participant API as FastAPI
    participant S3 as Supabase Storage
    participant MQ as RabbitMQ
    participant W as GPU Worker

    U->>F: é€‰æ‹©è§†é¢‘æ–‡ä»¶
    F->>API: POST /api/upload (æ–‡ä»¶å…ƒä¿¡æ¯)
    API->>F: è¿”å›é¢„ç­¾å URL
    F->>S3: ç›´ä¼ è§†é¢‘æ–‡ä»¶ (é¿å…å ç”¨åç«¯å¸¦å®½)
    S3->>F: ä¸Šä¼ å®Œæˆ
    F->>API: POST /api/projects/create (video_url)
    API->>MQ: å‘å¸ƒ transcribe_video ä»»åŠ¡
    API->>F: è¿”å› project_id + task_id
    F->>U: æ˜¾ç¤º "è½¬å†™ä¸­..." è¿›åº¦æ¡

    loop æ¯2ç§’è½®è¯¢
        F->>API: GET /api/tasks/{task_id}
        API->>F: { progress: 45% }
    end

    W->>API: ä»»åŠ¡å®Œæˆï¼Œæ›´æ–° DB
    API->>F: WebSocket æ¨é€å®Œæˆäº‹ä»¶
    F->>U: æ˜¾ç¤º "è½¬å†™å®Œæˆï¼ŒåŠ è½½ç¼–è¾‘å™¨"
```

### 4.2 æ¯«ç§’çº§è‡ªåŠ¨ä¿å­˜æµç¨‹

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant Z as Zustand Store
    participant IDB as IndexedDB
    participant D as Debouncer
    participant API as Backend API

    U->>Z: æ‹–åŠ¨ç‰‡æ®µ
    Z->>Z: æ›´æ–°å†…å­˜çŠ¶æ€ (0ms)
    Z->>IDB: å†™å…¥æœ¬åœ°å¤‡ä»½ (5ms)
    Z->>D: è§¦å‘é˜²æŠ–å®šæ—¶å™¨

    Note over D: ç­‰å¾… 3 ç§’æ— æ–°æ“ä½œ

    D->>API: PATCH /api/projects/{id}/state
    alt ç½‘ç»œæ­£å¸¸
        API->>D: 200 OK, version: 42
        D->>IDB: æ ‡è®°å·²åŒæ­¥
    else ç½‘ç»œå¤±è´¥
        API->>D: 503 Service Unavailable
        D->>IDB: åŠ å…¥å¾…åŒæ­¥é˜Ÿåˆ—
        D->>U: æ˜¾ç¤º "ç¦»çº¿æ¨¡å¼"
    end
```

---

## 5. API æ¥å£å…¨æ™¯å›¾

### 5.1 æ¥å£åˆ†ç±»æ€»è§ˆ

```
HoppingRabbit AI Backend API
â”œâ”€â”€ /api/auth          # ğŸ” è®¤è¯é‰´æƒï¼ˆé¢„ç•™ï¼‰
â”‚   â”œâ”€â”€ POST   /register
â”‚   â”œâ”€â”€ POST   /login
â”‚   â””â”€â”€ POST   /logout
â”‚
â”œâ”€â”€ /api/projects      # ğŸ“ é¡¹ç›®ç®¡ç†
â”‚   â”œâ”€â”€ GET    /                      # è·å–é¡¹ç›®åˆ—è¡¨
â”‚   â”œâ”€â”€ POST   /                      # åˆ›å»ºæ–°é¡¹ç›®
â”‚   â”œâ”€â”€ GET    /{project_id}          # è·å–é¡¹ç›®è¯¦æƒ…
â”‚   â”œâ”€â”€ PATCH  /{project_id}          # æ›´æ–°é¡¹ç›®å…ƒä¿¡æ¯
â”‚   â”œâ”€â”€ DELETE /{project_id}          # åˆ é™¤é¡¹ç›®
â”‚   â”œâ”€â”€ PATCH  /{project_id}/state    # ä¿å­˜æ—¶é—´è½´çŠ¶æ€ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â””â”€â”€ GET    /{project_id}/history  # è·å–å†å²ç‰ˆæœ¬
â”‚
â”œâ”€â”€ /api/assets        # ğŸ¬ èµ„æºç®¡ç†
â”‚   â”œâ”€â”€ GET    /                      # è·å–èµ„æºåˆ—è¡¨
â”‚   â”œâ”€â”€ POST   /presign-upload        # è·å–ä¸Šä¼ ç­¾å URL
â”‚   â”œâ”€â”€ POST   /confirm-upload        # ç¡®è®¤ä¸Šä¼ å®Œæˆ
â”‚   â”œâ”€â”€ DELETE /{asset_id}            # åˆ é™¤èµ„æº
â”‚   â””â”€â”€ GET    /{asset_id}/waveform   # è·å–éŸ³é¢‘æ³¢å½¢æ•°æ®
â”‚
â”œâ”€â”€ /api/tasks         # âš™ï¸ AI è®¡ç®—ä»»åŠ¡
â”‚   â”œâ”€â”€ POST   /asr                   # è§¦å‘ ASR è½¬å†™
â”‚   â”œâ”€â”€ POST   /stem-separation       # è§¦å‘äººå£°åˆ†ç¦»
â”‚   â”œâ”€â”€ POST   /speaker-diarization   # è§¦å‘è¯´è¯äººåˆ†ç¦»
â”‚   â”œâ”€â”€ POST   /auto-caption          # è§¦å‘è‡ªåŠ¨å­—å¹•
â”‚   â”œâ”€â”€ GET    /{task_id}             # æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
â”‚   â””â”€â”€ DELETE /{task_id}             # å–æ¶ˆä»»åŠ¡
â”‚
â”œâ”€â”€ /api/export        # ğŸ¥ å¯¼å‡ºæ¸²æŸ“
â”‚   â”œâ”€â”€ POST   /                      # æäº¤å¯¼å‡ºä»»åŠ¡
â”‚   â”œâ”€â”€ GET    /{export_id}           # æŸ¥è¯¢å¯¼å‡ºè¿›åº¦
â”‚   â””â”€â”€ GET    /{export_id}/download  # ä¸‹è½½å¯¼å‡ºè§†é¢‘
â”‚
â””â”€â”€ /api/system        # ğŸ› ï¸ ç³»ç»Ÿå·¥å…·
    â”œâ”€â”€ GET    /health                # å¥åº·æ£€æŸ¥
    â”œâ”€â”€ GET    /fonts                 # è·å–å­—ä½“åˆ—è¡¨
    â””â”€â”€ GET    /presets               # è·å–å¯¼å‡ºé¢„è®¾
```

### 5.2 æ ¸å¿ƒæ¥å£è®¾è®¡ç»†èŠ‚

---

## 6. é¡¹ç›®ç®¡ç†æ¨¡å—

### 6.1 åˆ›å»ºé¡¹ç›®

**ç«¯ç‚¹**: `POST /api/projects`

**è¯·æ±‚ä½“**:
```json
{
  "name": "æˆ‘çš„ç¬¬ä¸€ä¸ªé¡¹ç›®",
  "video_asset_id": "uuid-of-uploaded-video",
  "settings": {
    "resolution": { "width": 1920, "height": 1080 },
    "fps": 30,
    "sample_rate": 48000
  }
}
```

**å“åº”ä½“** (201 Created):
```json
{
  "id": "proj_a1b2c3d4e5f6",
  "name": "æˆ‘çš„ç¬¬ä¸€ä¸ªé¡¹ç›®",
  "status": "initializing",
  "created_at": "2026-01-06T10:30:00Z",
  "assets": [
    {
      "id": "asset_video_001",
      "type": "video",
      "url": "https://storage.supabase.co/videos/raw/proj_xxx.mp4",
      "proxy_url": "https://storage.supabase.co/videos/proxy/proj_xxx_720p.mp4",
      "metadata": {
        "duration": 125.5,
        "width": 1920,
        "height": 1080,
        "codec": "h264"
      }
    }
  ],
  "timeline": {
    "tracks": [],
    "clips": [],
    "version": 1
  }
}
```

**ä¸šåŠ¡é€»è¾‘**:
1. éªŒè¯ `video_asset_id` å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·ã€‚
2. åœ¨æ•°æ®åº“åˆ›å»º `projects` è®°å½•ï¼ŒçŠ¶æ€ä¸º `initializing`ã€‚
3. å¼‚æ­¥è§¦å‘ **è§†é¢‘é¢„å¤„ç†ä»»åŠ¡**:
   - ç”Ÿæˆ 720p ä»£ç†è§†é¢‘ï¼ˆç”¨äºç¼–è¾‘é¢„è§ˆï¼‰ã€‚
   - ç”Ÿæˆé›ªç¢§å›¾ï¼ˆtimeline ç¼©ç•¥å›¾ï¼‰ã€‚
   - æå–éŸ³é¢‘å¹¶ç”Ÿæˆæ³¢å½¢æ•°æ®ã€‚
4. è¿”å›é¡¹ç›® IDï¼Œå‰ç«¯å¯ç«‹å³è·³è½¬åˆ°ç¼–è¾‘å™¨é¡µé¢ã€‚

---

### 6.2 ä¿å­˜é¡¹ç›®çŠ¶æ€ï¼ˆæ ¸å¿ƒï¼‰

**ç«¯ç‚¹**: `PATCH /api/projects/{project_id}/state`

**è®¾è®¡å“²å­¦**: 
- æ”¯æŒ**å¢é‡åŒæ­¥**å’Œ**å…¨é‡åŒæ­¥**ä¸¤ç§æ¨¡å¼ã€‚
- ä½¿ç”¨ç‰ˆæœ¬å· (Optimistic Concurrency Control) é¿å…å†²çªã€‚

**è¯·æ±‚ä½“ï¼ˆå¢é‡æ¨¡å¼ï¼‰**:
```json
{
  "version": 42,  // å®¢æˆ·ç«¯å½“å‰ç‰ˆæœ¬å·
  "operations": [
    {
      "type": "UPDATE_CLIP",
      "timestamp": 1704524400000,
      "payload": {
        "clip_id": "clip_001",
        "start": 5.2,
        "duration": 3.5
      }
    },
    {
      "type": "DELETE_TRACK",
      "timestamp": 1704524401000,
      "payload": {
        "track_id": "track_003"
      }
    }
  ]
}
```

**è¯·æ±‚ä½“ï¼ˆå…¨é‡æ¨¡å¼ï¼‰**:
```json
{
  "version": 42,
  "timeline": {
    "tracks": [ /* å®Œæ•´ tracks æ•°ç»„ */ ],
    "clips": [ /* å®Œæ•´ clips æ•°ç»„ */ ],
    "effects": [ /* ç‰¹æ•ˆé…ç½® */ ]
  }
}
```

**å“åº”ä½“** (200 OK):
```json
{
  "success": true,
  "new_version": 43,
  "saved_at": "2026-01-06T10:35:12Z"
}
```

**å†²çªå¤„ç†** (409 Conflict):
```json
{
  "error": "version_conflict",
  "message": "æœåŠ¡å™¨ç‰ˆæœ¬ä¸º 45ï¼Œå®¢æˆ·ç«¯ç‰ˆæœ¬ä¸º 42",
  "server_version": 45,
  "suggestion": "è¯·åˆ·æ–°é¡µé¢é‡æ–°åŠ è½½æœ€æ–°çŠ¶æ€"
}
```

**åç«¯å®ç°ä¼ªä»£ç **:
```python
@router.patch("/projects/{project_id}/state")
async def save_project_state(
    project_id: str,
    request: SaveStateRequest,
    db: Database = Depends(get_db)
):
    # 1. æ£€æŸ¥ç‰ˆæœ¬å†²çª
    current = await db.fetch_one(
        "SELECT version FROM projects WHERE id = $1", project_id
    )
    if current["version"] != request.version:
        raise HTTPException(409, detail={
            "error": "version_conflict",
            "server_version": current["version"]
        })
    
    # 2. åº”ç”¨æ“ä½œæˆ–ä¿å­˜å…¨é‡çŠ¶æ€
    if request.operations:
        # å¢é‡æ¨¡å¼ï¼šé€ä¸ªåº”ç”¨æ“ä½œ
        for op in request.operations:
            await apply_operation(db, project_id, op)
    else:
        # å…¨é‡æ¨¡å¼ï¼šæ›¿æ¢æ•´ä¸ª timeline
        await db.execute(
            "UPDATE projects SET timeline = $1 WHERE id = $2",
            request.timeline, project_id
        )
    
    # 3. é€’å¢ç‰ˆæœ¬å·
    new_version = request.version + 1
    await db.execute(
        "UPDATE projects SET version = $1, updated_at = NOW() WHERE id = $2",
        new_version, project_id
    )
    
    # 4. ï¼ˆå¯é€‰ï¼‰ä¿å­˜å†å²å¿«ç…§åˆ° timeline_snapshots è¡¨
    if new_version % 10 == 0:  # æ¯ 10 ä¸ªç‰ˆæœ¬ä¿å­˜ä¸€æ¬¡
        await save_snapshot(db, project_id, new_version)
    
    return {"success": True, "new_version": new_version}
```

---

### 6.3 è·å–é¡¹ç›®å†å²ç‰ˆæœ¬

**ç«¯ç‚¹**: `GET /api/projects/{project_id}/history`

**æŸ¥è¯¢å‚æ•°**:
- `limit`: è¿”å›æœ€è¿‘ N ä¸ªç‰ˆæœ¬ï¼ˆé»˜è®¤ 20ï¼‰
- `before_version`: åˆ†é¡µå‚æ•°

**å“åº”ä½“**:
```json
{
  "snapshots": [
    {
      "version": 50,
      "created_at": "2026-01-06T11:00:00Z",
      "description": "è‡ªåŠ¨ä¿å­˜",
      "size_bytes": 1024000
    },
    {
      "version": 40,
      "created_at": "2026-01-06T10:45:00Z",
      "description": "æ‰‹åŠ¨ä¿å­˜æ£€æŸ¥ç‚¹",
      "size_bytes": 980000
    }
  ]
}
```

**ç”¨é€”**: å®ç°"ç‰ˆæœ¬å†å²"åŠŸèƒ½ï¼Œç”¨æˆ·å¯ä»¥å›æ»šåˆ°ä»»æ„ä¿å­˜ç‚¹ã€‚

---

## 7. èµ„æºç®¡ç†æ¨¡å—

### 7.1 è·å–é¢„ç­¾åä¸Šä¼  URL

**ç«¯ç‚¹**: `POST /api/assets/presign-upload`

**ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ¥å£ï¼Ÿ**
- å¤§æ–‡ä»¶ï¼ˆå¦‚ 4K è§†é¢‘ï¼‰ä¸åº”è¯¥ç»è¿‡ FastAPI æœåŠ¡å™¨ã€‚
- ç›´æ¥ä»æµè§ˆå™¨ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨ï¼ˆSupabase Storage / S3ï¼‰ã€‚
- åç«¯åªè´Ÿè´£ç”Ÿæˆå¸¦æƒé™çš„ä¸´æ—¶ URLã€‚

**è¯·æ±‚ä½“**:
```json
{
  "file_name": "my_video.mp4",
  "file_size": 524288000,  // 500 MB
  "content_type": "video/mp4"
}
```

**å“åº”ä½“**:
```json
{
  "asset_id": "asset_abc123",
  "upload_url": "https://storage.supabase.co/v1/upload/videos/temp/abc123.mp4?token=...",
  "expires_in": 3600,  // 1 å°æ—¶åè¿‡æœŸ
  "callback_url": "/api/assets/confirm-upload"
}
```

**å‰ç«¯ä¸Šä¼ æµç¨‹**:
```typescript
// 1. è·å–ç­¾å URL
const { asset_id, upload_url } = await api.post('/assets/presign-upload', {
  file_name: file.name,
  file_size: file.size,
  content_type: file.type
});

// 2. ç›´ä¼ åˆ°å¯¹è±¡å­˜å‚¨
await axios.put(upload_url, file, {
  headers: { 'Content-Type': file.type },
  onUploadProgress: (e) => setProgress(e.loaded / e.total * 100)
});

// 3. é€šçŸ¥åç«¯ä¸Šä¼ å®Œæˆ
await api.post('/assets/confirm-upload', { asset_id });
```

---

### 7.2 ç¡®è®¤ä¸Šä¼ å®Œæˆ

**ç«¯ç‚¹**: `POST /api/assets/confirm-upload`

**è¯·æ±‚ä½“**:
```json
{
  "asset_id": "asset_abc123"
}
```

**åç«¯é€»è¾‘**:
1. éªŒè¯æ–‡ä»¶ç¡®å®å­˜åœ¨äºå¯¹è±¡å­˜å‚¨ä¸­ã€‚
2. ä½¿ç”¨ FFprobe æå–è§†é¢‘å…ƒæ•°æ®ï¼ˆåˆ†è¾¨ç‡ã€æ—¶é•¿ã€ç¼–ç æ ¼å¼ï¼‰ã€‚
3. è§¦å‘åå°ä»»åŠ¡ï¼š
   - ç”Ÿæˆä»£ç†è§†é¢‘ï¼ˆ720p H.264ï¼‰ã€‚
   - ç”Ÿæˆé›ªç¢§å›¾ï¼ˆæ¯ç§’ä¸€å¸§ï¼‰ã€‚
   - æå–éŸ³é¢‘å¹¶ç”Ÿæˆæ³¢å½¢æ•°æ®ã€‚
4. æ›´æ–° `assets` è¡¨ï¼Œè®¾ç½®çŠ¶æ€ä¸º `processing`ã€‚

**å“åº”ä½“**:
```json
{
  "asset_id": "asset_abc123",
  "status": "processing",
  "tasks": [
    { "type": "generate_proxy", "task_id": "task_001" },
    { "type": "extract_audio", "task_id": "task_002" },
    { "type": "generate_waveform", "task_id": "task_003" }
  ]
}
```

---

### 7.3 è·å–éŸ³é¢‘æ³¢å½¢æ•°æ®

**ç«¯ç‚¹**: `GET /api/assets/{asset_id}/waveform`

**æŸ¥è¯¢å‚æ•°**:
- `resolution`: æ¯ç§’é‡‡æ ·ç‚¹æ•°ï¼ˆé»˜è®¤ 100ï¼‰
- `cache`: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ trueï¼‰

**å“åº”ä½“**:
```json
{
  "asset_id": "asset_abc123",
  "duration": 125.5,
  "sample_rate": 48000,
  "channels": 2,
  "data": {
    "left": [0.1, 0.3, 0.5, ..., 0.2],   // 12550 ä¸ªç‚¹ï¼ˆ125.5ç§’ Ã— 100ï¼‰
    "right": [0.12, 0.28, 0.48, ..., 0.18]
  },
  "peaks": {
    "min": -0.95,
    "max": 0.98
  }
}
```

**ç¼“å­˜ç­–ç•¥**:
- é¦–æ¬¡è¯·æ±‚æ—¶è®¡ç®—å¹¶å­˜å…¥ Redisï¼ˆkey: `waveform:{asset_id}:{resolution}`ï¼‰ã€‚
- è®¾ç½® TTL ä¸º 7 å¤©ã€‚
- åŒæ—¶ä¸Šä¼  `.waveform.json` æ–‡ä»¶åˆ°å¯¹è±¡å­˜å‚¨ä½œä¸ºæ°¸ä¹…å¤‡ä»½ã€‚

**ç”Ÿæˆç®—æ³•**:
```python
import librosa
import numpy as np

def generate_waveform(audio_path: str, target_points: int = 12000):
    # 1. åŠ è½½éŸ³é¢‘ï¼ˆåªå–å•å£°é“ï¼Œé™ä½è®¡ç®—é‡ï¼‰
    y, sr = librosa.load(audio_path, sr=16000, mono=False)
    
    # 2. è®¡ç®—æ¯ä¸ªé‡‡æ ·çª—å£çš„ RMS èƒ½é‡
    hop_length = len(y[0]) // target_points
    rms_left = librosa.feature.rms(y=y[0], hop_length=hop_length)[0]
    rms_right = librosa.feature.rms(y=y[1], hop_length=hop_length)[0]
    
    # 3. å½’ä¸€åŒ–åˆ° [-1, 1]
    max_val = max(rms_left.max(), rms_right.max())
    return {
        "left": (rms_left / max_val).tolist(),
        "right": (rms_right / max_val).tolist()
    }
```

---

## 8. AI å¤„ç†æ¨¡å—

### 8.1 è§¦å‘ ASR è½¬å†™

**ç«¯ç‚¹**: `POST /api/tasks/asr`

**è¯·æ±‚ä½“**:
```json
{
  "asset_id": "asset_abc123",
  "language": "zh",  // zh, en, auto
  "model": "large-v3",  // tiny, base, small, medium, large-v3
  "enable_diarization": true,  // æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»
  "enable_word_timestamps": true  // æ˜¯å¦è¿”å›å­—çº§æ—¶é—´æˆ³
}
```

**å“åº”ä½“**:
```json
{
  "task_id": "task_asr_xyz789",
  "status": "pending",
  "estimated_time": 45  // é¢„è®¡éœ€è¦ 45 ç§’
}
```

**è½®è¯¢ä»»åŠ¡çŠ¶æ€**: `GET /api/tasks/task_asr_xyz789`

```json
{
  "task_id": "task_asr_xyz789",
  "status": "processing",  // pending | processing | completed | failed
  "progress": 68,
  "current_step": "æ­£åœ¨è½¬å†™éŸ³é¢‘...",
  "result": null
}
```

**ä»»åŠ¡å®Œæˆæ—¶çš„ result å­—æ®µ**:
```json
{
  "task_id": "task_asr_xyz789",
  "status": "completed",
  "progress": 100,
  "result": {
    "segments": [
      {
        "id": "seg_001",
        "text": "å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°æˆ‘çš„é¢‘é“",
        "start": 0.5,
        "end": 3.2,
        "speaker": "SPEAKER_00",  // è¯´è¯äºº IDï¼ˆå¦‚æœå¯ç”¨ diarizationï¼‰
        "words": [
          { "word": "å¤§å®¶å¥½", "start": 0.5, "end": 1.1, "confidence": 0.95 },
          { "word": "æ¬¢è¿", "start": 1.2, "end": 1.6, "confidence": 0.92 },
          { "word": "æ¥åˆ°", "start": 1.7, "end": 2.0, "confidence": 0.89 },
          { "word": "æˆ‘çš„", "start": 2.1, "end": 2.5, "confidence": 0.94 },
          { "word": "é¢‘é“", "start": 2.6, "end": 3.2, "confidence": 0.97 }
        ]
      },
      {
        "id": "seg_002",
        "text": "ä»Šå¤©æˆ‘ä»¬æ¥èŠä¸€èŠäººå·¥æ™ºèƒ½",
        "start": 3.5,
        "end": 6.8,
        "speaker": "SPEAKER_00",
        "words": [ /* ... */ ]
      }
    ],
    "language": "zh",
    "duration": 125.5,
    "word_count": 342
  }
}
```

**åç«¯ Celery ä»»åŠ¡å®ç°è¦ç‚¹**:

```python
@celery_app.task(bind=True)
def transcribe_with_diarization(self, asset_id: str, options: dict):
    try:
        # 1. ä¸‹è½½éŸ³é¢‘æ–‡ä»¶
        audio_path = download_asset(asset_id)
        
        # 2. ä½¿ç”¨ faster-whisper è¿›è¡Œ ASR
        from faster_whisper import WhisperModel
        model = WhisperModel("large-v3", device="cuda", compute_type="float16")
        
        segments_raw, info = model.transcribe(
            audio_path,
            language=options["language"],
            word_timestamps=True,
            vad_filter=True,  # è¿‡æ»¤é™éŸ³æ®µ
            vad_parameters=dict(min_silence_duration_ms=500)
        )
        
        # 3. å¦‚æœå¯ç”¨è¯´è¯äººåˆ†ç¦»
        if options["enable_diarization"]:
            from pyannote.audio import Pipeline
            diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=settings.HUGGINGFACE_TOKEN
            )
            diarization = diarization_pipeline(audio_path)
            
            # 4. åˆå¹¶ ASR å’Œ Diarization ç»“æœ
            segments = merge_asr_and_diarization(segments_raw, diarization)
        else:
            segments = list(segments_raw)
        
        # 5. ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        save_transcription_result(asset_id, segments)
        
        return {"segments": segments, "language": info.language}
        
    except Exception as e:
        self.update_state(state="FAILURE", meta={"error": str(e)})
        raise
```

---

### 8.2 è§¦å‘äººå£°ä¼´å¥åˆ†ç¦»

**ç«¯ç‚¹**: `POST /api/tasks/stem-separation`

**è¯·æ±‚ä½“**:
```json
{
  "asset_id": "asset_abc123",
  "model": "htdemucs",  // htdemucs | htdemucs_ft | mdx_extra
  "stems": ["vocals", "accompaniment"]  // å¯é€‰: drums, bass, other
}
```

**å“åº”ä½“**:
```json
{
  "task_id": "task_stem_xyz456",
  "status": "pending",
  "estimated_time": 120  // é¢„è®¡ 2 åˆ†é’Ÿ
}
```

**ä»»åŠ¡å®Œæˆåçš„ result**:
```json
{
  "task_id": "task_stem_xyz456",
  "status": "completed",
  "result": {
    "stems": [
      {
        "type": "vocals",
        "asset_id": "asset_vocals_001",
        "url": "https://storage.supabase.co/audio/stems/vocals_abc123.mp3",
        "duration": 125.5
      },
      {
        "type": "accompaniment",
        "asset_id": "asset_acc_001",
        "url": "https://storage.supabase.co/audio/stems/accompaniment_abc123.mp3",
        "duration": 125.5
      }
    ]
  }
}
```

**å‰ç«¯æ¥æ”¶åˆ°ç»“æœåçš„å¤„ç†**:
```typescript
// 1. è‡ªåŠ¨æ·»åŠ ä¸¤æ¡æ–°éŸ³è½¨åˆ°æ—¶é—´è½´
const { stems } = taskResult.result;

stems.forEach(stem => {
  const trackId = editorStore.addTrack(`${stem.type} Track`);
  editorStore.addClip({
    id: `clip_${stem.asset_id}`,
    type: 'audio',
    trackId,
    assetId: stem.asset_id,
    url: stem.url,
    start: 0,
    duration: stem.duration
  });
});

// 2. å¯é€‰ï¼šé™éŸ³æˆ–åˆ é™¤åŸå§‹éŸ³è½¨
editorStore.updateClip(originalClipId, { muted: true });
```

**Celery ä»»åŠ¡å®ç°**:
```python
@celery_app.task(bind=True)
def separate_stems(self, asset_id: str, options: dict):
    import demucs.separate
    
    # 1. ä¸‹è½½éŸ³é¢‘
    audio_path = download_asset(asset_id)
    
    # 2. è¿è¡Œ Demucs
    self.update_state(state="PROCESSING", meta={"progress": 10})
    
    demucs.separate.main([
        "--two-stems", "vocals",  # åªåˆ†ç¦»äººå£°å’Œä¼´å¥
        "-n", options["model"],
        audio_path
    ])
    
    # 3. ä¸Šä¼ åˆ†ç¦»åçš„æ–‡ä»¶åˆ°å¯¹è±¡å­˜å‚¨
    stems = []
    for stem_type in ["vocals", "no_vocals"]:
        stem_path = f"separated/{stem_type}.wav"
        asset_id = upload_to_storage(stem_path)
        stems.append({
            "type": stem_type,
            "asset_id": asset_id,
            "url": get_asset_url(asset_id)
        })
    
    return {"stems": stems}
```

---

### 8.3 è¯´è¯äººåˆ†ç¦» (Speaker Diarization)

**ç«¯ç‚¹**: `POST /api/tasks/speaker-diarization`

**ç”¨é€”**: åœ¨å¤šäººå¯¹è¯çš„è§†é¢‘ä¸­ï¼Œè¯†åˆ«"è°åœ¨ä»€ä¹ˆæ—¶å€™è¯´è¯"ã€‚

**è¯·æ±‚ä½“**:
```json
{
  "asset_id": "asset_abc123",
  "num_speakers": null,  // null è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šäººæ•°
  "min_speakers": 1,
  "max_speakers": 10
}
```

**å“åº”ä½“**:
```json
{
  "task_id": "task_diarization_xyz",
  "status": "pending"
}
```

**å®Œæˆåçš„ result**:
```json
{
  "speakers": [
    {
      "speaker_id": "SPEAKER_00",
      "segments": [
        { "start": 0.5, "end": 5.2 },
        { "start": 12.3, "end": 18.7 }
      ],
      "total_speaking_time": 11.1
    },
    {
      "speaker_id": "SPEAKER_01",
      "segments": [
        { "start": 5.5, "end": 12.0 }
      ],
      "total_speaking_time": 6.5
    }
  ],
  "num_speakers_detected": 2
}
```

**å‰ç«¯åº”ç”¨åœºæ™¯**:
- åœ¨æ—¶é—´è½´ä¸Šç”¨ä¸åŒé¢œè‰²æ ‡è®°ä¸åŒè¯´è¯äººçš„ç‰‡æ®µã€‚
- å®ç°"åªä¿ç•™ä¸»è®²äººçš„è¯"åŠŸèƒ½ï¼ˆè‡ªåŠ¨åˆ é™¤å…¶ä»–äººçš„æ’è¯ï¼‰ã€‚

---

## 9. å¯¼å‡ºæ¸²æŸ“æ¨¡å—

### 9.1 æäº¤å¯¼å‡ºä»»åŠ¡

**ç«¯ç‚¹**: `POST /api/export`

**è¯·æ±‚ä½“**ï¼ˆæ ¸å¿ƒï¼šä¼ é€’å®Œæ•´çš„ Timeline æè¿°ï¼‰:
```json
{
  "project_id": "proj_abc123",
  "preset": "1080p_h264",  // æˆ–è‡ªå®šä¹‰å‚æ•°
  "custom_settings": {
    "resolution": { "width": 1920, "height": 1080 },
    "fps": 30,
    "video_codec": "libx264",
    "video_bitrate": "5M",
    "audio_codec": "aac",
    "audio_bitrate": "192k"
  },
  "timeline": {
    "duration": 125.5,
    "tracks": [
      {
        "id": "track_001",
        "type": "video",
        "clips": [
          {
            "id": "clip_001",
            "asset_id": "asset_abc123",
            "start": 0,
            "duration": 10.5,
            "trim_start": 2.3,  // ç´ æå†…åç§»
            "trim_end": 12.8,
            "effects": [
              { "type": "speed", "rate": 1.2 },
              { "type": "crop", "x": 100, "y": 50, "width": 1720, "height": 980 }
            ]
          }
        ]
      },
      {
        "id": "track_002",
        "type": "audio",
        "clips": [ /* ... */ ]
      }
    ]
  }
}
```

**åç«¯æ¸²æŸ“æµç¨‹**:

```python
@celery_app.task(bind=True)
def render_video(self, export_id: str, timeline: dict, settings: dict):
    import ffmpeg
    
    # 1. è§£æ timelineï¼Œç”Ÿæˆ FFmpeg æ»¤é•œå›¾
    filter_complex = build_filter_graph(timeline)
    
    # 2. æ„å»º FFmpeg å‘½ä»¤
    inputs = []
    for clip in get_all_clips(timeline):
        inputs.append(ffmpeg.input(clip["url"]))
    
    output = ffmpeg.output(
        *inputs,
        f"exports/{export_id}.mp4",
        vcodec=settings["video_codec"],
        acodec=settings["audio_codec"],
        video_bitrate=settings["video_bitrate"],
        filter_complex=filter_complex
    )
    
    # 3. æ‰§è¡Œæ¸²æŸ“ï¼ˆç›‘æ§è¿›åº¦ï¼‰
    process = output.run_async(pipe_stdout=True, pipe_stderr=True)
    
    while True:
        stderr_line = process.stderr.readline().decode()
        if "time=" in stderr_line:
            # è§£æå½“å‰æ¸²æŸ“åˆ°ç¬¬å‡ ç§’
            current_time = parse_ffmpeg_time(stderr_line)
            progress = int(current_time / timeline["duration"] * 100)
            self.update_state(state="PROCESSING", meta={"progress": progress})
        
        if process.poll() is not None:
            break
    
    # 4. ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨
    final_url = upload_to_storage(f"exports/{export_id}.mp4")
    
    return {"download_url": final_url, "file_size": get_file_size(final_url)}
```

**FFmpeg æ»¤é•œå›¾ç¤ºä¾‹**ï¼ˆé’ˆå¯¹ä¸Šé¢çš„ timelineï¼‰:
```
ffmpeg -i input.mp4 \
  -filter_complex "\
    [0:v]trim=2.3:12.8,setpts=PTS-STARTPTS[v0]; \
    [v0]setpts=PTS/1.2[v1]; \
    [v1]crop=1720:980:100:50[vout]; \
  " \
  -map "[vout]" -map 0:a output.mp4
```

---

## 10. æ•°æ®åº“è¡¨ç»“æ„è®¾è®¡

### 10.1 æ ¸å¿ƒè¡¨è®¾è®¡

#### 10.1.1 projects è¡¨ï¼ˆé¡¹ç›®ä¸»è¡¨ï¼‰

```sql
CREATE TABLE projects (
    -- ä¸»é”®
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    
    -- ç”¨æˆ·å…³è”ï¼ˆé¢„ç•™å¤šç§Ÿæˆ·æ”¯æŒï¼‰
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    
    -- åŸºæœ¬ä¿¡æ¯
    name TEXT NOT NULL,
    description TEXT,
    thumbnail_url TEXT,
    
    -- é¡¹ç›®è®¾ç½®
    settings JSONB NOT NULL DEFAULT '{
        "resolution": {"width": 1920, "height": 1080},
        "fps": 30,
        "sample_rate": 48000,
        "background_color": "#000000"
    }'::jsonb,
    
    -- æ—¶é—´è½´æ•°æ®ï¼ˆæ ¸å¿ƒå­—æ®µï¼‰
    timeline JSONB NOT NULL DEFAULT '{
        "tracks": [],
        "clips": [],
        "effects": [],
        "markers": []
    }'::jsonb,
    
    -- ç‰ˆæœ¬æ§åˆ¶
    version INTEGER NOT NULL DEFAULT 1,
    last_synced_at TIMESTAMPTZ,
    last_synced_by UUID REFERENCES auth.users(id),
    
    -- çŠ¶æ€ç®¡ç†
    status TEXT NOT NULL DEFAULT 'draft',
        CHECK (status IN ('draft', 'ready', 'exporting', 'archived', 'deleted')),
    
    -- ç»Ÿè®¡ä¿¡æ¯
    duration FLOAT GENERATED ALWAYS AS (
        (timeline->>'duration')::float
    ) STORED,
    clip_count INTEGER GENERATED ALWAYS AS (
        jsonb_array_length(timeline->'clips')
    ) STORED,
    
    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    deleted_at TIMESTAMPTZ
);

-- ç´¢å¼•
CREATE INDEX idx_projects_user_id ON projects(user_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_projects_status ON projects(status);
CREATE INDEX idx_projects_updated_at ON projects(updated_at DESC);
CREATE INDEX idx_projects_timeline_gin ON projects USING GIN (timeline);

-- è‡ªåŠ¨æ›´æ–° updated_at
CREATE TRIGGER update_projects_updated_at
    BEFORE UPDATE ON projects
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
```

#### 10.1.2 assets è¡¨ï¼ˆèµ„æºåº“ï¼‰

```sql
CREATE TABLE assets (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    
    -- æ‰€å±é¡¹ç›®
    project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    
    -- èµ„æºç±»å‹
    type TEXT NOT NULL CHECK (type IN (
        'video', 'audio', 'image', 'subtitle', 
        'stem_vocals', 'stem_accompaniment', 'stem_drums', 'stem_bass'
    )),
    
    -- å­˜å‚¨è·¯å¾„
    storage_path TEXT NOT NULL,  -- Supabase Storage ç›¸å¯¹è·¯å¾„
    url TEXT NOT NULL,           -- å®Œæ•´è®¿é—® URL
    proxy_url TEXT,              -- ä»£ç†è§†é¢‘ URLï¼ˆä½åˆ†è¾¨ç‡ï¼‰
    thumbnail_url TEXT,          -- ç¼©ç•¥å›¾
    sprite_sheet_url TEXT,       -- é›ªç¢§å›¾ï¼ˆæ—¶é—´è½´é¢„è§ˆï¼‰
    
    -- æ–‡ä»¶ä¿¡æ¯
    file_name TEXT NOT NULL,
    file_size BIGINT NOT NULL,   -- å­—èŠ‚
    mime_type TEXT NOT NULL,
    
    -- åª’ä½“å…ƒæ•°æ®
    metadata JSONB NOT NULL DEFAULT '{}'::jsonb,
    /*
    è§†é¢‘ç¤ºä¾‹:
    {
        "width": 1920,
        "height": 1080,
        "duration": 125.5,
        "fps": 30,
        "codec": "h264",
        "bitrate": 5000000,
        "has_audio": true
    }
    
    éŸ³é¢‘ç¤ºä¾‹:
    {
        "duration": 125.5,
        "sample_rate": 48000,
        "channels": 2,
        "codec": "aac",
        "bitrate": 192000
    }
    */
    
    -- æ³¢å½¢æ•°æ®ï¼ˆéŸ³é¢‘ä¸“ç”¨ï¼‰
    waveform_data JSONB,  -- å­˜å‚¨æ³¢å½¢å³°å€¼æ•°ç»„
    waveform_resolution INTEGER DEFAULT 100,  -- æ¯ç§’é‡‡æ ·ç‚¹æ•°
    
    -- æ¥æºæ ‡è®°
    is_generated BOOLEAN DEFAULT FALSE,  -- æ˜¯å¦ä¸º AI ç”Ÿæˆçš„è¡ç”Ÿèµ„æº
    parent_asset_id UUID REFERENCES assets(id) ON DELETE SET NULL,
    generation_method TEXT,  -- 'asr', 'stem_separation', 'proxy_generation'
    
    -- çŠ¶æ€
    status TEXT NOT NULL DEFAULT 'uploading',
        CHECK (status IN ('uploading', 'processing', 'ready', 'failed')),
    processing_progress INTEGER DEFAULT 0,  -- 0-100
    error_message TEXT,
    
    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    deleted_at TIMESTAMPTZ
);

-- ç´¢å¼•
CREATE INDEX idx_assets_project_id ON assets(project_id) WHERE deleted_at IS NULL;
CREATE INDEX idx_assets_user_id ON assets(user_id);
CREATE INDEX idx_assets_type ON assets(type);
CREATE INDEX idx_assets_parent_id ON assets(parent_asset_id);
CREATE INDEX idx_assets_status ON assets(status);
CREATE INDEX idx_assets_metadata_gin ON assets USING GIN (metadata);
```

#### 10.1.3 timeline_snapshots è¡¨ï¼ˆç‰ˆæœ¬å†å²ï¼‰

```sql
CREATE TABLE timeline_snapshots (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    
    -- ç‰ˆæœ¬ä¿¡æ¯
    version INTEGER NOT NULL,
    version_hash TEXT,  -- SHA256(timeline JSON)ï¼Œç”¨äºå»é‡
    
    -- å¿«ç…§æ•°æ®
    timeline_data JSONB NOT NULL,
    
    -- æè¿°
    snapshot_type TEXT NOT NULL DEFAULT 'auto',
        CHECK (snapshot_type IN ('auto', 'manual', 'checkpoint')),
    description TEXT,
    
    -- å¤§å°ä¼˜åŒ–
    is_compressed BOOLEAN DEFAULT FALSE,
    compressed_size BIGINT,  -- å‹ç¼©åå¤§å°
    original_size BIGINT,    -- åŸå§‹å¤§å°
    
    -- åˆ›å»ºä¿¡æ¯
    created_by UUID REFERENCES auth.users(id),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    -- è¿‡æœŸç­–ç•¥
    expires_at TIMESTAMPTZ,  -- è‡ªåŠ¨å¿«ç…§ 90 å¤©ååˆ é™¤
    
    UNIQUE (project_id, version)
);

-- ç´¢å¼•
CREATE INDEX idx_snapshots_project_version ON timeline_snapshots(project_id, version DESC);
CREATE INDEX idx_snapshots_created_at ON timeline_snapshots(created_at DESC);
CREATE INDEX idx_snapshots_expires_at ON timeline_snapshots(expires_at) 
    WHERE expires_at IS NOT NULL;
```

#### 10.1.4 tasks è¡¨ï¼ˆå¼‚æ­¥ä»»åŠ¡è¿½è¸ªï¼‰

```sql
CREATE TABLE tasks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    
    -- ä»»åŠ¡ç±»å‹
    task_type TEXT NOT NULL CHECK (task_type IN (
        'asr', 'stem_separation', 'speaker_diarization',
        'generate_proxy', 'generate_waveform', 'generate_sprite',
        'export_video', 'auto_caption'
    )),
    
    -- å…³è”èµ„æº
    project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
    asset_id UUID REFERENCES assets(id) ON DELETE CASCADE,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    
    -- Celery ä»»åŠ¡ ID
    celery_task_id TEXT NOT NULL UNIQUE,
    
    -- çŠ¶æ€
    status TEXT NOT NULL DEFAULT 'pending',
        CHECK (status IN ('pending', 'processing', 'completed', 'failed', 'cancelled')),
    progress INTEGER DEFAULT 0,  -- 0-100
    current_step TEXT,
    
    -- é…ç½®å‚æ•°
    input_params JSONB NOT NULL DEFAULT '{}'::jsonb,
    
    -- ç»“æœ
    result JSONB,
    error_message TEXT,
    error_traceback TEXT,
    
    -- æ€§èƒ½æŒ‡æ ‡
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    execution_time FLOAT GENERATED ALWAYS AS (
        EXTRACT(EPOCH FROM (completed_at - started_at))
    ) STORED,
    
    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ç´¢å¼•
CREATE INDEX idx_tasks_celery_id ON tasks(celery_task_id);
CREATE INDEX idx_tasks_project_id ON tasks(project_id);
CREATE INDEX idx_tasks_asset_id ON tasks(asset_id);
CREATE INDEX idx_tasks_status ON tasks(status);
CREATE INDEX idx_tasks_type_status ON tasks(task_type, status);
CREATE INDEX idx_tasks_created_at ON tasks(created_at DESC);
```

#### 10.1.5 export_jobs è¡¨ï¼ˆå¯¼å‡ºä»»åŠ¡ï¼‰

```sql
CREATE TABLE export_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
    user_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    
    -- å¯¼å‡ºé…ç½®
    preset TEXT,  -- '1080p_h264', '4k_hevc', 'web_optimized'
    custom_settings JSONB,
    /*
    {
        "resolution": {"width": 1920, "height": 1080},
        "fps": 30,
        "video_codec": "libx264",
        "video_bitrate": "5M",
        "audio_codec": "aac",
        "audio_bitrate": "192k"
    }
    */
    
    -- æ—¶é—´è½´å¿«ç…§ï¼ˆå¯¼å‡ºæ—¶çš„çŠ¶æ€ï¼‰
    timeline_snapshot JSONB NOT NULL,
    
    -- ä»»åŠ¡çŠ¶æ€
    task_id UUID REFERENCES tasks(id),
    status TEXT NOT NULL DEFAULT 'queued',
        CHECK (status IN ('queued', 'rendering', 'uploading', 'completed', 'failed')),
    progress INTEGER DEFAULT 0,
    
    -- è¾“å‡ºæ–‡ä»¶
    output_url TEXT,
    output_file_size BIGINT,
    output_duration FLOAT,
    
    -- é”™è¯¯ä¿¡æ¯
    error_message TEXT,
    
    -- æ—¶é—´æˆ³
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    completed_at TIMESTAMPTZ,
    expires_at TIMESTAMPTZ  -- å¯¼å‡ºæ–‡ä»¶ä¿ç•™ 7 å¤©
);

-- ç´¢å¼•
CREATE INDEX idx_exports_project_id ON export_jobs(project_id);
CREATE INDEX idx_exports_user_id ON export_jobs(user_id);
CREATE INDEX idx_exports_status ON export_jobs(status);
CREATE INDEX idx_exports_expires_at ON export_jobs(expires_at) 
    WHERE expires_at IS NOT NULL;
```

#### 10.1.6 transcriptions è¡¨ï¼ˆè½¬å†™ç»“æœï¼‰

```sql
CREATE TABLE transcriptions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    asset_id UUID NOT NULL REFERENCES assets(id) ON DELETE CASCADE,
    project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
    
    -- è½¬å†™å‚æ•°
    language TEXT NOT NULL,
    model TEXT NOT NULL,  -- 'large-v3', 'medium', etc.
    
    -- è½¬å†™ç»“æœ
    segments JSONB NOT NULL DEFAULT '[]'::jsonb,
    /*
    [
        {
            "id": "seg_001",
            "text": "å¤§å®¶å¥½",
            "start": 0.5,
            "end": 1.2,
            "speaker": "SPEAKER_00",
            "words": [
                {"word": "å¤§å®¶å¥½", "start": 0.5, "end": 1.2, "confidence": 0.95}
            ]
        }
    ]
    */
    
    -- ç»Ÿè®¡ä¿¡æ¯
    total_segments INTEGER GENERATED ALWAYS AS (
        jsonb_array_length(segments)
    ) STORED,
    total_words INTEGER,
    average_confidence FLOAT,
    
    -- è¯´è¯äººåˆ†ç¦»
    has_diarization BOOLEAN DEFAULT FALSE,
    num_speakers INTEGER,
    
    -- åˆ›å»ºä¿¡æ¯
    task_id UUID REFERENCES tasks(id),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- ç´¢å¼•
CREATE INDEX idx_transcriptions_asset_id ON transcriptions(asset_id);
CREATE INDEX idx_transcriptions_project_id ON transcriptions(project_id);
CREATE INDEX idx_transcriptions_segments_gin ON transcriptions USING GIN (segments);
```

---

### 10.2 è¾…åŠ©è¡¨è®¾è®¡

#### 10.2.1 user_preferences è¡¨

```sql
CREATE TABLE user_preferences (
    user_id UUID PRIMARY KEY REFERENCES auth.users(id) ON DELETE CASCADE,
    
    -- ç¼–è¾‘å™¨è®¾ç½®
    editor_settings JSONB DEFAULT '{
        "auto_save_interval": 3000,
        "default_track_height": 60,
        "waveform_color": "#4CAF50",
        "snap_to_grid": true,
        "grid_size": 0.1
    }'::jsonb,
    
    -- å¯¼å‡ºé¢„è®¾
    export_presets JSONB DEFAULT '[]'::jsonb,
    
    -- å¿«æ·é”®æ˜ å°„
    keyboard_shortcuts JSONB,
    
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

#### 10.2.2 system_fonts è¡¨

```sql
CREATE TABLE system_fonts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name TEXT NOT NULL,
    display_name TEXT NOT NULL,
    font_family TEXT NOT NULL,
    
    -- å­—ä½“æ–‡ä»¶
    variants JSONB NOT NULL,  -- ['regular', 'bold', 'italic']
    file_urls JSONB NOT NULL,
    /*
    {
        "regular": "https://.../font-regular.woff2",
        "bold": "https://.../font-bold.woff2"
    }
    */
    
    -- è®¸å¯ä¿¡æ¯
    license TEXT,
    is_free BOOLEAN DEFAULT TRUE,
    
    -- è¯­è¨€æ”¯æŒ
    supported_languages TEXT[] DEFAULT ARRAY['zh', 'en'],
    
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

---

## 11. ç´¢å¼•ä¸æŸ¥è¯¢ä¼˜åŒ–

### 11.1 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

#### 11.1.1 æœ€é¢‘ç¹æŸ¥è¯¢ï¼ˆçƒ­è·¯å¾„ï¼‰

```sql
-- 1. åŠ è½½é¡¹ç›®è¯¦æƒ…ï¼ˆç¼–è¾‘å™¨æ‰“å¼€æ—¶ï¼‰
-- ç›®æ ‡: < 50ms
EXPLAIN ANALYZE
SELECT 
    p.*,
    json_agg(DISTINCT a.*) FILTER (WHERE a.id IS NOT NULL) AS assets,
    json_agg(DISTINCT t.*) FILTER (WHERE t.id IS NOT NULL) AS recent_tasks
FROM projects p
LEFT JOIN assets a ON a.project_id = p.id AND a.deleted_at IS NULL
LEFT JOIN tasks t ON t.project_id = p.id AND t.created_at > NOW() - INTERVAL '1 hour'
WHERE p.id = $1 AND p.deleted_at IS NULL
GROUP BY p.id;

-- ä¼˜åŒ–: æ·»åŠ éƒ¨åˆ†ç´¢å¼•
CREATE INDEX idx_assets_project_recent ON assets(project_id, created_at DESC) 
    WHERE deleted_at IS NULL AND created_at > NOW() - INTERVAL '7 days';
```

#### 11.1.2 å…¨æ–‡æœç´¢ï¼ˆæœç´¢è½¬å†™æ–‡æœ¬ï¼‰

```sql
-- åœ¨ transcriptions è¡¨æ·»åŠ å…¨æ–‡æœç´¢ç´¢å¼•
ALTER TABLE transcriptions ADD COLUMN search_vector tsvector 
    GENERATED ALWAYS AS (
        to_tsvector('chinese_zh', 
            jsonb_path_query_array(segments, '$[*].text')::text
        )
    ) STORED;

CREATE INDEX idx_transcriptions_search ON transcriptions USING GIN (search_vector);

-- æœç´¢æŸ¥è¯¢
SELECT t.id, t.asset_id, 
       ts_headline('chinese_zh', 
           (segments->>0)::text, 
           plainto_tsquery('chinese_zh', 'äººå·¥æ™ºèƒ½')
       ) AS snippet
FROM transcriptions t
WHERE search_vector @@ plainto_tsquery('chinese_zh', 'äººå·¥æ™ºèƒ½')
ORDER BY ts_rank(search_vector, plainto_tsquery('chinese_zh', 'äººå·¥æ™ºèƒ½')) DESC
LIMIT 20;
```

### 11.2 åˆ†åŒºç­–ç•¥

å¯¹äºå¤§ç”¨æˆ·é‡åœºæ™¯ï¼ŒæŒ‰ç”¨æˆ· ID è¿›è¡Œæ°´å¹³åˆ†åŒºï¼š

```sql
-- å°† projects è¡¨åˆ†åŒºï¼ˆä»… PostgreSQL 12+ï¼‰
CREATE TABLE projects_partitioned (
    LIKE projects INCLUDING ALL
) PARTITION BY HASH (user_id);

-- åˆ›å»º 16 ä¸ªåˆ†åŒº
CREATE TABLE projects_part_0 PARTITION OF projects_partitioned
    FOR VALUES WITH (MODULUS 16, REMAINDER 0);
-- ... åˆ›å»º projects_part_1 åˆ° projects_part_15
```

---

## 12. å¯¹è±¡å­˜å‚¨ç­–ç•¥

### 12.1 å­˜å‚¨æ¡¶è§„åˆ’

```
videos/                    # ä¸»è¦å­˜å‚¨æ¡¶
â”œâ”€â”€ raw/                   # ç”¨æˆ·ä¸Šä¼ çš„åŸå§‹è§†é¢‘
â”‚   â””â”€â”€ {project_id}/{filename}.mp4
â”œâ”€â”€ proxy/                 # 720p ä»£ç†è§†é¢‘
â”‚   â””â”€â”€ {project_id}/{filename}_proxy.mp4
â”œâ”€â”€ sprites/               # æ—¶é—´è½´é›ªç¢§å›¾
â”‚   â””â”€â”€ {asset_id}.jpg
â”œâ”€â”€ audio/                 # æå–çš„éŸ³é¢‘
â”‚   â””â”€â”€ stems/             # åˆ†ç¦»åçš„éŸ³è½¨
â”‚       â”œâ”€â”€ {asset_id}_vocals.mp3
â”‚       â””â”€â”€ {asset_id}_accompaniment.mp3
â”œâ”€â”€ waveforms/             # æ³¢å½¢æ•°æ®
â”‚   â””â”€â”€ {asset_id}.json
â””â”€â”€ exports/               # å¯¼å‡ºçš„è§†é¢‘
    â””â”€â”€ {export_id}.mp4
```

### 12.2 ç”Ÿå‘½å‘¨æœŸç­–ç•¥

```json
{
  "rules": [
    {
      "name": "Delete temporary exports after 7 days",
      "prefix": "exports/",
      "expiration": { "days": 7 }
    },
    {
      "name": "Move old proxies to cold storage after 90 days",
      "prefix": "proxy/",
      "transitions": [
        {
          "days": 90,
          "storage_class": "GLACIER"
        }
      ]
    }
  ]
}
```

### 12.3 CDN åŠ é€Ÿé…ç½®

```typescript
// å‰ç«¯åŠ è½½èµ„æºæ—¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ CDN
const getAssetUrl = (path: string) => {
  const cdnDomains = [
    'cdn1.hoppingrabbit.ai',
    'cdn2.hoppingrabbit.ai',
    'cdn3.hoppingrabbit.ai'
  ];
  
  // æ ¹æ®è·¯å¾„å“ˆå¸Œé€‰æ‹© CDNï¼ˆç®€å•è´Ÿè½½å‡è¡¡ï¼‰
  const hash = path.split('').reduce((acc, char) => acc + char.charCodeAt(0), 0);
  const domain = cdnDomains[hash % cdnDomains.length];
  
  return `https://${domain}/${path}`;
};
```

---

## 13. å‰ç«¯ Store æ¶æ„

### 13.1 Zustand Store åˆ†å±‚è®¾è®¡

ä¸ºäº†é¿å…å•ä¸€ Store è¿‡äºåºå¤§ï¼Œé‡‡ç”¨**åˆ‡ç‰‡æ¨¡å¼ (Slices Pattern)**ï¼š

```typescript
// src/stores/index.ts
import { create } from 'zustand';
import { devtools, persist } from 'zustand/middleware';
import { createTimelineSlice, TimelineSlice } from './slices/timeline';
import { createProjectSlice, ProjectSlice } from './slices/project';
import { createPlaybackSlice, PlaybackSlice } from './slices/playback';
import { createSyncSlice, SyncSlice } from './slices/sync';

type EditorStore = TimelineSlice & ProjectSlice & PlaybackSlice & SyncSlice;

export const useEditorStore = create<EditorStore>()(
  devtools(
    persist(
      (set, get, api) => ({
        ...createTimelineSlice(set, get, api),
        ...createProjectSlice(set, get, api),
        ...createPlaybackSlice(set, get, api),
        ...createSyncSlice(set, get, api),
      }),
      {
        name: 'hoppingrabbit-editor',
        // åªæŒä¹…åŒ–éƒ¨åˆ†å­—æ®µåˆ° LocalStorage
        partialize: (state) => ({
          recentProjects: state.recentProjects,
          editorPreferences: state.editorPreferences,
        }),
      }
    )
  )
);
```

### 13.2 Timeline Sliceï¼ˆæ—¶é—´è½´ç®¡ç†ï¼‰

```typescript
// src/stores/slices/timeline.ts
export interface TimelineSlice {
  // ========== çŠ¶æ€ ==========
  tracks: Track[];
  clips: Clip[];
  selectedClipIds: Set<string>;
  
  // ========== è½¨é“æ“ä½œ ==========
  addTrack: (type: ClipType, name?: string) => string;
  removeTrack: (trackId: string) => void;
  updateTrack: (trackId: string, updates: Partial<Track>) => void;
  
  // ========== ç‰‡æ®µæ“ä½œ ==========
  addClip: (clip: Clip) => void;
  removeClip: (clipId: string) => void;
  updateClip: (clipId: string, updates: Partial<Clip>) => void;
  moveClip: (clipId: string, trackId: string, newStart: number) => void;
  splitClip: (clipId: string, splitTime: number) => void;
  
  // ========== å¤šé€‰æ“ä½œ ==========
  selectClip: (clipId: string, addToSelection?: boolean) => void;
  selectAll: () => void;
  clearSelection: () => void;
  deleteSelectedClips: () => void;
  
  // ========== å†å²è®°å½• ==========
  history: TimelineState[];
  historyIndex: number;
  undo: () => void;
  redo: () => void;
  saveSnapshot: () => void;
}

export const createTimelineSlice: StateCreator<TimelineSlice> = (set, get) => ({
  tracks: [],
  clips: [],
  selectedClipIds: new Set(),
  history: [],
  historyIndex: -1,
  
  addClip: (clip) => {
    set((state) => {
      const newClips = [...state.clips, clip];
      
      // ğŸ”¥ ä¹è§‚æ›´æ–°ï¼šç«‹å³åæ˜ åœ¨ UI
      return { clips: newClips };
    });
    
    // ğŸ”¥ è§¦å‘è‡ªåŠ¨ä¿å­˜ï¼ˆé˜²æŠ–ï¼‰
    get().debouncedSave();
  },
  
  updateClip: (clipId, updates) => {
    set((state) => ({
      clips: state.clips.map(c => 
        c.id === clipId ? { ...c, ...updates } : c
      )
    }));
    
    get().debouncedSave();
  },
  
  // ... å…¶ä»–æ–¹æ³•å®ç°
});
```

### 13.3 Sync Sliceï¼ˆåŒæ­¥ç®¡ç†ï¼‰

```typescript
// src/stores/slices/sync.ts
import { debounce } from 'lodash';

export interface SyncSlice {
  // ========== åŒæ­¥çŠ¶æ€ ==========
  isSyncing: boolean;
  lastSyncedAt: Date | null;
  syncError: Error | null;
  isDirty: boolean;  // æ˜¯å¦æœ‰æœªä¿å­˜çš„æ›´æ”¹
  isOffline: boolean;
  
  // ========== ç‰ˆæœ¬æ§åˆ¶ ==========
  serverVersion: number;
  pendingOperations: Operation[];  // ç¦»çº¿æ—¶çš„æ“ä½œé˜Ÿåˆ—
  
  // ========== åŒæ­¥æ–¹æ³• ==========
  saveToServer: () => Promise<void>;
  debouncedSave: () => void;
  loadFromServer: (projectId: string) => Promise<void>;
  resolveConflict: (strategy: 'local' | 'server') => Promise<void>;
}

export const createSyncSlice: StateCreator<SyncSlice> = (set, get) => ({
  isSyncing: false,
  lastSyncedAt: null,
  syncError: null,
  isDirty: false,
  isOffline: false,
  serverVersion: 0,
  pendingOperations: [],
  
  saveToServer: async () => {
    const state = get();
    
    if (state.isSyncing) return;  // é˜²æ­¢å¹¶å‘ä¿å­˜
    
    set({ isSyncing: true, syncError: null });
    
    try {
      const response = await api.patch(`/api/projects/${state.projectId}/state`, {
        version: state.serverVersion,
        timeline: {
          tracks: state.tracks,
          clips: state.clips,
        }
      });
      
      set({
        isSyncing: false,
        lastSyncedAt: new Date(),
        serverVersion: response.new_version,
        isDirty: false,
        isOffline: false,
      });
      
    } catch (error) {
      if (error.response?.status === 409) {
        // ç‰ˆæœ¬å†²çª
        set({ 
          syncError: new Error('ç‰ˆæœ¬å†²çªï¼Œè¯·åˆ·æ–°é¡µé¢'),
          isSyncing: false 
        });
        
      } else if (!navigator.onLine) {
        // ç½‘ç»œç¦»çº¿
        set({ 
          isOffline: true,
          isSyncing: false 
        });
        
        // å°†å½“å‰çŠ¶æ€åŠ å…¥ç¦»çº¿é˜Ÿåˆ—
        saveToIndexedDB(state);
        
      } else {
        set({ 
          syncError: error,
          isSyncing: false 
        });
      }
    }
  },
  
  // ğŸ”¥ é˜²æŠ–ä¿å­˜ï¼ˆ3ç§’å†…æ— æ–°æ“ä½œæ‰è§¦å‘ï¼‰
  debouncedSave: debounce(() => {
    get().saveToServer();
  }, 3000),
  
  loadFromServer: async (projectId) => {
    const response = await api.get(`/api/projects/${projectId}`);
    
    set({
      projectId,
      tracks: response.timeline.tracks,
      clips: response.timeline.clips,
      serverVersion: response.version,
      lastSyncedAt: new Date(),
      isDirty: false,
    });
  },
});
```

---

## 14. API è°ƒç”¨ç‚¹æ˜ å°„

### 14.1 å‰ç«¯ç»„ä»¶ä¸ API çš„å¯¹åº”å…³ç³»

| ç»„ä»¶ | è§¦å‘æ—¶æœº | API ç«¯ç‚¹ | ä½œç”¨ |
|------|---------|---------|------|
| **UploadModal** | ç”¨æˆ·é€‰æ‹©è§†é¢‘æ–‡ä»¶ | `POST /api/assets/presign-upload` | è·å–ä¸Šä¼ ç­¾å URL |
| **UploadModal** | ä¸Šä¼ å®Œæˆ | `POST /api/assets/confirm-upload` | è§¦å‘é¢„å¤„ç†ä»»åŠ¡ |
| **Editor (onMount)** | æ‰“å¼€é¡¹ç›® | `GET /api/projects/{id}` | åŠ è½½é¡¹ç›®å®Œæ•´çŠ¶æ€ |
| **TranscriptEditor** | ç‚¹å‡»"å¼€å§‹è½¬å†™" | `POST /api/tasks/asr` | è§¦å‘ ASR ä»»åŠ¡ |
| **TranscriptEditor** | è½®è¯¢ä»»åŠ¡çŠ¶æ€ | `GET /api/tasks/{task_id}` | è·å–è½¬å†™è¿›åº¦ |
| **AssetPanel** | ç‚¹å‡»"äººå£°åˆ†ç¦»" | `POST /api/tasks/stem-separation` | åˆ†ç¦»äººå£°å’Œä¼´å¥ |
| **Timeline** | æ‹–åŠ¨ç‰‡æ®µã€åˆ‡å‰²ç­‰ | `PATCH /api/projects/{id}/state` | ä¿å­˜æ—¶é—´è½´çŠ¶æ€ï¼ˆé˜²æŠ–ï¼‰ |
| **Header** | ç‚¹å‡»"å¯¼å‡º" | `POST /api/export` | æäº¤å¯¼å‡ºä»»åŠ¡ |
| **ExportDialog** | è½®è¯¢å¯¼å‡ºè¿›åº¦ | `GET /api/export/{export_id}` | è·å–æ¸²æŸ“è¿›åº¦ |
| **AssetPanel** | åŠ è½½éŸ³é¢‘æ³¢å½¢ | `GET /api/assets/{asset_id}/waveform` | è·å–æ³¢å½¢æ•°æ® |

### 14.2 å…³é”®ç»„ä»¶çš„ API é›†æˆç¤ºä¾‹

#### 14.2.1 TranscriptEditor.tsxï¼ˆè½¬å†™åŠŸèƒ½ï¼‰

```typescript
// src/components/editor/TranscriptEditor.tsx
export const TranscriptEditor = () => {
  const [taskId, setTaskId] = useState<string | null>(null);
  const [progress, setProgress] = useState(0);
  const { project, addAsset } = useEditorStore();
  
  const startTranscription = async () => {
    try {
      // 1. å‘èµ· ASR ä»»åŠ¡
      const response = await api.post('/api/tasks/asr', {
        asset_id: project.mainVideoAssetId,
        language: 'zh',
        model: 'large-v3',
        enable_diarization: true,
        enable_word_timestamps: true,
      });
      
      setTaskId(response.task_id);
      
      // 2. å¼€å§‹è½®è¯¢è¿›åº¦
      pollTaskStatus(response.task_id);
      
    } catch (error) {
      toast.error('å¯åŠ¨è½¬å†™å¤±è´¥');
    }
  };
  
  const pollTaskStatus = async (taskId: string) => {
    const interval = setInterval(async () => {
      const status = await api.get(`/api/tasks/${taskId}`);
      
      setProgress(status.progress);
      
      if (status.status === 'completed') {
        clearInterval(interval);
        
        // 3. å°†è½¬å†™ç»“æœåŠ è½½åˆ°ç¼–è¾‘å™¨
        useEditorStore.setState({
          transcript: status.result.segments,
        });
        
        toast.success('è½¬å†™å®Œæˆï¼');
        
      } else if (status.status === 'failed') {
        clearInterval(interval);
        toast.error(status.error_message);
      }
    }, 2000);  // æ¯ 2 ç§’è½®è¯¢ä¸€æ¬¡
  };
  
  return (
    <div>
      {taskId ? (
        <Progress value={progress} />
      ) : (
        <Button onClick={startTranscription}>å¼€å§‹è½¬å†™</Button>
      )}
    </div>
  );
};
```

#### 14.2.2 Timeline.tsxï¼ˆè‡ªåŠ¨ä¿å­˜ï¼‰

```typescript
// src/components/editor/Timeline.tsx
export const Timeline = () => {
  const { clips, updateClip, debouncedSave } = useEditorStore();
  
  const handleClipDrag = (clipId: string, newStart: number) => {
    // ğŸ”¥ ç«‹å³æ›´æ–° UIï¼ˆ0ms å»¶è¿Ÿï¼‰
    updateClip(clipId, { start: newStart });
    
    // ğŸ”¥ debouncedSave ä¼šåœ¨ 3 ç§’åè‡ªåŠ¨è°ƒç”¨ saveToServer
    // ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨ä»»ä½•ä¿å­˜æ–¹æ³•
  };
  
  return (
    <div onMouseMove={handleDragMove}>
      {clips.map(clip => (
        <ClipComponent 
          key={clip.id} 
          clip={clip} 
          onDrag={handleClipDrag} 
        />
      ))}
    </div>
  );
};
```

---

## 15. æ¯«ç§’çº§ä¿å­˜æœºåˆ¶

### 15.1 å®Œæ•´å®ç°æµç¨‹

```typescript
// src/lib/sync-manager.ts
import Dexie, { Table } from 'dexie';

// ========== IndexedDB å®šä¹‰ ==========
class EditorDatabase extends Dexie {
  projects!: Table<ProjectState>;
  pendingOperations!: Table<Operation>;

  constructor() {
    super('HoppingRabbitEditor');
    this.version(1).stores({
      projects: 'id, lastModified',
      pendingOperations: '++id, projectId, timestamp, synced',
    });
  }
}

const db = new EditorDatabase();

// ========== æ“ä½œè®°å½•ç±»å‹ ==========
interface Operation {
  id?: number;
  projectId: string;
  type: string;
  timestamp: number;
  payload: any;
  synced: boolean;
}

// ========== åŒæ­¥ç®¡ç†å™¨ ==========
export class SyncManager {
  private syncTimer: NodeJS.Timeout | null = null;
  private isDirty = false;
  
  constructor(private projectId: string) {
    this.setupAutoSync();
    this.setupOnlineListener();
  }
  
  // è®°å½•æ“ä½œï¼ˆä¾› Zustand actions è°ƒç”¨ï¼‰
  recordOperation(type: string, payload: any) {
    const operation: Operation = {
      projectId: this.projectId,
      type,
      timestamp: Date.now(),
      payload,
      synced: false,
    };
    
    // 1. ç«‹å³ä¿å­˜åˆ° IndexedDBï¼ˆ5msï¼‰
    db.pendingOperations.add(operation);
    
    // 2. æ ‡è®°ä¸ºè„æ•°æ®
    this.isDirty = true;
    
    // 3. é‡ç½®é˜²æŠ–è®¡æ—¶å™¨
    this.resetSyncTimer();
  }
  
  private resetSyncTimer() {
    if (this.syncTimer) {
      clearTimeout(this.syncTimer);
    }
    
    // 3 ç§’åæ‰§è¡ŒåŒæ­¥
    this.syncTimer = setTimeout(() => {
      this.syncToServer();
    }, 3000);
  }
  
  private async syncToServer() {
    if (!this.isDirty) return;
    
    try {
      // 1. è·å–æ‰€æœ‰æœªåŒæ­¥çš„æ“ä½œ
      const operations = await db.pendingOperations
        .where({ projectId: this.projectId, synced: false })
        .toArray();
      
      if (operations.length === 0) return;
      
      // 2. å‘é€åˆ°æœåŠ¡å™¨
      const response = await api.patch(
        `/api/projects/${this.projectId}/state`,
        {
          version: useEditorStore.getState().serverVersion,
          operations: operations.map(op => ({
            type: op.type,
            timestamp: op.timestamp,
            payload: op.payload,
          })),
        }
      );
      
      // 3. æ ‡è®°ä¸ºå·²åŒæ­¥
      await db.pendingOperations
        .where('id')
        .anyOf(operations.map(op => op.id!))
        .modify({ synced: true });
      
      // 4. æ›´æ–°ç‰ˆæœ¬å·
      useEditorStore.setState({
        serverVersion: response.new_version,
        lastSyncedAt: new Date(),
        isDirty: false,
      });
      
      this.isDirty = false;
      
    } catch (error) {
      console.error('Sync failed:', error);
      
      // ç½‘ç»œé”™è¯¯ï¼šæ ‡è®°ä¸ºç¦»çº¿æ¨¡å¼
      if (!navigator.onLine) {
        useEditorStore.setState({ isOffline: true });
      }
    }
  }
  
  // ç›‘å¬ç½‘ç»œæ¢å¤
  private setupOnlineListener() {
    window.addEventListener('online', () => {
      console.log('ç½‘ç»œå·²æ¢å¤ï¼Œå¼€å§‹åŒæ­¥...');
      this.syncToServer();
    });
  }
  
  // è‡ªåŠ¨ä¿å­˜ï¼ˆæ¯ 30 ç§’å¼ºåˆ¶åŒæ­¥ä¸€æ¬¡ï¼‰
  private setupAutoSync() {
    setInterval(() => {
      if (this.isDirty) {
        this.syncToServer();
      }
    }, 30000);
  }
}
```

### 15.2 åœ¨ Zustand Store ä¸­é›†æˆ

```typescript
// src/stores/slices/timeline.ts
import { SyncManager } from '@/lib/sync-manager';

export const createTimelineSlice: StateCreator<TimelineSlice> = (set, get) => {
  let syncManager: SyncManager | null = null;
  
  return {
    // ... çŠ¶æ€å®šä¹‰
    
    initProject: (projectId: string) => {
      syncManager = new SyncManager(projectId);
    },
    
    updateClip: (clipId, updates) => {
      set((state) => ({
        clips: state.clips.map(c => 
          c.id === clipId ? { ...c, ...updates } : c
        )
      }));
      
      // ğŸ”¥ è®°å½•æ“ä½œåˆ°åŒæ­¥é˜Ÿåˆ—
      syncManager?.recordOperation('UPDATE_CLIP', {
        clipId,
        updates,
      });
    },
    
    deleteClip: (clipId) => {
      set((state) => ({
        clips: state.clips.filter(c => c.id !== clipId)
      }));
      
      syncManager?.recordOperation('DELETE_CLIP', { clipId });
    },
  };
};
```

---

## 16. ç¦»çº¿å®¹ç¾æ–¹æ¡ˆ

### 16.1 ä¸‰çº§å®¹ç¾ä½“ç³»

```
[ç”¨æˆ·æ“ä½œ] 
    â†“
[Zustand å†…å­˜çŠ¶æ€] (0ms å“åº”)
    â†“
[IndexedDB æœ¬åœ°æŒä¹…åŒ–] (5ms å†™å…¥)
    â†“
[åç«¯ PostgreSQL] (3s é˜²æŠ–ååŒæ­¥)
```

### 16.2 ç¦»çº¿æ¨¡å¼å¤„ç†

```typescript
// src/lib/offline-manager.ts
export class OfflineManager {
  static async saveFullSnapshot() {
    const state = useEditorStore.getState();
    
    await db.projects.put({
      id: state.projectId,
      tracks: state.tracks,
      clips: state.clips,
      lastModified: Date.now(),
    });
  }
  
  static async restoreFromLocal(projectId: string) {
    const localState = await db.projects.get(projectId);
    
    if (!localState) {
      throw new Error('æœ¬åœ°æœªæ‰¾åˆ°é¡¹ç›®æ•°æ®');
    }
    
    // è¯¢é—®ç”¨æˆ·æ˜¯å¦ä½¿ç”¨æœ¬åœ°ç‰ˆæœ¬
    const useLocal = confirm(
      'æ£€æµ‹åˆ°æœ¬åœ°æœ‰æœªåŒæ­¥çš„æ›´æ”¹ï¼Œæ˜¯å¦åŠ è½½æœ¬åœ°ç‰ˆæœ¬ï¼Ÿ\n' +
      `æœ€åä¿®æ”¹æ—¶é—´: ${new Date(localState.lastModified).toLocaleString()}`
    );
    
    if (useLocal) {
      useEditorStore.setState({
        tracks: localState.tracks,
        clips: localState.clips,
        isDirty: true,  // æ ‡è®°éœ€è¦åŒæ­¥
      });
      
      // ç«‹å³å°è¯•åŒæ­¥åˆ°æœåŠ¡å™¨
      new SyncManager(projectId).syncToServer();
    }
  }
}
```

### 16.3 å†²çªè§£å†³ UI

```typescript
// src/components/ConflictDialog.tsx
export const ConflictDialog = ({ serverVersion, localVersion }) => {
  const resolveConflict = async (strategy: 'local' | 'server') => {
    if (strategy === 'server') {
      // æ”¾å¼ƒæœ¬åœ°æ›´æ”¹ï¼Œä½¿ç”¨æœåŠ¡å™¨ç‰ˆæœ¬
      const response = await api.get(`/api/projects/${projectId}`);
      useEditorStore.setState({
        tracks: response.timeline.tracks,
        clips: response.timeline.clips,
        serverVersion: response.version,
      });
      
    } else {
      // å¼ºåˆ¶è¦†ç›–æœåŠ¡å™¨ç‰ˆæœ¬
      await api.patch(`/api/projects/${projectId}/state`, {
        version: null,  // ç»•è¿‡ç‰ˆæœ¬æ£€æŸ¥
        timeline: {
          tracks: useEditorStore.getState().tracks,
          clips: useEditorStore.getState().clips,
        },
        force_override: true,
      });
    }
  };
  
  return (
    <Dialog>
      <p>æ£€æµ‹åˆ°ç‰ˆæœ¬å†²çªï¼Œè¯·é€‰æ‹©ä¿ç•™å“ªä¸ªç‰ˆæœ¬ï¼š</p>
      <Button onClick={() => resolveConflict('local')}>
        ä¿ç•™æˆ‘çš„æ›´æ”¹ï¼ˆè¦†ç›–æœåŠ¡å™¨ï¼‰
      </Button>
      <Button onClick={() => resolveConflict('server')}>
        ä½¿ç”¨æœåŠ¡å™¨ç‰ˆæœ¬ï¼ˆæ”¾å¼ƒæˆ‘çš„æ›´æ”¹ï¼‰
      </Button>
    </Dialog>
  );
};
```

---

## 17. ASR è¯­éŸ³è¯†åˆ«

### 17.1 æŠ€æœ¯é€‰å‹å¯¹æ¯”

| æ¨¡å‹ | ç²¾åº¦ | é€Ÿåº¦ | æ˜¾å­˜éœ€æ±‚ | è¯­è¨€æ”¯æŒ | æ¨èåœºæ™¯ |
|------|------|------|---------|---------|---------|
| **Whisper Tiny** | ä¸­ç­‰ | æå¿« | < 1GB | 99ç§ | å®æ—¶é¢„è§ˆ |
| **Whisper Base** | è‰¯å¥½ | å¿« | 1-2GB | 99ç§ | å¿«é€Ÿè‰ç¨¿ |
| **Whisper Large-V3** | ä¼˜ç§€ | ä¸­ç­‰ | 10GB | 99ç§ | **ç”Ÿäº§æ¨è** |
| **Paraformer** | ä¼˜ç§€ | å¿« | 2-3GB | ä¸­æ–‡ | çº¯ä¸­æ–‡åœºæ™¯ |

**æœ€ç»ˆé€‰æ‹©**: **faster-whisper Large-V3**
- ä½¿ç”¨ CTranslate2 åŠ é€Ÿï¼Œæ¯”åŸç‰ˆ Whisper å¿« 4 å€ã€‚
- è´¨é‡æ¥è¿‘ OpenAI Whisper APIã€‚
- æœ¬åœ°éƒ¨ç½²ï¼Œæ—  API è°ƒç”¨æˆæœ¬ã€‚

### 17.2 å®Œæ•´ ASR ä»»åŠ¡å®ç°

```python
# backend/app/tasks/transcribe.py
import os
import subprocess
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline
import torch

from app.tasks import celery_app
from app.config import settings
from app.services.supabase_client import get_supabase

@celery_app.task(bind=True, max_retries=3)
def transcribe_video(
    self,
    asset_id: str,
    project_id: str,
    options: dict
):
    """
    å®Œæ•´çš„ ASR æµç¨‹ï¼š
    1. ä¸‹è½½è§†é¢‘
    2. æå–éŸ³é¢‘
    3. VAD æ£€æµ‹é™éŸ³
    4. Whisper è½¬å†™
    5. è¯´è¯äººåˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
    6. ä¿å­˜ç»“æœ
    """
    try:
        # ========== Step 1: ä¸‹è½½èµ„æº ==========
        self.update_state(
            state='PROCESSING',
            meta={'progress': 5, 'message': 'æ­£åœ¨ä¸‹è½½è§†é¢‘...'}
        )
        
        supabase = get_supabase()
        asset = supabase.table('assets').select('*').eq('id', asset_id).single().execute()
        
        video_path = f"/tmp/{asset_id}.mp4"
        download_from_storage(asset.data['storage_path'], video_path)
        
        # ========== Step 2: æå–éŸ³é¢‘ ==========
        self.update_state(
            state='PROCESSING',
            meta={'progress': 10, 'message': 'æ­£åœ¨æå–éŸ³é¢‘...'}
        )
        
        audio_path = f"/tmp/{asset_id}.wav"
        subprocess.run([
            'ffmpeg', '-i', video_path,
            '-vn',  # ä¸è¦è§†é¢‘æµ
            '-acodec', 'pcm_s16le',  # 16-bit PCM
            '-ar', '16000',  # 16kHz é‡‡æ ·ç‡ï¼ˆWhisper æ¨èï¼‰
            '-ac', '1',  # å•å£°é“
            audio_path
        ], check=True)
        
        # ========== Step 3: åŠ è½½ Whisper æ¨¡å‹ ==========
        self.update_state(
            state='PROCESSING',
            meta={'progress': 20, 'message': 'æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹...'}
        )
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        compute_type = "float16" if device == "cuda" else "int8"
        
        model = WhisperModel(
            options.get('model', 'large-v3'),
            device=device,
            compute_type=compute_type,
            download_root=settings.MODEL_CACHE_DIR
        )
        
        # ========== Step 4: æ‰§è¡Œè½¬å†™ ==========
        self.update_state(
            state='PROCESSING',
            meta={'progress': 30, 'message': 'æ­£åœ¨è½¬å†™éŸ³é¢‘...'}
        )
        
        segments_raw, info = model.transcribe(
            audio_path,
            language=options.get('language', 'zh'),
            word_timestamps=True,  # âœ… è·å–å­—çº§æ—¶é—´æˆ³
            vad_filter=True,  # âœ… è¿‡æ»¤é™éŸ³æ®µ
            vad_parameters=dict(
                min_silence_duration_ms=500,  # 500ms ä»¥ä¸Šçš„é™éŸ³æ‰åˆ‡åˆ†
                threshold=0.5  # VAD ç½®ä¿¡åº¦é˜ˆå€¼
            ),
            beam_size=5,  # Beam Search å®½åº¦ï¼ˆè¶Šå¤§è¶Šå‡†ç¡®ä½†è¶Šæ…¢ï¼‰
        )
        
        # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆç”Ÿæˆå™¨åªèƒ½è¿­ä»£ä¸€æ¬¡ï¼‰
        segments = list(segments_raw)
        
        # ========== Step 5: è¯´è¯äººåˆ†ç¦»ï¼ˆå¯é€‰ï¼‰==========
        if options.get('enable_diarization', False):
            self.update_state(
                state='PROCESSING',
                meta={'progress': 60, 'message': 'æ­£åœ¨è¯†åˆ«è¯´è¯äºº...'}
            )
            
            diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=settings.HUGGINGFACE_TOKEN
            )
            
            # è¿è¡Œ Diarization
            diarization = diarization_pipeline(audio_path)
            
            # åˆå¹¶ ASR å’Œ Diarization ç»“æœ
            segments = merge_segments_with_speakers(segments, diarization)
        
        # ========== Step 6: æ ¼å¼åŒ–è¾“å‡º ==========
        formatted_segments = []
        for i, segment in enumerate(segments):
            formatted_segments.append({
                "id": f"seg_{i:04d}",
                "text": segment.text.strip(),
                "start": round(segment.start, 2),
                "end": round(segment.end, 2),
                "speaker": getattr(segment, 'speaker', None),
                "words": [
                    {
                        "word": word.word,
                        "start": round(word.start, 2),
                        "end": round(word.end, 2),
                        "confidence": round(word.probability, 2)
                    }
                    for word in segment.words
                ] if segment.words else []
            })
        
        # ========== Step 7: ä¿å­˜åˆ°æ•°æ®åº“ ==========
        self.update_state(
            state='PROCESSING',
            meta={'progress': 90, 'message': 'æ­£åœ¨ä¿å­˜ç»“æœ...'}
        )
        
        supabase.table('transcriptions').insert({
            'asset_id': asset_id,
            'project_id': project_id,
            'language': info.language,
            'model': options.get('model', 'large-v3'),
            'segments': formatted_segments,
            'total_words': sum(len(seg['words']) for seg in formatted_segments),
            'average_confidence': calculate_avg_confidence(formatted_segments),
            'has_diarization': options.get('enable_diarization', False),
            'num_speakers': len(set(seg.get('speaker') for seg in formatted_segments if seg.get('speaker')))
        }).execute()
        
        # ========== Step 8: æ¸…ç†ä¸´æ—¶æ–‡ä»¶ ==========
        os.remove(video_path)
        os.remove(audio_path)
        
        return {
            'segments': formatted_segments,
            'language': info.language,
            'duration': info.duration,
        }
        
    except Exception as e:
        self.update_state(
            state='FAILURE',
            meta={'error': str(e)}
        )
        raise


def merge_segments_with_speakers(whisper_segments, diarization):
    """
    å°† Whisper çš„è½¬å†™ç»“æœä¸ Pyannote çš„è¯´è¯äººåˆ†ç¦»ç»“æœåˆå¹¶
    """
    result = []
    
    for segment in whisper_segments:
        # æ‰¾åˆ°è¯¥æ—¶é—´æ®µå†…çš„ä¸»è¦è¯´è¯äºº
        speaker_votes = {}
        
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            # è®¡ç®—é‡å æ—¶é—´
            overlap_start = max(segment.start, turn.start)
            overlap_end = min(segment.end, turn.end)
            overlap_duration = max(0, overlap_end - overlap_start)
            
            if overlap_duration > 0:
                speaker_votes[speaker] = speaker_votes.get(speaker, 0) + overlap_duration
        
        # é€‰æ‹©å ç”¨æ—¶é—´æœ€é•¿çš„è¯´è¯äºº
        if speaker_votes:
            main_speaker = max(speaker_votes, key=speaker_votes.get)
            segment.speaker = main_speaker
        
        result.append(segment)
    
    return result
```

### 17.3 VADï¼ˆé™éŸ³æ£€æµ‹ï¼‰ä¼˜åŒ–

```python
# backend/app/tasks/vad.py
import torch
import torchaudio

def detect_silence_segments(audio_path: str, threshold_db=-40, min_duration_ms=500):
    """
    ä½¿ç”¨ Silero VAD æ£€æµ‹é™éŸ³ç‰‡æ®µ
    è¿”å›éœ€è¦åˆ é™¤çš„æ—¶é—´æ®µ
    """
    model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False
    )
    
    (get_speech_timestamps, _, _, _, _) = utils
    
    # åŠ è½½éŸ³é¢‘
    waveform, sample_rate = torchaudio.load(audio_path)
    
    # æ£€æµ‹è¯­éŸ³æ—¶é—´æˆ³
    speech_timestamps = get_speech_timestamps(
        waveform,
        model,
        sampling_rate=sample_rate,
        min_silence_duration_ms=min_duration_ms,
        threshold=0.5
    )
    
    # è®¡ç®—é™éŸ³ç‰‡æ®µï¼ˆè¯­éŸ³ä¹‹é—´çš„é—´éš™ï¼‰
    silence_segments = []
    audio_duration = waveform.shape[1] / sample_rate
    
    for i in range(len(speech_timestamps) - 1):
        silence_start = speech_timestamps[i]['end'] / sample_rate
        silence_end = speech_timestamps[i+1]['start'] / sample_rate
        
        if silence_end - silence_start >= min_duration_ms / 1000:
            silence_segments.append({
                'start': silence_start,
                'end': silence_end,
                'duration': silence_end - silence_start
            })
    
    return silence_segments
```

---

## 18. äººå£°ä¼´å¥åˆ†ç¦»

### 18.1 Demucs æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | è´¨é‡ | é€Ÿåº¦ | è¾“å‡º |
|------|------|------|------|
| **htdemucs** | â­â­â­â­ | ä¸­ç­‰ | vocals, drums, bass, other |
| **htdemucs_ft** | â­â­â­â­â­ | æ…¢ | vocals, drums, bass, other |
| **mdx_extra** | â­â­â­ | å¿« | vocals, instrumental |

**æ¨è**: **htdemucs** ï¼ˆè´¨é‡å’Œé€Ÿåº¦å¹³è¡¡ï¼‰

### 18.2 å®Œæ•´åˆ†ç¦»ä»»åŠ¡

```python
# backend/app/tasks/stem_separation.py
import demucs.separate
import os
from pathlib import Path

@celery_app.task(bind=True)
def separate_audio_stems(
    self,
    asset_id: str,
    project_id: str,
    options: dict
):
    """
    éŸ³é¢‘è½¨é“åˆ†ç¦»ï¼š
    - vocals: äººå£°
    - accompaniment: ä¼´å¥ï¼ˆdrums + bass + otherï¼‰
    """
    try:
        # 1. ä¸‹è½½éŸ³é¢‘æ–‡ä»¶
        self.update_state(state='PROCESSING', meta={'progress': 5})
        
        supabase = get_supabase()
        asset = supabase.table('assets').select('*').eq('id', asset_id).single().execute()
        
        audio_path = f"/tmp/{asset_id}.wav"
        download_from_storage(asset.data['storage_path'], audio_path)
        
        # 2. è¿è¡Œ Demucs
        self.update_state(state='PROCESSING', meta={'progress': 10, 'message': 'æ­£åœ¨åˆ†ç¦»éŸ³è½¨...'})
        
        output_dir = f"/tmp/demucs_output/{asset_id}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä½¿ç”¨ Two-Stems æ¨¡å¼ï¼ˆåªåˆ†ç¦»äººå£°å’Œä¼´å¥ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰
        demucs.separate.main([
            "--two-stems", "vocals",  # åªè¾“å‡º vocals å’Œ no_vocals
            "-n", options.get('model', 'htdemucs'),
            "-o", output_dir,
            audio_path
        ])
        
        # 3. ä¸Šä¼ åˆ†ç¦»åçš„æ–‡ä»¶
        self.update_state(state='PROCESSING', meta={'progress': 70, 'message': 'æ­£åœ¨ä¸Šä¼ ç»“æœ...'})
        
        model_name = options.get('model', 'htdemucs')
        vocals_path = f"{output_dir}/{model_name}/{Path(audio_path).stem}/vocals.wav"
        accompaniment_path = f"{output_dir}/{model_name}/{Path(audio_path).stem}/no_vocals.wav"
        
        # ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨
        vocals_asset_id = upload_audio_to_storage(
            vocals_path,
            f"stems/{asset_id}_vocals.wav"
        )
        
        acc_asset_id = upload_audio_to_storage(
            accompaniment_path,
            f"stems/{asset_id}_accompaniment.wav"
        )
        
        # 4. åˆ›å»º assets è®°å½•
        stems = []
        
        for stem_type, stem_asset_id, stem_path in [
            ('stem_vocals', vocals_asset_id, vocals_path),
            ('stem_accompaniment', acc_asset_id, accompaniment_path)
        ]:
            # è·å–éŸ³é¢‘å…ƒæ•°æ®
            duration = get_audio_duration(stem_path)
            file_size = os.path.getsize(stem_path)
            
            asset_record = supabase.table('assets').insert({
                'project_id': project_id,
                'type': stem_type,
                'storage_path': f"stems/{asset_id}_{stem_type}.wav",
                'url': get_public_url(f"stems/{asset_id}_{stem_type}.wav"),
                'file_name': f"{stem_type}.wav",
                'file_size': file_size,
                'mime_type': 'audio/wav',
                'metadata': {
                    'duration': duration,
                    'sample_rate': 44100,
                    'channels': 2,
                    'codec': 'pcm_s16le'
                },
                'is_generated': True,
                'parent_asset_id': asset_id,
                'generation_method': 'stem_separation',
                'status': 'ready'
            }).execute()
            
            stems.append({
                'type': stem_type,
                'asset_id': asset_record.data['id'],
                'url': asset_record.data['url'],
                'duration': duration
            })
        
        # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(output_dir)
        os.remove(audio_path)
        
        return {'stems': stems}
        
    except Exception as e:
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise
```

### 18.3 å®æ—¶è¿›åº¦ç›‘æ§

Demucs æœ¬èº«ä¸æä¾›è¿›åº¦å›è°ƒï¼Œå¯ä»¥é€šè¿‡ç›‘æ§è¾“å‡ºæ—¥å¿—å®ç°ï¼š

```python
import subprocess
import re

def run_demucs_with_progress(self, audio_path, output_dir, model):
    """
    è¿è¡Œ Demucs å¹¶è§£æè¿›åº¦
    """
    process = subprocess.Popen(
        ['demucs', '--two-stems', 'vocals', '-n', model, '-o', output_dir, audio_path],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True
    )
    
    for line in process.stdout:
        # è§£æè¿›åº¦ï¼ˆDemucs è¾“å‡ºç¤ºä¾‹ï¼šProcessing chunk 45/100ï¼‰
        match = re.search(r'Processing chunk (\d+)/(\d+)', line)
        if match:
            current, total = int(match.group(1)), int(match.group(2))
            progress = int((current / total) * 100)
            
            self.update_state(
                state='PROCESSING',
                meta={'progress': 10 + int(progress * 0.6), 'message': f'åˆ†ç¦»ä¸­ {current}/{total}'}
            )
    
    process.wait()
    
    if process.returncode != 0:
        raise Exception('Demucs æ‰§è¡Œå¤±è´¥')
```

---

## 19. è¯´è¯äººåˆ†ç¦» (Diarization)

### 19.1 Pyannote.audio Pipeline

```python
# backend/app/tasks/diarization.py
from pyannote.audio import Pipeline
import torch

@celery_app.task(bind=True)
def speaker_diarization(
    self,
    asset_id: str,
    project_id: str,
    options: dict
):
    """
    è¯´è¯äººåˆ†ç¦»ï¼šè¯†åˆ«"è°åœ¨ä»€ä¹ˆæ—¶å€™è¯´è¯"
    """
    try:
        # 1. ä¸‹è½½éŸ³é¢‘
        audio_path = download_asset_audio(asset_id)
        
        # 2. åŠ è½½ Diarization Pipeline
        self.update_state(state='PROCESSING', meta={'progress': 10, 'message': 'åŠ è½½æ¨¡å‹...'})
        
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=settings.HUGGINGFACE_TOKEN
        )
        
        # GPU åŠ é€Ÿ
        if torch.cuda.is_available():
            pipeline.to(torch.device("cuda"))
        
        # 3. è¿è¡Œ Diarization
        self.update_state(state='PROCESSING', meta={'progress': 20, 'message': 'åˆ†æè¯´è¯äºº...'})
        
        diarization = pipeline(
            audio_path,
            num_speakers=options.get('num_speakers'),  # None è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
            min_speakers=options.get('min_speakers', 1),
            max_speakers=options.get('max_speakers', 10)
        )
        
        # 4. æ ¼å¼åŒ–ç»“æœ
        speakers = {}
        
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            if speaker not in speakers:
                speakers[speaker] = {
                    'speaker_id': speaker,
                    'segments': [],
                    'total_speaking_time': 0
                }
            
            segment = {
                'start': round(turn.start, 2),
                'end': round(turn.end, 2)
            }
            
            speakers[speaker]['segments'].append(segment)
            speakers[speaker]['total_speaking_time'] += turn.end - turn.start
        
        # 5. ä¿å­˜ç»“æœ
        result = {
            'speakers': list(speakers.values()),
            'num_speakers_detected': len(speakers)
        }
        
        supabase = get_supabase()
        supabase.table('tasks').update({
            'status': 'completed',
            'result': result
        }).eq('celery_task_id', self.request.id).execute()
        
        return result
        
    except Exception as e:
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise
```

### 19.2 å‰ç«¯åº”ç”¨ï¼šæŒ‰è¯´è¯äººç­›é€‰

```typescript
// src/components/editor/SpeakerFilter.tsx
export const SpeakerFilter = () => {
  const { transcript, updateSegment } = useEditorStore();
  const [selectedSpeakers, setSelectedSpeakers] = useState<Set<string>>(new Set());
  
  // è·å–æ‰€æœ‰è¯´è¯äºº
  const speakers = useMemo(() => {
    const uniqueSpeakers = new Set<string>();
    transcript.forEach(seg => {
      if (seg.speaker) {
        uniqueSpeakers.add(seg.speaker);
      }
    });
    return Array.from(uniqueSpeakers);
  }, [transcript]);
  
  const toggleSpeaker = (speaker: string) => {
    const newSet = new Set(selectedSpeakers);
    if (newSet.has(speaker)) {
      newSet.delete(speaker);
    } else {
      newSet.add(speaker);
    }
    setSelectedSpeakers(newSet);
  };
  
  const deleteOtherSpeakers = () => {
    // åˆ é™¤æœªé€‰ä¸­è¯´è¯äººçš„æ‰€æœ‰ç‰‡æ®µ
    transcript.forEach(seg => {
      if (seg.speaker && !selectedSpeakers.has(seg.speaker)) {
        updateSegment(seg.id, { is_deleted: true, delete_reason: 'speaker_filter' });
      }
    });
  };
  
  return (
    <div className="speaker-filter">
      <h3>è¯´è¯äººç­›é€‰</h3>
      {speakers.map(speaker => (
        <label key={speaker}>
          <input
            type="checkbox"
            checked={selectedSpeakers.has(speaker)}
            onChange={() => toggleSpeaker(speaker)}
          />
          {speaker}
        </label>
      ))}
      <Button onClick={deleteOtherSpeakers}>
        åªä¿ç•™é€‰ä¸­çš„è¯´è¯äºº
      </Button>
    </div>
  );
};
```

---

## 20. æ™ºèƒ½å‰ªè¾‘ç®—æ³•

### 20.1 è‡ªåŠ¨åˆ é™¤å†—ä½™ç‰‡æ®µ

```python
# backend/app/tasks/smart_clean.py
@celery_app.task
def auto_delete_redundant_segments(project_id: str, options: dict):
    """
    æ™ºèƒ½å‰ªè¾‘ï¼šè‡ªåŠ¨æ ‡è®°éœ€è¦åˆ é™¤çš„ç‰‡æ®µ
    1. è¯­æ°”è¯/å£å¤´ç¦…ï¼ˆ"å‘ƒ"ã€"å—¯"ã€"é‚£ä¸ª"ï¼‰
    2. é•¿æ—¶é—´é™éŸ³ï¼ˆ> 2ç§’ï¼‰
    3. é‡å¤è¯­å¥
    """
    supabase = get_supabase()
    
    # è·å–è½¬å†™ç»“æœ
    transcription = supabase.table('transcriptions') \
        .select('segments') \
        .eq('project_id', project_id) \
        .single() \
        .execute()
    
    segments = transcription.data['segments']
    suggestions = []
    
    # ========== 1. æ£€æµ‹è¯­æ°”è¯ ==========
    filler_words = ['å‘ƒ', 'å—¯', 'å•Š', 'é‚£ä¸ª', 'å°±æ˜¯', 'ç„¶å']
    
    for seg in segments:
        text = seg['text'].strip()
        if text in filler_words or len(text) <= 2:
            suggestions.append({
                'segment_id': seg['id'],
                'reason': 'filler_word',
                'confidence': 0.95,
                'description': f'æ£€æµ‹åˆ°è¯­æ°”è¯ï¼š"{text}"'
            })
    
    # ========== 2. æ£€æµ‹é•¿æ—¶é—´é™éŸ³ ==========
    for i in range(len(segments) - 1):
        gap = segments[i+1]['start'] - segments[i]['end']
        if gap > options.get('max_silence_duration', 2.0):
            suggestions.append({
                'segment_id': None,
                'time_range': [segments[i]['end'], segments[i+1]['start']],
                'reason': 'long_silence',
                'confidence': 0.9,
                'description': f'æ£€æµ‹åˆ° {gap:.1f} ç§’é™éŸ³'
            })
    
    # ========== 3. æ£€æµ‹é‡å¤è¯­å¥ ==========
    for i in range(len(segments) - 1):
        similarity = calculate_text_similarity(
            segments[i]['text'],
            segments[i+1]['text']
        )
        if similarity > 0.8:  # 80% ç›¸ä¼¼åº¦
            suggestions.append({
                'segment_id': segments[i+1]['id'],
                'reason': 'duplicate',
                'confidence': similarity,
                'description': 'æ£€æµ‹åˆ°é‡å¤è¯­å¥'
            })
    
    return {'suggestions': suggestions, 'total': len(suggestions)}


def calculate_text_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆLevenshtein è·ç¦»ï¼‰"""
    from difflib import SequenceMatcher
    return SequenceMatcher(None, text1, text2).ratio()
```

### 20.2 å‰ç«¯æ™ºèƒ½å‰ªè¾‘ UI

```typescript
// src/components/editor/SmartCleanPanel.tsx
export const SmartCleanPanel = () => {
  const [suggestions, setSuggestions] = useState<Suggestion[]>([]);
  const { projectId, updateSegment } = useEditorStore();
  
  const runSmartClean = async () => {
    const result = await api.post(`/api/tasks/smart-clean`, {
      project_id: projectId,
      options: {
        max_silence_duration: 2.0,
        remove_filler_words: true,
        remove_duplicates: true
      }
    });
    
    setSuggestions(result.suggestions);
  };
  
  const applyAllSuggestions = () => {
    suggestions.forEach(sug => {
      if (sug.segment_id) {
        updateSegment(sug.segment_id, {
          is_deleted: true,
          delete_reason: sug.reason
        });
      }
    });
  };
  
  return (
    <div>
      <Button onClick={runSmartClean}>è¿è¡Œæ™ºèƒ½å‰ªè¾‘</Button>
      
      {suggestions.length > 0 && (
        <>
          <p>æ‰¾åˆ° {suggestions.length} ä¸ªå»ºè®®åˆ é™¤çš„ç‰‡æ®µ</p>
          <Button onClick={applyAllSuggestions}>å…¨éƒ¨åº”ç”¨</Button>
          
          <ul>
            {suggestions.map(sug => (
              <li key={sug.segment_id}>
                {sug.description} (ç½®ä¿¡åº¦: {(sug.confidence * 100).toFixed(0)}%)
              </li>
            ))}
          </ul>
        </>
      )}
    </div>
  );
};
```

---

## 21. ç¼“å­˜ç­–ç•¥è®¾è®¡

### 21.1 å¤šçº§ç¼“å­˜æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1: æµè§ˆå™¨å†…å­˜ (React State)        â”‚ TTL: Session    â”‚
â”‚ - å½“å‰é¡¹ç›®çš„ Timeline çŠ¶æ€           â”‚ å®¹é‡: ~10MB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L2: IndexedDB                       â”‚ TTL: 30 days    â”‚
â”‚ - é¡¹ç›®å†å²ç‰ˆæœ¬                       â”‚ å®¹é‡: ~50MB    â”‚
â”‚ - éŸ³é¢‘æ³¢å½¢æ•°æ®                       â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L3: Redis (æœåŠ¡å™¨ç«¯)                â”‚ TTL: 7 days     â”‚
â”‚ - API å“åº”ç¼“å­˜                       â”‚ å®¹é‡: 2GB      â”‚
â”‚ - éŸ³é¢‘æ³¢å½¢æ•°æ®                       â”‚                â”‚
â”‚ - ä»»åŠ¡çŠ¶æ€                          â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L4: CDN Edge Cache                  â”‚ TTL: 365 days   â”‚
â”‚ - é™æ€èµ„æºï¼ˆä»£ç†è§†é¢‘ã€é›ªç¢§å›¾ï¼‰        â”‚ å®¹é‡: Unlimitedâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L5: Object Storage (Supabase/S3)    â”‚ TTL: Permanent  â”‚
â”‚ - åŸå§‹è§†é¢‘æ–‡ä»¶                       â”‚                â”‚
â”‚ - å¯¼å‡ºçš„æˆå“è§†é¢‘                     â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 21.2 Redis ç¼“å­˜ç­–ç•¥

```python
# backend/app/services/cache.py
import redis
import json
from functools import wraps
from app.config import settings

redis_client = redis.Redis(
    host=settings.REDIS_HOST,
    port=settings.REDIS_PORT,
    db=0,
    decode_responses=True
)

def cache_api_response(ttl: int = 300):
    """
    è£…é¥°å™¨ï¼šç¼“å­˜ API å“åº”
    
    ç”¨æ³•:
    @cache_api_response(ttl=3600)
    async def get_project(project_id: str):
        ...
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜ key
            cache_key = f"api:{func.__name__}:{args}:{kwargs}"
            
            # å°è¯•ä»ç¼“å­˜è¯»å–
            cached = redis_client.get(cache_key)
            if cached:
                return json.loads(cached)
            
            # æ‰§è¡ŒåŸå‡½æ•°
            result = await func(*args, **kwargs)
            
            # å†™å…¥ç¼“å­˜
            redis_client.setex(
                cache_key,
                ttl,
                json.dumps(result, default=str)
            )
            
            return result
        
        return wrapper
    return decorator


# ä½¿ç”¨ç¤ºä¾‹
@router.get("/projects/{project_id}")
@cache_api_response(ttl=60)  # ç¼“å­˜ 1 åˆ†é’Ÿ
async def get_project(project_id: str):
    # ... æ•°æ®åº“æŸ¥è¯¢
    return project_data
```

### 21.3 æ³¢å½¢æ•°æ®ç¼“å­˜

```python
# backend/app/services/waveform_cache.py
import numpy as np
import redis
import pickle

class WaveformCache:
    def __init__(self):
        self.redis = redis_client
    
    def get_waveform(self, asset_id: str, resolution: int = 100) -> dict | None:
        """ä»ç¼“å­˜è·å–æ³¢å½¢æ•°æ®"""
        cache_key = f"waveform:{asset_id}:{resolution}"
        
        data = self.redis.get(cache_key)
        if data:
            return pickle.loads(data)
        
        return None
    
    def set_waveform(self, asset_id: str, waveform_data: dict, resolution: int = 100):
        """å­˜å‚¨æ³¢å½¢æ•°æ®åˆ° Redis"""
        cache_key = f"waveform:{asset_id}:{resolution}"
        
        # ä½¿ç”¨ pickle åºåˆ—åŒ– NumPy æ•°ç»„
        serialized = pickle.dumps(waveform_data)
        
        # è®¾ç½® 7 å¤©è¿‡æœŸ
        self.redis.setex(cache_key, 604800, serialized)
    
    def generate_and_cache(self, asset_id: str, audio_path: str, resolution: int = 100):
        """ç”Ÿæˆæ³¢å½¢å¹¶ç¼“å­˜"""
        import librosa
        
        # æ£€æŸ¥ç¼“å­˜
        cached = self.get_waveform(asset_id, resolution)
        if cached:
            return cached
        
        # ç”Ÿæˆæ³¢å½¢
        y, sr = librosa.load(audio_path, sr=16000, mono=False)
        
        hop_length = len(y[0]) // resolution
        rms_left = librosa.feature.rms(y=y[0], hop_length=hop_length)[0]
        rms_right = librosa.feature.rms(y=y[1], hop_length=hop_length)[0]
        
        max_val = max(rms_left.max(), rms_right.max())
        
        waveform_data = {
            "left": (rms_left / max_val).tolist(),
            "right": (rms_right / max_val).tolist(),
            "duration": len(y[0]) / sr,
            "sample_rate": sr
        }
        
        # å­˜å…¥ç¼“å­˜
        self.set_waveform(asset_id, waveform_data, resolution)
        
        return waveform_data
```

---

## 22. è§†é¢‘ä»£ç†ä¸é¢„è§ˆ

### 22.1 ä¸ºä»€ä¹ˆéœ€è¦ä»£ç†è§†é¢‘ï¼Ÿ

| é—®é¢˜ | åŸå§‹ 4K è§†é¢‘ | 720p ä»£ç†è§†é¢‘ |
|------|-------------|--------------|
| æ–‡ä»¶å¤§å° | 500 MB | 50 MB |
| æµè§ˆå™¨è§£ç æ€§èƒ½ | å¡é¡¿ï¼ˆ10 FPSï¼‰ | æµç•…ï¼ˆ60 FPSï¼‰ |
| ç½‘ç»œåŠ è½½æ—¶é—´ | 30 ç§’ | 3 ç§’ |
| é¢„è§ˆç²¾ç¡®åº¦ | å®Œç¾ | 99.9% |

**ç»“è®º**: ç¼–è¾‘æ—¶ä½¿ç”¨ä»£ç†è§†é¢‘ï¼Œå¯¼å‡ºæ—¶ä½¿ç”¨åŸå§‹è§†é¢‘ã€‚

### 22.2 è‡ªåŠ¨ç”Ÿæˆä»£ç†è§†é¢‘

```python
# backend/app/tasks/generate_proxy.py
@celery_app.task(bind=True)
def generate_proxy_video(self, asset_id: str):
    """
    ç”Ÿæˆç¼–è¾‘ä»£ç†è§†é¢‘ï¼š
    - åˆ†è¾¨ç‡: 720p (1280x720)
    - ç¼–ç : H.264
    - ç ç‡: 2 Mbps
    - å¸§ç‡: ä¿æŒåŸå§‹å¸§ç‡
    """
    try:
        # 1. ä¸‹è½½åŸå§‹è§†é¢‘
        supabase = get_supabase()
        asset = supabase.table('assets').select('*').eq('id', asset_id).single().execute()
        
        original_path = f"/tmp/{asset_id}_original.mp4"
        download_from_storage(asset.data['storage_path'], original_path)
        
        # 2. ä½¿ç”¨ FFmpeg ç”Ÿæˆä»£ç†è§†é¢‘
        self.update_state(state='PROCESSING', meta={'progress': 20})
        
        proxy_path = f"/tmp/{asset_id}_proxy.mp4"
        
        subprocess.run([
            'ffmpeg', '-i', original_path,
            '-vf', 'scale=-2:720',  # ä¿æŒå®½é«˜æ¯”ï¼Œé«˜åº¦å›ºå®š 720
            '-c:v', 'libx264',
            '-preset', 'fast',  # å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡
            '-crf', '23',  # è´¨é‡å‚æ•°ï¼ˆ18-28ï¼Œè¶Šå°è¶Šå¥½ï¼‰
            '-b:v', '2M',  # ç ç‡ 2 Mbps
            '-c:a', 'aac',
            '-b:a', '128k',
            '-movflags', '+faststart',  # ä¼˜åŒ–åœ¨çº¿æ’­æ”¾
            '-y',
            proxy_path
        ], check=True)
        
        # 3. ä¸Šä¼ ä»£ç†è§†é¢‘
        self.update_state(state='PROCESSING', meta={'progress': 80})
        
        proxy_storage_path = f"proxy/{asset_id}_720p.mp4"
        upload_to_storage(proxy_path, proxy_storage_path)
        
        proxy_url = get_public_url(proxy_storage_path)
        
        # 4. æ›´æ–° assets è¡¨
        supabase.table('assets').update({
            'proxy_url': proxy_url,
            'status': 'ready'
        }).eq('id', asset_id).execute()
        
        # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(original_path)
        os.remove(proxy_path)
        
        return {'proxy_url': proxy_url}
        
    except Exception as e:
        self.update_state(state='FAILURE', meta={'error': str(e)})
        raise
```

### 22.3 å‰ç«¯æ™ºèƒ½åˆ‡æ¢

```typescript
// src/hooks/useVideoSource.ts
export const useVideoSource = (assetId: string) => {
  const [videoUrl, setVideoUrl] = useState<string>('');
  const [isProxy, setIsProxy] = useState(true);
  
  useEffect(() => {
    const loadAsset = async () => {
      const asset = await api.get(`/api/assets/${assetId}`);
      
      // ä¼˜å…ˆä½¿ç”¨ä»£ç†è§†é¢‘ï¼ˆç¼–è¾‘æ¨¡å¼ï¼‰
      if (asset.proxy_url) {
        setVideoUrl(asset.proxy_url);
        setIsProxy(true);
      } else {
        // åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹è§†é¢‘
        setVideoUrl(asset.url);
        setIsProxy(false);
      }
    };
    
    loadAsset();
  }, [assetId]);
  
  // åˆ‡æ¢åˆ°åŸå§‹è§†é¢‘ï¼ˆé«˜è´¨é‡é¢„è§ˆï¼‰
  const switchToOriginal = async () => {
    const asset = await api.get(`/api/assets/${assetId}`);
    setVideoUrl(asset.url);
    setIsProxy(false);
  };
  
  return { videoUrl, isProxy, switchToOriginal };
};
```

---

## 23. éŸ³é¢‘æ³¢å½¢ç”Ÿæˆ

### 23.1 é«˜æ€§èƒ½æ³¢å½¢æ¸²æŸ“

```typescript
// src/components/editor/Waveform.tsx
import { useEffect, useRef } from 'react';

interface WaveformProps {
  assetId: string;
  width: number;
  height: number;
  color?: string;
}

export const Waveform = ({ assetId, width, height, color = '#4CAF50' }: WaveformProps) => {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  
  useEffect(() => {
    const loadAndRender = async () => {
      // 1. å°è¯•ä» IndexedDB è¯»å–ç¼“å­˜
      const db = await openDB('waveforms');
      let waveformData = await db.get('waveforms', assetId);
      
      // 2. å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œä» API è·å–
      if (!waveformData) {
        waveformData = await api.get(`/api/assets/${assetId}/waveform`);
        // å­˜å…¥ IndexedDB
        await db.put('waveforms', waveformData, assetId);
      }
      
      // 3. æ¸²æŸ“åˆ° Canvas
      renderWaveform(canvasRef.current!, waveformData, color);
    };
    
    loadAndRender();
  }, [assetId]);
  
  return <canvas ref={canvasRef} width={width} height={height} />;
};

function renderWaveform(
  canvas: HTMLCanvasElement,
  data: { left: number[]; right: number[] },
  color: string
) {
  const ctx = canvas.getContext('2d')!;
  const { width, height } = canvas;
  const halfHeight = height / 2;
  
  ctx.clearRect(0, 0, width, height);
  ctx.fillStyle = color;
  
  const barWidth = width / data.left.length;
  
  data.left.forEach((amplitude, i) => {
    const x = i * barWidth;
    const barHeight = amplitude * halfHeight;
    
    // ç»˜åˆ¶ä¸ŠåŠéƒ¨åˆ†ï¼ˆå·¦å£°é“ï¼‰
    ctx.fillRect(x, halfHeight - barHeight, barWidth - 1, barHeight);
    
    // ç»˜åˆ¶ä¸‹åŠéƒ¨åˆ†ï¼ˆå³å£°é“ï¼‰
    const rightAmplitude = data.right[i];
    const rightBarHeight = rightAmplitude * halfHeight;
    ctx.fillRect(x, halfHeight, barWidth - 1, rightBarHeight);
  });
}
```

### 23.2 æ³¢å½¢æ•°æ®å‹ç¼©

å¯¹äºé•¿æ—¶é—´éŸ³é¢‘ï¼ˆå¦‚ 1 å°æ—¶ï¼‰ï¼Œæ³¢å½¢æ•°æ®å¯èƒ½è¾¾åˆ°æ•° MBï¼Œéœ€è¦å‹ç¼©ï¼š

```python
# backend/app/services/waveform.py
def compress_waveform(waveform_data: dict, target_points: int = 2000) -> dict:
    """
    å°†é«˜åˆ†è¾¨ç‡æ³¢å½¢æ•°æ®å‹ç¼©åˆ°ç›®æ ‡ç‚¹æ•°
    
    ç¤ºä¾‹ï¼š
    - åŸå§‹: 100 points/sec Ã— 3600 sec = 360,000 points
    - å‹ç¼©å: 2000 points (é€‚åˆæ—¶é—´è½´æ˜¾ç¤º)
    """
    left = np.array(waveform_data['left'])
    right = np.array(waveform_data['right'])
    
    # è®¡ç®—å‹ç¼©æ¯”
    ratio = len(left) / target_points
    
    if ratio <= 1:
        return waveform_data  # æ— éœ€å‹ç¼©
    
    # åˆ†æ®µå–æœ€å¤§å€¼ï¼ˆä¿ç•™å³°å€¼ä¿¡æ¯ï¼‰
    compressed_left = []
    compressed_right = []
    
    for i in range(target_points):
        start = int(i * ratio)
        end = int((i + 1) * ratio)
        
        compressed_left.append(float(left[start:end].max()))
        compressed_right.append(float(right[start:end].max()))
    
    return {
        'left': compressed_left,
        'right': compressed_right,
        'original_length': len(left),
        'compression_ratio': ratio
    }
```

---

## 24. CDN ä¸åŠ é€Ÿ

### 24.1 Cloudflare CDN é…ç½®

```typescript
// src/lib/cdn.ts
const CDN_ENDPOINTS = {
  video: 'https://video-cdn.hoppingrabbit.ai',
  audio: 'https://audio-cdn.hoppingrabbit.ai',
  static: 'https://static-cdn.hoppingrabbit.ai'
};

export const getCDNUrl = (path: string, type: 'video' | 'audio' | 'static') => {
  const endpoint = CDN_ENDPOINTS[type];
  return `${endpoint}/${path}`;
};

// ä½¿ç”¨ç¤ºä¾‹
const proxyVideoUrl = getCDNUrl('proxy/abc123_720p.mp4', 'video');
```

### 24.2 Range Request æ”¯æŒï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰

```python
# backend/app/api/routes.py
from fastapi.responses import StreamingResponse
from starlette.requests import Request

@router.get("/stream/{asset_id}")
async def stream_video(asset_id: str, request: Request):
    """
    æ”¯æŒ HTTP Range è¯·æ±‚çš„è§†é¢‘æµåª’ä½“æœåŠ¡
    å…è®¸ç”¨æˆ·æ‹–åŠ¨è¿›åº¦æ¡æ—¶å¿«é€Ÿè·³è½¬
    """
    asset = await db.fetch_one("SELECT * FROM assets WHERE id = $1", asset_id)
    file_path = get_file_path(asset['storage_path'])
    file_size = os.path.getsize(file_path)
    
    # è§£æ Range header
    range_header = request.headers.get('Range')
    
    if not range_header:
        # å®Œæ•´æ–‡ä»¶
        return StreamingResponse(
            open(file_path, 'rb'),
            media_type='video/mp4'
        )
    
    # è§£æ Range: bytes=0-1023
    start, end = parse_range_header(range_header, file_size)
    
    def file_chunk_generator():
        with open(file_path, 'rb') as f:
            f.seek(start)
            remaining = end - start + 1
            chunk_size = 8192
            
            while remaining > 0:
                chunk = f.read(min(chunk_size, remaining))
                if not chunk:
                    break
                remaining -= len(chunk)
                yield chunk
    
    return StreamingResponse(
        file_chunk_generator(),
        status_code=206,  # Partial Content
        headers={
            'Content-Range': f'bytes {start}-{end}/{file_size}',
            'Accept-Ranges': 'bytes',
            'Content-Length': str(end - start + 1)
        },
        media_type='video/mp4'
    )
```

### 24.3 é¢„åŠ è½½ç­–ç•¥

```typescript
// src/hooks/useVideoPreload.ts
export const useVideoPreload = (clips: Clip[]) => {
  useEffect(() => {
    // é¢„åŠ è½½æ¥ä¸‹æ¥ 3 ä¸ªç‰‡æ®µçš„ä»£ç†è§†é¢‘
    const currentClipIndex = /* ... */;
    const nextClips = clips.slice(currentClipIndex + 1, currentClipIndex + 4);
    
    nextClips.forEach(clip => {
      const link = document.createElement('link');
      link.rel = 'prefetch';
      link.href = clip.proxy_url;
      link.as = 'video';
      document.head.appendChild(link);
    });
  }, [clips]);
};
```

---

## 25. Docker å®¹å™¨åŒ–

### 25.1 å®Œæ•´çš„ Docker Compose é…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  # ========== å‰ç«¯ ==========
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_WS_URL=ws://localhost:8001
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - backend

  # ========== åç«¯ API ==========
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/hoppingrabbit
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672//
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_KEY=${SUPABASE_KEY}
    volumes:
      - ./backend:/app
      - model-cache:/root/.cache  # ç¼“å­˜ AI æ¨¡å‹
    depends_on:
      - db
      - redis
      - rabbitmq

  # ========== Celery Worker (CPU ä»»åŠ¡) ==========
  celery-worker-cpu:
    build:
      context: ./backend
      dockerfile: Dockerfile
    command: celery -A app.tasks worker --loglevel=info --queues=cpu --concurrency=4
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/hoppingrabbit
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672//
    volumes:
      - ./backend:/app
      - /tmp:/tmp  # ä¸´æ—¶æ–‡ä»¶
    depends_on:
      - rabbitmq
      - redis

  # ========== Celery Worker (GPU ä»»åŠ¡) ==========
  celery-worker-gpu:
    build:
      context: ./backend
      dockerfile: Dockerfile.gpu
    command: celery -A app.tasks worker --loglevel=info --queues=gpu --concurrency=1
    runtime: nvidia  # éœ€è¦ nvidia-docker
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/hoppingrabbit
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672//
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./backend:/app
      - model-cache:/root/.cache
      - /tmp:/tmp
    depends_on:
      - rabbitmq
      - redis
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ========== PostgreSQL ==========
  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=hoppingrabbit
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./supabase/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql

  # ========== Redis ==========
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

  # ========== RabbitMQ ==========
  rabbitmq:
    image: rabbitmq:3-management-alpine
    ports:
      - "5672:5672"
      - "15672:15672"  # ç®¡ç†ç•Œé¢
    environment:
      - RABBITMQ_DEFAULT_USER=guest
      - RABBITMQ_DEFAULT_PASS=guest
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq

  # ========== Nginx (åå‘ä»£ç†) ==========
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - frontend
      - backend

volumes:
  postgres-data:
  redis-data:
  rabbitmq-data:
  model-cache:  # å…±äº« AI æ¨¡å‹ç¼“å­˜
```

### 25.2 GPU ä¼˜åŒ–çš„ Dockerfile

```dockerfile
# backend/Dockerfile.gpu
FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04

# å®‰è£… Python
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    ffmpeg \
    libsndfile1 \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# å®‰è£… CUDA åŠ é€Ÿç‰ˆæœ¬çš„åº“
RUN pip3 install --no-cache-dir \
    torch==2.1.0+cu121 \
    torchaudio==2.1.0+cu121 \
    --index-url https://download.pytorch.org/whl/cu121

# å¤åˆ¶ä»£ç 
COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## 26. Celery ä»»åŠ¡é˜Ÿåˆ—

### 26.1 é˜Ÿåˆ—åˆ†ç¦»ç­–ç•¥

```python
# backend/app/tasks/__init__.py
from celery import Celery

celery_app = Celery(
    'hoppingrabbit',
    broker='amqp://guest:guest@localhost:5672//',
    backend='redis://localhost:6379/0'
)

# ========== é˜Ÿåˆ—é…ç½® ==========
celery_app.conf.task_routes = {
    # CPU å¯†é›†å‹ä»»åŠ¡ -> cpu é˜Ÿåˆ—
    'app.tasks.export.export_video': {'queue': 'cpu'},
    'app.tasks.export.generate_proxy': {'queue': 'cpu'},
    'app.tasks.export.generate_sprite': {'queue': 'cpu'},
    'app.tasks.export.generate_waveform': {'queue': 'cpu'},
    
    # GPU å¯†é›†å‹ä»»åŠ¡ -> gpu é˜Ÿåˆ—
    'app.tasks.transcribe.transcribe_video': {'queue': 'gpu'},
    'app.tasks.stem_separation.separate_stems': {'queue': 'gpu'},
    'app.tasks.diarization.speaker_diarization': {'queue': 'gpu'},
}

# ========== é€Ÿç‡é™åˆ¶ ==========
celery_app.conf.task_annotations = {
    # GPU ä»»åŠ¡é™æµï¼ˆé˜²æ­¢æ˜¾å­˜æº¢å‡ºï¼‰
    'app.tasks.transcribe.*': {'rate_limit': '2/m'},  # æ¯åˆ†é’Ÿæœ€å¤š 2 ä¸ª
    'app.tasks.stem_separation.*': {'rate_limit': '1/m'},  # æ¯åˆ†é’Ÿæœ€å¤š 1 ä¸ª
    
    # CPU ä»»åŠ¡
    'app.tasks.export.*': {'rate_limit': '10/m'},
}

# ========== è¶…æ—¶è®¾ç½® ==========
celery_app.conf.task_time_limit = 3600  # 1 å°æ—¶ç¡¬è¶…æ—¶
celery_app.conf.task_soft_time_limit = 3300  # 55 åˆ†é’Ÿè½¯è¶…æ—¶
```

### 26.2 ä»»åŠ¡ä¼˜å…ˆçº§

```python
# é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆå®æ—¶é¢„è§ˆï¼‰
@celery_app.task(priority=9)
def generate_thumbnail(asset_id: str):
    pass

# ä¸­ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆå¸¸è§„è½¬å†™ï¼‰
@celery_app.task(priority=5)
def transcribe_video(asset_id: str):
    pass

# ä½ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆåå°å¯¼å‡ºï¼‰
@celery_app.task(priority=1)
def export_video(export_id: str):
    pass
```

### 26.3 ä»»åŠ¡é‡è¯•ä¸é”™è¯¯å¤„ç†

```python
@celery_app.task(
    bind=True,
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError),
    retry_backoff=True,  # æŒ‡æ•°é€€é¿
    retry_backoff_max=600,  # æœ€å¤§é€€é¿ 10 åˆ†é’Ÿ
    retry_jitter=True  # åŠ å…¥éšæœºæ€§é¿å…é›·é¸£ç¾Šç¾¤
)
def transcribe_video(self, asset_id: str, options: dict):
    try:
        # ... ä»»åŠ¡é€»è¾‘
        pass
    except OutOfMemoryError as e:
        # æ˜¾å­˜ä¸è¶³ï¼Œè®°å½•é”™è¯¯ä½†ä¸é‡è¯•
        logger.error(f"GPU OOM for asset {asset_id}")
        raise self.retry(exc=e, countdown=300)  # 5 åˆ†é’Ÿåé‡è¯•
    except Exception as e:
        # å…¶ä»–é”™è¯¯ï¼Œæ ‡è®°ä»»åŠ¡å¤±è´¥
        update_task_status(self.request.id, 'failed', error=str(e))
        raise
```

---

## 27. ç›‘æ§ä¸å‘Šè­¦

### 27.1 Prometheus + Grafana é›†æˆ

```yaml
# docker-compose.monitoring.yml
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'

  grafana:
    image: grafana/grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    depends_on:
      - prometheus

  # Celery ç›‘æ§
  flower:
    build:
      context: ./backend
    command: celery -A app.tasks flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672//
    depends_on:
      - rabbitmq
```

### 27.2 è‡ªå®šä¹‰æŒ‡æ ‡æš´éœ²

```python
# backend/app/monitoring.py
from prometheus_client import Counter, Histogram, Gauge
from fastapi import Request
import time

# ========== å®šä¹‰æŒ‡æ ‡ ==========
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

ACTIVE_TASKS = Gauge(
    'celery_active_tasks',
    'Number of active Celery tasks',
    ['queue', 'task_type']
)

# ========== ä¸­é—´ä»¶ ==========
@app.middleware("http")
async def prometheus_middleware(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    REQUEST_LATENCY.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)
    
    return response

# ========== æš´éœ²ç«¯ç‚¹ ==========
from prometheus_client import make_asgi_app

metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)
```

### 27.3 å…³é”®å‘Šè­¦è§„åˆ™

```yaml
# prometheus/alerts.yml
groups:
  - name: hoppingrabbit_alerts
    interval: 30s
    rules:
      # API é”™è¯¯ç‡è¿‡é«˜
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        annotations:
          summary: "API é”™è¯¯ç‡è¶…è¿‡ 5%"
      
      # GPU ä»»åŠ¡é˜Ÿåˆ—ç§¯å‹
      - alert: GPUQueueBacklog
        expr: |
          celery_queue_length{queue="gpu"} > 10
        for: 10m
        annotations:
          summary: "GPU ä»»åŠ¡é˜Ÿåˆ—ç§¯å‹è¶…è¿‡ 10 ä¸ª"
      
      # æ•°æ®åº“è¿æ¥æ± è€—å°½
      - alert: DatabaseConnectionPoolExhausted
        expr: |
          pg_stat_activity_count > 90
        for: 5m
        annotations:
          summary: "æ•°æ®åº“è¿æ¥æ•°æ¥è¿‘ä¸Šé™"
      
      # Redis å†…å­˜ä½¿ç”¨è¿‡é«˜
      - alert: RedisMemoryHigh
        expr: |
          redis_memory_used_bytes / redis_memory_max_bytes > 0.8
        for: 5m
        annotations:
          summary: "Redis å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡ 80%"
```

---

## 28. æ‰©å±•æ€§è®¾è®¡

### 28.1 æ°´å¹³æ‰©å±•æ–¹æ¡ˆ

```
                      Load Balancer (Nginx)
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                    â–¼                    â–¼
  Backend API 1       Backend API 2       Backend API 3
  (Stateless)         (Stateless)         (Stateless)
        â”‚                    â”‚                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                         â–¼
         PostgreSQL                   Redis Cluster
         (Primary + Replicas)         (Sharded)
                â”‚
                â–¼
          RabbitMQ Cluster
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼
   CPU Workers      GPU Workers
   (Auto-scale)     (Fixed)
```

### 28.2 æ•°æ®åº“è¯»å†™åˆ†ç¦»

```python
# backend/app/database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.asyncio import create_async_engine

# ä¸»åº“ï¼ˆå†™ï¼‰
engine_write = create_async_engine(
    "postgresql+asyncpg://user:pass@primary:5432/hoppingrabbit"
)

# ä»åº“ï¼ˆè¯»ï¼‰
engine_read = create_async_engine(
    "postgresql+asyncpg://user:pass@replica:5432/hoppingrabbit"
)

class Database:
    @staticmethod
    async def execute_write(query: str, *args):
        async with engine_write.begin() as conn:
            return await conn.execute(query, *args)
    
    @staticmethod
    async def execute_read(query: str, *args):
        async with engine_read.begin() as conn:
            return await conn.execute(query, *args)
```

### 28.3 GPU èµ„æºæ± ç®¡ç†

```python
# backend/app/gpu_pool.py
import asyncio
from contextlib import asynccontextmanager

class GPUPool:
    def __init__(self, num_gpus: int = 2):
        self.semaphores = {
            i: asyncio.Semaphore(1)  # æ¯ä¸ª GPU åŒæ—¶åªèƒ½è·‘ 1 ä¸ªä»»åŠ¡
            for i in range(num_gpus)
        }
    
    @asynccontextmanager
    async def acquire(self):
        """è·å–ä¸€ä¸ªå¯ç”¨çš„ GPU"""
        # è½®è¯¢æ‰¾åˆ°ç©ºé—²çš„ GPU
        for gpu_id, sem in self.semaphores.items():
            if not sem.locked():
                async with sem:
                    os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
                    yield gpu_id
                    return
        
        # æ‰€æœ‰ GPU éƒ½å¿™ï¼Œç­‰å¾…ç¬¬ä¸€ä¸ªç©ºé—²çš„
        tasks = [sem.acquire() for sem in self.semaphores.values()]
        done, pending = await asyncio.wait(
            tasks,
            return_when=asyncio.FIRST_COMPLETED
        )
        
        # å–æ¶ˆå…¶ä»–ç­‰å¾…
        for p in pending:
            p.cancel()
        
        gpu_id = list(self.semaphores.keys())[list(done)[0]]
        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        yield gpu_id

# ä½¿ç”¨ç¤ºä¾‹
gpu_pool = GPUPool(num_gpus=2)

@celery_app.task
async def transcribe_video(asset_id: str):
    async with gpu_pool.acquire() as gpu_id:
        logger.info(f"Using GPU {gpu_id} for transcription")
        # ... è¿è¡Œ Whisper
```

---

## é™„å½•A: é”™è¯¯ç è§„èŒƒ

```typescript
// src/types/errors.ts
export enum ErrorCode {
  // 4xx å®¢æˆ·ç«¯é”™è¯¯
  INVALID_REQUEST = 'INVALID_REQUEST',
  UNAUTHORIZED = 'UNAUTHORIZED',
  FORBIDDEN = 'FORBIDDEN',
  NOT_FOUND = 'NOT_FOUND',
  CONFLICT = 'CONFLICT',
  
  // 5xx æœåŠ¡å™¨é”™è¯¯
  INTERNAL_ERROR = 'INTERNAL_ERROR',
  DATABASE_ERROR = 'DATABASE_ERROR',
  STORAGE_ERROR = 'STORAGE_ERROR',
  
  // ä¸šåŠ¡é”™è¯¯
  UPLOAD_FAILED = 'UPLOAD_FAILED',
  TRANSCRIPTION_FAILED = 'TRANSCRIPTION_FAILED',
  EXPORT_FAILED = 'EXPORT_FAILED',
  GPU_OUT_OF_MEMORY = 'GPU_OUT_OF_MEMORY',
  TASK_TIMEOUT = 'TASK_TIMEOUT',
  
  // åŒæ­¥é”™è¯¯
  VERSION_CONFLICT = 'VERSION_CONFLICT',
  SYNC_FAILED = 'SYNC_FAILED',
}

export interface APIError {
  code: ErrorCode;
  message: string;
  details?: any;
  timestamp: string;
}
```

---

## é™„å½•B: WebSocket æ¶ˆæ¯åè®®

```typescript
// å®¢æˆ·ç«¯ -> æœåŠ¡å™¨
interface WSMessage {
  type: 'subscribe' | 'unsubscribe' | 'ping';
  payload: {
    project_id?: string;
    task_id?: string;
  };
}

// æœåŠ¡å™¨ -> å®¢æˆ·ç«¯
interface WSNotification {
  type: 'task_progress' | 'task_completed' | 'project_updated';
  payload: {
    task_id?: string;
    progress?: number;
    result?: any;
    error?: string;
  };
  timestamp: string;
}

// ä½¿ç”¨ç¤ºä¾‹
const ws = new WebSocket('ws://localhost:8001');

ws.send(JSON.stringify({
  type: 'subscribe',
  payload: { project_id: 'abc123' }
}));

ws.onmessage = (event) => {
  const notification: WSNotification = JSON.parse(event.data);
  
  if (notification.type === 'task_completed') {
    toast.success('ä»»åŠ¡å®Œæˆï¼');
    loadTaskResult(notification.payload.task_id);
  }
};
```

---

## é™„å½•C: Timeline JSON Schema

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "version": { "type": "number" },
    "duration": { "type": "number" },
    "tracks": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "id": { "type": "string" },
          "type": { "enum": ["video", "audio", "subtitle"] },
          "name": { "type": "string" },
          "layer": { "type": "number" },
          "muted": { "type": "boolean" },
          "solo": { "type": "boolean" }
        },
        "required": ["id", "type"]
      }
    },
    "clips": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "id": { "type": "string" },
          "trackId": { "type": "string" },
          "assetId": { "type": "string" },
          "start": { "type": "number" },
          "duration": { "type": "number" },
          "trimStart": { "type": "number" },
          "trimEnd": { "type": "number" },
          "effects": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "type": { "type": "string" },
                "params": { "type": "object" }
              }
            }
          }
        },
        "required": ["id", "trackId", "assetId", "start", "duration"]
      }
    }
  }
}
```

---

## é™„å½•D: æ€§èƒ½æŒ‡æ ‡ SLA

| æ“ä½œ | P50 | P95 | P99 | ç›®æ ‡ |
|------|-----|-----|-----|------|
| **API è¯·æ±‚** |
| GET /projects | 50ms | 100ms | 200ms | < 100ms (P95) |
| PATCH /projects/state | 80ms | 150ms | 300ms | < 200ms (P95) |
| POST /upload | 10s | 30s | 60s | < 30s (P95) |
| **ä»»åŠ¡å¤„ç†** |
| ASR (1åˆ†é’Ÿè§†é¢‘) | 15s | 30s | 45s | < 30s (P95) |
| äººå£°åˆ†ç¦» (5åˆ†é’Ÿ) | 60s | 120s | 180s | < 120s (P95) |
| å¯¼å‡º 1080p (5åˆ†é’Ÿ) | 120s | 180s | 300s | < 180s (P95) |
| **å‰ç«¯æ€§èƒ½** |
| é¦–å±åŠ è½½ (TTI) | 1.5s | 2.5s | 4s | < 2.5s (P95) |
| æ‹–æ‹½æ“ä½œå»¶è¿Ÿ | 16ms | 32ms | 50ms | < 32ms (P95) |
| æ—¶é—´è½´æ¸²æŸ“å¸§ç‡ | 60 FPS | 50 FPS | 30 FPS | > 50 FPS (P95) |

---

## æ€»ç»“ä¸å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€åŠŸèƒ½ï¼ˆ2å‘¨ï¼‰
- [x] å‰ç«¯ç¼–è¾‘å™¨åŸºç¡€ UI
- [ ] åç«¯ API æ¡†æ¶æ­å»º
- [ ] æ•°æ®åº“è¡¨åˆ›å»º
- [ ] åŸºç¡€ä¸Šä¼ åŠŸèƒ½

### Phase 2: æ ¸å¿ƒ AI åŠŸèƒ½ï¼ˆ4å‘¨ï¼‰
- [ ] Whisper ASR é›†æˆ
- [ ] äººå£°åˆ†ç¦»é›†æˆ
- [ ] æ³¢å½¢æ•°æ®ç”Ÿæˆ
- [ ] ä»£ç†è§†é¢‘ç”Ÿæˆ

### Phase 3: é«˜çº§åŠŸèƒ½ï¼ˆ3å‘¨ï¼‰
- [ ] è¯´è¯äººåˆ†ç¦»
- [ ] æ™ºèƒ½å‰ªè¾‘ç®—æ³•
- [ ] è§†é¢‘å¯¼å‡ºæ¸²æŸ“
- [ ] æ¯«ç§’çº§è‡ªåŠ¨ä¿å­˜

### Phase 4: æ€§èƒ½ä¼˜åŒ–ï¼ˆ2å‘¨ï¼‰
- [ ] Redis ç¼“å­˜
- [ ] CDN é›†æˆ
- [ ] æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
- [ ] å‰ç«¯æ‡’åŠ è½½

### Phase 5: ç”Ÿäº§éƒ¨ç½²ï¼ˆ1å‘¨ï¼‰
- [ ] Docker éƒ¨ç½²
- [ ] ç›‘æ§å‘Šè­¦
- [ ] å¤‡ä»½ç­–ç•¥
- [ ] è´Ÿè½½æµ‹è¯•

---

**æ–‡æ¡£ç¼–å†™å®Œæˆæ—¥æœŸ**: 2026å¹´1æœˆ6æ—¥  
**é¢„è®¡å®æ–½å‘¨æœŸ**: 12 å‘¨  
**å›¢é˜Ÿé…ç½®å»ºè®®**: 2 å‰ç«¯ + 2 åç«¯ + 1 DevOps

---

*æœ¬æ–‡æ¡£æ˜¯ä¸€ä¸ªåŠ¨æ€æ–‡ä»¶ï¼Œåº”éšç€é¡¹ç›®æ¼”è¿›æŒç»­æ›´æ–°ã€‚*
