# 一键AI成片系统设计 (AI Video Creator Design)

## 1. 概述
本设计旨在实现“一键AI成片”功能，通过多模态分析用户上传的原始素材，自动完成剪辑、特效添加、镜头缩放与运镜（Camera Movement），从而极大地降低视频制作门槛。

核心目标：
1. **细粒度切片**：基于语音（VAD）和画面（场景检测）将长视频精确切分为有意义的短片段（Clips）。
2. **智能运镜**：分析画面主体与显著性区域，自动生成缩放（Zoom In/Out）、平移（Pan）等关键帧动画。
3. **特效匹配**：根据音频节奏和内容情绪，自动添加转场、文字动画和视觉特效。

## 2. 核心流程 (Pipeline)

整个处理流程分为四个阶段：**输入预处理 -> 内容理解与切片 -> 决策引擎 -> 效果合成**。

### 阶段一：输入预处理 & 基础分析
- **输入**：用户上传的一个或多个视频/音频文件。
- **转码与规范化**：确保所有素材统一帧率与格式。
- **基础特征提取**：
    - **ASR (语音识别)**：提取带时间戳的逐字字幕。
    - **VAD (静音检测)**：识别有效语音片段，去除长时间静音。
    - **Shot Boundary Detection (镜头检测)**：如果素材已是成片，检测物理镜头切换点。

### 阶段二：智能切片 (Smart Segmentation)
- **策略**：
    1. **语音驱动切片**：以句子为单位切分 Clip。每个 Clip 对应一句完整的台词。
    2. **视觉辅助**：确保切分点不会破坏画面连贯性（如避免只有几帧的闪烁片段）。
    3. **去噪/去废片**：移除无声、画面模糊或无效的片段。
- **输出**：一个包含多个 `Pre-Clip` 的初步时间轴序列。

### 阶段三：多模态内容分析 (Multimodal Analysis)
对于每一个切分好的 Clip，进行深度分析：
1. **主体识别 (Subject Detection)**：
    - 使用轻量级模型 (如 YOLO 或 MediaPipe) 识别画面中的人脸、人体或主要物体。
    - 获取主体在画面中的坐标 (Bounding Box)。
2. **显著性检测 (Saliency Detection)**：
    - 如果没有明确主体，计算画面的视觉显著图 (Saliency Map)，确定观众视线焦点。
3. **情绪与语义分析 (可选/高级)**：
    - 通过 LLM 分析 ASR 文本，判断该片段的情绪（激动、悲伤、平静）。
    - 识别关键词（“看这里”、“非常重要”），作为特效触发的信号。

### 阶段四：决策引擎 (Decision Engine)
基于分析结果，为每个 Clip 生成 `Effect Plan`（特效计划）：

#### 4.1 智能运镜规则 (Camera Movement Rules)
*目标：通过调整 `Clip.transform` 属性生成动态效果*

| 场景特点 | 运镜策略 | 参数示例 (Keyframes) |
|:---|:---|:---|
| **人物演讲 (高重点)** | **Face Zoom (脸部特写)** | Start: Scale 1.0 -> End: Scale 1.3, Center on Face |
| **人物演讲 (普通)** | **Slow Zoom In (缓慢推进)** | Start: Scale 1.0 -> End: Scale 1.1 (呼吸感) |
| **转换话题/转场** | **Zoom Out (拉远)** | Start: Scale 1.1 -> End: Scale 1.0 |
| **展示物体** | **Pan to Subject (平移跟随)** | 保持 Scale > 1.0，根据物体位移调整 Position X/Y |
| **静态图片/PPT** | **Ken Burns Effect** | 随机缓慢推拉摇移，打破静止感 |
| **激昂/强调片段** | **Rapid Zoom (快速冲击)** | 短时间内 Scale 1.0 -> 1.5 (配合 Elastic 曲线) |

#### 4.2 视觉特效与转场
- **文字动画**：检测到重点关键词时，在 ASSET 图层叠加 Highlight 文字或动态字幕。
- **转场 (Transition)**：在大部分切片间使用硬切（Hard Cut），在场景/话题转换处使用“叠化”或“擦除”。

### 阶段五：工程化落地 (Output Generation)
- 将上述决策转换为 `hoppingrabbit-ai` 的数据结构：
    - 新增/更新 `Clip` 对象。
    - 填充 `Clip.transform` 字段（包含 start/end keyframes）。
    - 插入 `Effect` 类型的 Clip 到上方 Video Track。
    - 保存为 `Project` 数据，前端直接渲染。

## 3. 数据结构扩展

### Clip.transform 扩展
我们需要在 `Clip` 模型中支持关键帧动画数据。

```json
{
  "transform": {
    "enable_animation": true,
    "keyframes": [
      {
        "time_offset": 0, // 相对于 Clip 开始的偏移（毫秒）
        "scale": 1.0,
        "position_x": 0.0,
        "position_y": 0.0,
        "rotation": 0
      },
      {
        "time_offset": 5000, 
        "scale": 1.3,
        "position_x": 0.1, // 归一化坐标，向右平移 10%
        "position_y": -0.05, // 向上平移 5%
        "easing": "ease-out" // 缓动函数
      }
    ]
  }
}
```

## 4. 所需 AI 模型与 API 需求

为了实现上述功能，建议集成以下能力：

| 功能 | 推荐模型/库 | 部署方式 | 备注 |
|:---|:---|:---|:---|
| **VAD (静音检测)** | Silero VAD | 本地 (CPU/GPU) | 已有类似基础，需优化精度 |
| **ASR (语音转文字)** | **Volcengine Doubao (豆包)** | 云端 API | **已确认支持**: 字级时间戳 (Word-level timestamps), 说话人分离 (Diarization) |
| **人脸/主体检测** | MediaPipe / YOLOv8-nano | 本地 (CPU/ONNX) | 用于计算运镜中心点 (云端视频分析成本过高，建议本地) |
| **显著性检测** | U^2-Net (Lite版) | 本地 (GPU) | 辅助无主体画面的运镜 |
| **语义理解(LLM)** | **Volcengine Doubao (豆包)** | 云端 API | **需接入**: 火山方舟 (Ark) 接口，用于情绪分析与脚本理解 |

## 5. 开发路线图
1.  **Phase 1**: 能够基于 VAD 将视频自动切碎，并移除静音部分。(Auto-Cut)
2.  **Phase 2**: 集成主体检测，实现简单的 "Center Crop" 或 "Smart Refrain"，保证切片后人物居中。
3.  **Phase 3**: 实现关键帧插值逻辑，添加 "Slow Zoom" 效果。
4.  **Phase 4**: 接入 LLM，实现基于语义的特效触发 (如：检测到“但是”，添加对立音效或转场)。
