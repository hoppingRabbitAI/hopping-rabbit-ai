"""
HoppingRabbit AI - å·¥ä½œå° API
å¤„ç†ä»ä¸Šä¼ åˆ°è¿›å…¥ç¼–è¾‘å™¨çš„å®Œæ•´æµç¨‹
é€‚é…æ–°è¡¨ç»“æ„ (2026-01-07)
"""
import logging
from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from pydantic import BaseModel
from typing import Optional, Literal, List, Dict, Any
from datetime import datetime
from uuid import uuid4
from enum import Enum

from ..services.supabase_client import supabase, get_file_url, create_signed_upload_url
from ..services.transform_rules import SegmentContext, transform_engine, sequence_processor, EmotionType, ImportanceLevel, TransformParams, ZoomStrategy
from .auth import get_current_user

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/workspace", tags=["Workspace"])


# ============================================
# æ•°æ®æ¨¡å‹
# ============================================

class TaskType(str, Enum):
    AI_CLIPS = "clips"      # AI æ™ºèƒ½åˆ‡ç‰‡
    SUMMARY = "summary"     # å†…å®¹æ€»ç»“
    AI_CREATE = "ai-create" # â˜… ä¸€é”®æˆç‰‡
    VOICE_EXTRACT = "voice-extract" # â˜… ä»…æå–å­—å¹•/éŸ³é¢‘


class SourceType(str, Enum):
    LOCAL = "local"         # æœ¬åœ°ä¸Šä¼ 
    YOUTUBE = "youtube"     # YouTube é“¾æ¥
    URL = "url"             # å…¶ä»– URL


# â˜… ProcessingStepsConfig å·²åˆ é™¤
# ä¸€é”®æˆç‰‡ç”± task_type == 'ai-create' å†³å®šï¼ŒASR é»˜è®¤å¼€å¯


class FileInfo(BaseModel):
    """å•ä¸ªæ–‡ä»¶ä¿¡æ¯ï¼ˆå¤šæ–‡ä»¶ä¸Šä¼ ï¼‰"""
    name: str
    size: int
    content_type: str
    duration: Optional[float] = None  # è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
    order_index: int = 0


class CreateSessionRequest(BaseModel):
    """åˆ›å»ºå¤„ç†ä¼šè¯è¯·æ±‚"""
    source_type: SourceType
    task_type: TaskType = TaskType.AI_CLIPS
    
    # === å•æ–‡ä»¶ä¸Šä¼ ï¼ˆå‘åå…¼å®¹ï¼‰===
    file_name: Optional[str] = None
    file_size: Optional[int] = None
    content_type: Optional[str] = None
    duration: Optional[float] = None  # è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œå‰ç«¯æœ¬åœ°æå–
    
    # === å¤šæ–‡ä»¶ä¸Šä¼ ï¼ˆæ–°å¢ï¼‰===
    files: Optional[List[FileInfo]] = None  # å¤šä¸ªæ–‡ä»¶ä¿¡æ¯
    
    # é“¾æ¥è§£æç›¸å…³
    source_url: Optional[str] = None
    # â˜… processing_steps å·²åˆ é™¤ï¼Œç”± task_type å†³å®šå¤„ç†æµç¨‹


class AssetUploadInfo(BaseModel):
    """å•ä¸ªèµ„æºçš„ä¸Šä¼ ä¿¡æ¯"""
    asset_id: str
    upload_url: str
    storage_path: str
    order_index: int  # ä¿æŒä¸å‰ç«¯ä¸€è‡´
    file_name: str


class CreateSessionResponse(BaseModel):
    """åˆ›å»ºä¼šè¯å“åº”"""
    session_id: str
    project_id: str
    # â˜… ç»Ÿä¸€ç”¨ assets æ•°ç»„ï¼ˆå³ä½¿å•æ–‡ä»¶ä¹Ÿæ˜¯ä¸€ä¸ªå…ƒç´ çš„æ•°ç»„ï¼‰
    assets: Optional[List[AssetUploadInfo]] = None


class SessionStatus(BaseModel):
    """ä¼šè¯çŠ¶æ€"""
    session_id: str
    project_id: str
    status: Literal["uploading", "processing", "completed", "failed", "cancelled", "expired"]
    current_step: Optional[str] = None
    progress: int = 0
    steps: List[dict] = []  # å¤„ç†æ­¥éª¤åˆ—è¡¨
    error: Optional[str] = None
    transcript_segments: Optional[int] = None
    marked_clips: Optional[int] = None
    
    # === å¤šæ–‡ä»¶ä¸Šä¼ è¿›åº¦ï¼ˆæ–°å¢ï¼‰===
    upload_progress: Optional[dict] = None  # {total_files, completed_files, ...}


# â˜… æ­¥éª¤å®šä¹‰å·²ç§»è‡³å‰ç«¯ ProcessingView.tsx
# åç«¯åªè¿”å› current_stepï¼Œå‰ç«¯æœ¬åœ°ç”Ÿæˆæ­¥éª¤åˆ—è¡¨
# è¿™æ ·é¿å…å‰åç«¯ç»´æŠ¤ä¸¤å¥—é‡å¤çš„æ­¥éª¤å®šä¹‰


def _get_file_type(content_type: str) -> str:
    """æ ¹æ® MIME ç±»å‹åˆ¤æ–­æ–‡ä»¶ç±»å‹"""
    if not content_type:
        return "video"
    if content_type.startswith("video/"):
        return "video"
    elif content_type.startswith("audio/"):
        return "audio"
    elif content_type.startswith("image/"):
        return "image"
    else:
        return "video"


# ============================================
# API ç«¯ç‚¹
# ============================================

@router.post("/sessions", response_model=CreateSessionResponse)
async def create_session(
    request: CreateSessionRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    åˆ›å»ºå¤„ç†ä¼šè¯ (æ­¥éª¤1: ä»…ä¸Šä¼ ï¼Œä¸æ‰£ç§¯åˆ†)
    
    â˜… æ¸è¿›å¼ä¸¤æ­¥æµç¨‹:
    1. create_session - åˆ›å»ºä¼šè¯ + ä¸Šä¼ è§†é¢‘ (æœ¬æ¥å£ï¼Œä¸æ‰£ç§¯åˆ†)
    2. start-ai-processing - ç”¨æˆ·ç¡®è®¤é…ç½®åå¯åŠ¨ AI å¤„ç† (æ‰£ç§¯åˆ†)
    """
    try:
        user_id = current_user["user_id"]
        
        # â˜… ç§»é™¤ç§¯åˆ†æ£€æŸ¥ï¼ç§¯åˆ†æ£€æŸ¥ç§»åˆ° start-ai-processing æ¥å£
        # è¿™æ ·ç”¨æˆ·å¯ä»¥å…ˆä¸Šä¼ è§†é¢‘ï¼Œå†å†³å®šæ˜¯å¦ä½¿ç”¨ AI åŠŸèƒ½
        
        session_id = str(uuid4())
        project_id = str(uuid4())
        now = datetime.utcnow().isoformat()
        
        logger.info(f"[Session] å¼€å§‹åˆ›å»ºä¼šè¯, user_id={user_id}, source_type={request.source_type.value}")
        
        # 1. åˆ›å»ºé¡¹ç›®
        project_name = _generate_project_name(request)
        project_data = {
            "id": project_id,
            "user_id": user_id,
            "name": project_name,
            "status": "processing",
            "resolution": {"width": 1920, "height": 1080},
            "fps": 30,
            "created_at": now,
            "updated_at": now,
        }
        supabase.table("projects").insert(project_data).execute()
        logger.info(f"[Session] âœ… åˆ›å»ºé¡¹ç›®æˆåŠŸ, project_id={project_id}, name={project_name}")
        
        # 2. åˆ›å»ºä¼šè¯è®°å½•
        session_data = {
            "id": session_id,
            "user_id": user_id,
            "project_id": project_id,
            "status": "uploading",
            "upload_source": request.source_type.value,
            "source_url": request.source_url,
            "selected_tasks": [request.task_type.value],
            "progress": 0,
            "created_at": now,
            "updated_at": now,
        }
        supabase.table("workspace_sessions").insert(session_data).execute()
        logger.info(f"[Session] âœ… åˆ›å»ºä¼šè¯æˆåŠŸ, session_id={session_id}, task_type={request.task_type.value}")
        
        response = CreateSessionResponse(
            session_id=session_id,
            project_id=project_id,
        )
        
        # 3. æ ¹æ®æ¥æºç±»å‹å¤„ç†
        if request.source_type == SourceType.LOCAL:
            # === åˆ¤æ–­æ˜¯å¤šæ–‡ä»¶è¿˜æ˜¯å•æ–‡ä»¶ä¸Šä¼  ===
            logger.info(f"[Session] ğŸ“‚ æ£€æŸ¥ä¸Šä¼ æ¨¡å¼:")
            logger.info(f"[Session]    request.files: {request.files}")
            logger.info(f"[Session]    request.file_name: {request.file_name}")
            logger.info(f"[Session]    files æ˜¯å¦æœ‰å€¼: {bool(request.files)}")
            logger.info(f"[Session]    files é•¿åº¦: {len(request.files) if request.files else 0}")
            
            if request.files and len(request.files) > 0:
                logger.info(f"[Session] âœ… è¿›å…¥å¤šæ–‡ä»¶ä¸Šä¼ æ¨¡å¼")
                # â˜… å¤šæ–‡ä»¶ä¸Šä¼ æ¨¡å¼
                assets_info = []
                asset_ids = []
                total_bytes = sum(f.size for f in request.files)
                
                for file_info in request.files:
                    asset_id = str(uuid4())
                    asset_ids.append(asset_id)
                    file_ext = file_info.name.split(".")[-1] if "." in file_info.name else "mp4"
                    storage_path = f"uploads/{project_id}/{asset_id}.{file_ext}"
                    
                    # ç”Ÿæˆé¢„ç­¾åä¸Šä¼  URLï¼ˆå¯ç”¨ upsert é¿å…é‡è¯•å¤±è´¥ï¼‰
                    presign_result = create_signed_upload_url("clips", storage_path, upsert=True)
                    upload_url = presign_result.get("signedURL") or presign_result.get("signed_url", "")
                    
                    # åˆ›å»º asset è®°å½•
                    asset_data = {
                        "id": asset_id,
                        "project_id": project_id,
                        "user_id": user_id,
                        "name": file_info.name,
                        "original_filename": file_info.name,
                        "file_type": _get_file_type(file_info.content_type),
                        "mime_type": file_info.content_type or "video/mp4",
                        "file_size": file_info.size,
                        "storage_path": storage_path,
                        "duration": file_info.duration,
                        "status": "uploading",  # ç­‰å¾…ä¸Šä¼ ï¼ˆçº¦æŸåªå…è®¸ uploading/processing/ready/errorï¼‰
                        "order_index": file_info.order_index,  # ç´ æé¡ºåº
                        "upload_progress": {
                            "bytes_uploaded": 0,
                            "total_bytes": file_info.size,
                            "percentage": 0,
                        },
                        "created_at": now,
                        "updated_at": now,
                    }
                    supabase.table("assets").insert(asset_data).execute()
                    
                    assets_info.append(AssetUploadInfo(
                        asset_id=asset_id,
                        upload_url=upload_url,
                        storage_path=storage_path,
                        order_index=file_info.order_index,
                        file_name=file_info.name,
                    ))
                
                # æ›´æ–°ä¼šè¯è®°å½•
                supabase.table("workspace_sessions").update({
                    "uploaded_asset_ids": asset_ids,  # JSON æ•°ç»„
                    "upload_progress": {
                        "total_files": len(request.files),
                        "completed_files": 0,
                        "failed_files": 0,
                        "pending_files": len(request.files),
                        "total_bytes": total_bytes,
                        "uploaded_bytes": 0,
                    },
                    "status": "uploading",
                }).eq("id", session_id).execute()
                
                response.assets = assets_info
                logger.info(f"[Session] âœ… å¤šæ–‡ä»¶ä¸Šä¼ æ¨¡å¼ï¼Œåˆ›å»º {len(assets_info)} ä¸ªèµ„æº")
                logger.info(f"[Session]    response.assets æ•°é‡: {len(response.assets)}")
                for i, a in enumerate(response.assets):
                    logger.info(f"[Session]    asset[{i}]: {a.asset_id}, order={a.order_index}")
                
            else:
                # â˜… å•æ–‡ä»¶ä¸Šä¼ æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                asset_id = str(uuid4())
                file_ext = request.file_name.split(".")[-1] if request.file_name and "." in request.file_name else "mp4"
                storage_path = f"uploads/{project_id}/{asset_id}.{file_ext}"
                
                logger.info(f"[Session] æ­£åœ¨ç”Ÿæˆé¢„ç­¾åä¸Šä¼ URL, storage_path={storage_path}")
                logger.debug(f"[Session] æ”¶åˆ°çš„ duration: {request.duration}")
                presign_result = create_signed_upload_url("clips", storage_path, upsert=True)
                upload_url = presign_result.get("signedURL") or presign_result.get("signed_url", "")
                logger.info(f"[Session] âœ… é¢„ç­¾åURLç”ŸæˆæˆåŠŸ, url_length={len(upload_url)}")
                
                asset_data = {
                    "id": asset_id,
                    "project_id": project_id,
                    "user_id": user_id,
                    "name": request.file_name or "æœªå‘½å",
                    "original_filename": request.file_name,
                    "file_type": _get_file_type(request.content_type),
                    "mime_type": request.content_type or "video/mp4",
                    "file_size": request.file_size,
                    "storage_path": storage_path,
                    "duration": request.duration,
                    "status": "uploading",
                    "order_index": 0,
                    "created_at": now,
                    "updated_at": now,
                }
                supabase.table("assets").insert(asset_data).execute()
                logger.info(f"[Session] âœ… åˆ›å»ºèµ„æºæˆåŠŸ, asset_id={asset_id}, file_name={request.file_name}")
                
                supabase.table("workspace_sessions").update({
                    "uploaded_asset_id": asset_id,
                    "uploaded_asset_ids": [asset_id],  # åŒæ—¶æ›´æ–°æ•°ç»„å­—æ®µ
                    "status": "uploading",
                }).eq("id", session_id).execute()
                logger.info(f"[Session] âœ… æ›´æ–°ä¼šè¯å…³è”èµ„æºæˆåŠŸ")
                
                # â˜… å•æ–‡ä»¶ä¹Ÿç»Ÿä¸€æ”¾å…¥ assets æ•°ç»„
                response.assets = [AssetUploadInfo(
                    asset_id=asset_id,
                    upload_url=upload_url,
                    storage_path=storage_path,
                    order_index=0,
                    file_name=request.file_name,
                )]
            
        elif request.source_type in (SourceType.YOUTUBE, SourceType.URL):
            supabase.table("workspace_sessions").update({
                "status": "processing",
                "current_step": "fetch",
            }).eq("id", session_id).execute()
            logger.info(f"[Session] âœ… URLç±»å‹ä¼šè¯å¼€å§‹å¤„ç†, source_url={request.source_url}")
        
        logger.info(f"[Session] âœ… ä¼šè¯åˆ›å»ºå®Œæˆ, session_id={session_id}, project_id={project_id}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"[Session] âŒ åˆ›å»ºä¼šè¯å¤±è´¥: {e}")
        logger.error(f"[Session] âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/sessions/{session_id}/asset/{asset_id}/uploaded")
async def notify_asset_uploaded(session_id: str, asset_id: str):
    """
    é€šçŸ¥å•ä¸ªæ–‡ä»¶ä¸Šä¼ å®Œæˆï¼ˆå¤šæ–‡ä»¶ä¸Šä¼ æ¨¡å¼ï¼‰
    å‰ç«¯æ¯ä¸Šä¼ å®Œä¸€ä¸ªæ–‡ä»¶å°±è°ƒç”¨ä¸€æ¬¡
    """
    logger.info(f"[Upload] ğŸ“¤ æ”¶åˆ°èµ„æºä¸Šä¼ å®Œæˆé€šçŸ¥: session={session_id}, asset={asset_id}")
    try:
        now = datetime.utcnow().isoformat()
        
        # æ›´æ–°è¯¥ asset çŠ¶æ€
        logger.info(f"[Upload]    æ›´æ–° asset {asset_id} çŠ¶æ€ä¸º uploaded")
        supabase.table("assets").update({
            "status": "uploaded",
            "upload_progress": {
                "percentage": 100,
                "completed": True,
            },
            "updated_at": now,
        }).eq("id", asset_id).execute()
        
        # è·å– session ä¿¡æ¯
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_data = session.data
        asset_ids = session_data.get("uploaded_asset_ids", [])
        
        # ç»Ÿè®¡ä¸Šä¼ è¿›åº¦
        assets_result = supabase.table("assets").select("id, status, file_size").in_("id", asset_ids).execute()
        assets = assets_result.data or []
        
        completed = sum(1 for a in assets if a.get("status") == "uploaded")
        failed = sum(1 for a in assets if a.get("status") == "error")
        pending = len(assets) - completed - failed
        uploaded_bytes = sum(a.get("file_size", 0) for a in assets if a.get("status") == "uploaded")
        total_bytes = sum(a.get("file_size", 0) for a in assets)
        
        # æ›´æ–° session è¿›åº¦
        supabase.table("workspace_sessions").update({
            "upload_progress": {
                "total_files": len(assets),
                "completed_files": completed,
                "failed_files": failed,
                "pending_files": pending,
                "total_bytes": total_bytes,
                "uploaded_bytes": uploaded_bytes,
            },
            "updated_at": now,
        }).eq("id", session_id).execute()
        
        logger.info(f"[Session] ğŸ“¤ Asset {asset_id} ä¸Šä¼ å®Œæˆ, è¿›åº¦: {completed}/{len(assets)}")
        
        return {
            "status": "ok",
            "progress": {
                "completed": completed,
                "total": len(assets),
                "all_completed": completed == len(assets),
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Session] âŒ é€šçŸ¥ä¸Šä¼ å®Œæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# â˜… ä¸Šä¼ å®Œæˆååˆ›å»ºåŸºç¡€é¡¹ç›®ç»“æ„ (æ–°å¢)
# ============================================

class FinalizeUploadResponse(BaseModel):
    """å®Œæˆä¸Šä¼ å“åº”"""
    status: str
    project_id: str
    tracks: list  # åˆ›å»ºçš„è½¨é“ä¿¡æ¯
    clips: list   # åˆ›å»ºçš„ clip ä¿¡æ¯
    message: str


@router.post("/sessions/{session_id}/finalize-upload", response_model=FinalizeUploadResponse)
async def finalize_upload(
    session_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    å®Œæˆä¸Šä¼ ï¼Œåˆ›å»ºåŸºç¡€é¡¹ç›®ç»“æ„ (track + video clip)
    
    â˜… æ¸è¿›å¼æµç¨‹çš„å…³é”®æ­¥éª¤:
    1. ç”¨æˆ·ä¸Šä¼ è§†é¢‘åè°ƒç”¨æ­¤æ¥å£
    2. åˆ›å»ºåŸºç¡€çš„è§†é¢‘è½¨é“å’Œå­—å¹•è½¨é“
    3. å°†ä¸Šä¼ çš„è§†é¢‘æ”¾åˆ°æ—¶é—´è½´ä¸Šï¼ˆåˆ›å»º video clipï¼‰
    4. æ­¤æ—¶ç”¨æˆ·å¯ä»¥åœ¨ç¼–è¾‘å™¨ä¸­é¢„è§ˆå’Œç¼–è¾‘
    5. åç»­ AI å¤„ç†æ˜¯å¯é€‰çš„å¢å€¼åŠŸèƒ½
    """
    try:
        user_id = current_user["user_id"]
        now = datetime.utcnow().isoformat()
        
        # 1. è·å–ä¼šè¯ä¿¡æ¯
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_data = session.data
        project_id = session_data.get("project_id")
        
        # æ ¡éªŒä¼šè¯å½’å±
        if session_data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        # 2. è·å–æ‰€æœ‰å…³è”çš„ assets
        asset_ids = session_data.get("uploaded_asset_ids", [])
        if not asset_ids:
            single_asset_id = session_data.get("uploaded_asset_id")
            if single_asset_id:
                asset_ids = [single_asset_id]
        
        if not asset_ids:
            raise HTTPException(status_code=400, detail="ä¼šè¯æœªå…³è”ä»»ä½•èµ„æº")
        
        # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦éƒ½ä¸Šä¼ å®Œæˆ
        assets_result = supabase.table("assets").select("*").in_("id", asset_ids).execute()
        assets = assets_result.data or []
        
        not_ready = [a for a in assets if a.get("status") not in ("uploaded", "ready")]
        if not_ready:
            pending_names = [a.get("name", a["id"]) for a in not_ready[:3]]
            raise HTTPException(
                status_code=400, 
                detail=f"éƒ¨åˆ†æ–‡ä»¶æœªä¸Šä¼ å®Œæˆ: {', '.join(pending_names)}"
            )
        
        # 3. æ£€æŸ¥æ˜¯å¦å·²åˆ›å»ºè¿‡ trackï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
        existing_tracks = supabase.table("tracks").select("id").eq("project_id", project_id).execute()
        if existing_tracks.data and len(existing_tracks.data) > 0:
            logger.info(f"[Finalize] âš ï¸ é¡¹ç›® {project_id} å·²å­˜åœ¨è½¨é“ï¼Œè·³è¿‡åˆ›å»º")
            return FinalizeUploadResponse(
                status="ok",
                project_id=project_id,
                tracks=[{"id": t["id"]} for t in existing_tracks.data],
                clips=[],
                message="é¡¹ç›®ç»“æ„å·²å­˜åœ¨",
            )
        
        # 4. åˆ›å»ºåŸºç¡€è½¨é“
        video_track_id = str(uuid4())
        text_track_id = str(uuid4())
        
        # è§†é¢‘è½¨é“
        supabase.table("tracks").insert({
            "id": video_track_id,
            "project_id": project_id,
            "name": "è§†é¢‘è½¨é“",
            "order_index": 0,
            "is_muted": False,
            "is_locked": False,
            "is_visible": True,
            "created_at": now,
            "updated_at": now,
        }).execute()
        
        # å­—å¹•è½¨é“
        supabase.table("tracks").insert({
            "id": text_track_id,
            "project_id": project_id,
            "name": "å­—å¹•è½¨é“",
            "order_index": 1,
            "is_muted": False,
            "is_locked": False,
            "is_visible": True,
            "created_at": now,
            "updated_at": now,
        }).execute()
        
        logger.info(f"[Finalize] âœ… åˆ›å»ºåŸºç¡€è½¨é“: video={video_track_id}, text={text_track_id}")
        
        # 5. æŒ‰é¡ºåºæ’åˆ— assets å¹¶åˆ›å»º video clips
        sorted_assets = sorted(assets, key=lambda a: a.get("order_index", 0))
        
        created_clips = []
        timeline_position = 0  # æ—¶é—´è½´ä½ç½®ï¼ˆæ¯«ç§’ï¼‰
        
        for asset in sorted_assets:
            asset_id = asset["id"]
            duration_sec = asset.get("duration") or 0
            duration_ms = int(duration_sec * 1000)
            
            # å¦‚æœæ²¡æœ‰æ—¶é•¿ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆåç»­å¯ä»¥é€šè¿‡ ffprobe è·å–ï¼‰
            if duration_ms <= 0:
                duration_ms = 10000  # é»˜è®¤ 10 ç§’
                logger.warning(f"[Finalize] âš ï¸ Asset {asset_id} æ— æ—¶é•¿ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ 10s")
            
            clip_id = str(uuid4())
            
            # åˆ›å»º video clip
            clip_data = {
                "id": clip_id,
                "track_id": video_track_id,
                "asset_id": asset_id,
                "clip_type": "video",
                "name": asset.get("name", "è§†é¢‘"),
                "start_time": timeline_position,
                "end_time": timeline_position + duration_ms,
                "source_start": 0,
                "source_end": duration_ms,
                "volume": 1.0,
                "is_muted": False,
                "transform": {
                    "x": 0, "y": 0,
                    "scaleX": 1, "scaleY": 1,
                    "rotation": 0,
                    "opacity": 1,
                },
                "speed": 1.0,
                "created_at": now,
                "updated_at": now,
            }
            
            supabase.table("clips").insert(clip_data).execute()
            
            created_clips.append({
                "id": clip_id,
                "asset_id": asset_id,
                "start_time": timeline_position,
                "end_time": timeline_position + duration_ms,
            })
            
            logger.info(f"[Finalize] âœ… åˆ›å»º clip: {clip_id}, asset={asset_id}, duration={duration_ms}ms")
            
            # æ›´æ–°æ—¶é—´è½´ä½ç½®ï¼ˆä¸‹ä¸€ä¸ª clip ç´§è·Ÿç€ï¼‰
            timeline_position += duration_ms
        
        # 6. æ›´æ–°æ‰€æœ‰ assets çŠ¶æ€ä¸º ready
        for asset in assets:
            supabase.table("assets").update({
                "status": "ready",
                "updated_at": now,
            }).eq("id", asset["id"]).execute()
        
        # 7. æ›´æ–°é¡¹ç›®çŠ¶æ€ä¸º ready (æ•°æ®åº“çº¦æŸ: draft/processing/ready/exported/archived)
        supabase.table("projects").update({
            "status": "ready",
            "updated_at": now,
        }).eq("id", project_id).execute()
        
        # 8. æ›´æ–°ä¼šè¯çŠ¶æ€ä¸º completed (æ•°æ®åº“çº¦æŸ: uploading/processing/completed/failed/cancelled)
        #    è¡¨ç¤ºä¸Šä¼ é˜¶æ®µå·²å®Œæˆï¼Œåç»­ AI å¤„ç†æ˜¯å¯é€‰çš„å¢å€¼åŠŸèƒ½
        supabase.table("workspace_sessions").update({
            "status": "completed",
            "updated_at": now,
        }).eq("id", session_id).execute()
        
        logger.info(f"[Finalize] âœ… å®Œæˆä¸Šä¼ ï¼Œé¡¹ç›® {project_id} å¯ä»¥ç¼–è¾‘äº†")
        logger.info(f"[Finalize]    åˆ›å»ºäº† {len(created_clips)} ä¸ª clips")
        
        return FinalizeUploadResponse(
            status="ok",
            project_id=project_id,
            tracks=[
                {"id": video_track_id, "name": "è§†é¢‘è½¨é“", "order_index": 0},
                {"id": text_track_id, "name": "å­—å¹•è½¨é“", "order_index": 1},
            ],
            clips=created_clips,
            message=f"åŸºç¡€é¡¹ç›®ç»“æ„åˆ›å»ºæˆåŠŸï¼ŒåŒ…å« {len(created_clips)} ä¸ªè§†é¢‘ç‰‡æ®µ",
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"[Finalize] âŒ å®Œæˆä¸Šä¼ å¤±è´¥: {e}")
        logger.error(f"[Finalize] âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/sessions/{session_id}/confirm-upload")
async def confirm_upload(session_id: str, background_tasks: BackgroundTasks):
    """
    ç¡®è®¤æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å®Œæˆï¼Œå¼€å§‹å¤„ç†
    æ”¯æŒå¤šæ–‡ä»¶å’Œå•æ–‡ä»¶æ¨¡å¼
    """
    try:
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_data = session.data
        project_id = session_data.get("project_id")
        now = datetime.utcnow().isoformat()
        
        # â˜… é˜²æ­¢é‡å¤è§¦å‘ï¼šå¦‚æœå·²ç»åœ¨å¤„ç†ä¸­ï¼Œç›´æ¥è¿”å›
        current_status = session_data.get("status")
        if current_status == "processing":
            logger.info(f"[Session] âš ï¸ ä¼šè¯ {session_id} å·²åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡é‡å¤è¯·æ±‚")
            return {
                "status": "processing",
                "message": "ä»»åŠ¡å·²åœ¨å¤„ç†ä¸­ï¼Œè¯·å‹¿é‡å¤æäº¤",
                "asset_count": len(session_data.get("uploaded_asset_ids", [])),
            }
        
        # === è·å–æ‰€æœ‰å…³è”çš„ assets ===
        asset_ids = session_data.get("uploaded_asset_ids", [])
        
        # å…¼å®¹æ—§çš„å•æ–‡ä»¶æ¨¡å¼
        if not asset_ids:
            single_asset_id = session_data.get("uploaded_asset_id")
            if single_asset_id:
                asset_ids = [single_asset_id]
        
        if not asset_ids:
            raise HTTPException(status_code=400, detail="ä¼šè¯æœªå…³è”ä»»ä½•èµ„æº")
        
        # === æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦éƒ½ä¸Šä¼ å®Œæˆ ===
        assets_result = supabase.table("assets").select("*").in_("id", asset_ids).execute()
        assets = assets_result.data or []
        
        if len(assets) != len(asset_ids):
            raise HTTPException(status_code=400, detail="éƒ¨åˆ†èµ„æºè®°å½•ä¸å­˜åœ¨")
        
        # æ£€æŸ¥æœªå®Œæˆçš„ä¸Šä¼ 
        not_uploaded = [a for a in assets if a.get("status") not in ("uploaded", "uploading", "processing", "ready")]
        if not_uploaded:
            pending_names = [a.get("name", a["id"]) for a in not_uploaded[:3]]
            raise HTTPException(
                status_code=400, 
                detail=f"éƒ¨åˆ†æ–‡ä»¶æœªä¸Šä¼ å®Œæˆ: {', '.join(pending_names)}"
            )
        
        # === æ›´æ–°æ‰€æœ‰ assets çŠ¶æ€ä¸ºå¤„ç†ä¸­ ===
        for asset in assets:
            supabase.table("assets").update({
                "status": "processing",
                "updated_at": now,
            }).eq("id", asset["id"]).execute()
        
        # === æ›´æ–° session çŠ¶æ€ ===
        supabase.table("workspace_sessions").update({
            "status": "processing",
            "current_step": "fetch",
            "progress": 0,
            "updated_at": now,
        }).eq("id", session_id).execute()
        
        selected_tasks = session_data.get("selected_tasks", ["clips"])
        task_type = selected_tasks[0] if selected_tasks else "clips"
        
        # === æŒ‰é¡ºåºæ’åˆ— assets ===
        sorted_assets = sorted(assets, key=lambda a: a.get("order_index", 0))
        
        logger.info(f"[Session] ========================================")
        logger.info(f"[Session] ğŸš€ å‡†å¤‡å¯åŠ¨åå°å¤„ç†ä»»åŠ¡")
        logger.info(f"[Session]    session_id: {session_id}")
        logger.info(f"[Session]    project_id: {project_id}")
        logger.info(f"[Session]    task_type: {task_type}")
        logger.info(f"[Session]    sorted_assets count: {len(sorted_assets)}")
        for i, a in enumerate(sorted_assets):
            logger.info(f"[Session]    asset[{i}]: {a.get('name')} (order={a.get('order_index')})")
        logger.info(f"[Session] ========================================")
        
        # === å¯åŠ¨åå°å¤„ç†ä»»åŠ¡ ===
        background_tasks.add_task(
            _process_session_multi_assets,
            session_id=session_id,
            project_id=project_id,
            assets=sorted_assets,
            task_type=task_type,
        )
        
        logger.info(f"[Session] âœ… åå°ä»»åŠ¡å·²æ·»åŠ , å¼€å§‹å¤„ç† {len(assets)} ä¸ªç´ æ")
        
        return {
            "status": "processing", 
            "message": f"å¼€å§‹å¤„ç† {len(assets)} ä¸ªç´ æ",
            "asset_count": len(assets),
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Session] âŒ ç¡®è®¤ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sessions/{session_id}", response_model=SessionStatus)
async def get_session_status(session_id: str):
    """è·å–ä¼šè¯å¤„ç†çŠ¶æ€
    
    è¿”å›:
    - current_step: å½“å‰æ­¥éª¤ IDï¼ˆfetch/transcribe/segment/vision/transform/subtitle/prepareï¼‰
    - progress: 0-100 è¿›åº¦
    - status: pending/processing/completed/failed
    
    æ³¨æ„: steps å­—æ®µå·²åºŸå¼ƒï¼Œå‰ç«¯æœ¬åœ°ç”Ÿæˆæ­¥éª¤åˆ—è¡¨
    """
    try:
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        data = session.data
        
        return SessionStatus(
            session_id=data["id"],
            project_id=data["project_id"],
            status=data["status"],
            current_step=data.get("current_step"),
            progress=data.get("progress", 0),
            steps=[],  # â˜… åºŸå¼ƒï¼Œå‰ç«¯æœ¬åœ°ç”Ÿæˆ
            error=data.get("error_message"),
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/sessions/{session_id}")
async def cancel_session(session_id: str):
    """å–æ¶ˆå¤„ç†ä¼šè¯"""
    try:
        supabase.table("workspace_sessions").update({
            "status": "cancelled",
            "updated_at": datetime.utcnow().isoformat(),
        }).eq("id", session_id).execute()
        
        return {"message": "ä¼šè¯å·²å–æ¶ˆ"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# â˜… å·¥ä½œæµæ­¥éª¤ç®¡ç† API
# ============================================

class UpdateWorkflowStepRequest(BaseModel):
    """æ›´æ–°å·¥ä½œæµæ­¥éª¤è¯·æ±‚"""
    workflow_step: str  # entry, upload, processing, defiller, broll_config
    entry_mode: Optional[str] = None  # ai-talk, refine


@router.put("/sessions/{session_id}/workflow-step")
async def update_workflow_step(
    session_id: str,
    request: UpdateWorkflowStepRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    æ›´æ–°ä¼šè¯çš„å·¥ä½œæµæ­¥éª¤çŠ¶æ€
    
    ç”¨äºå‰ç«¯ä¿å­˜ç”¨æˆ·å½“å‰æ‰€åœ¨çš„å·¥ä½œæµæ­¥éª¤ï¼Œæ”¯æŒæ–­ç‚¹æ¢å¤ã€‚
    """
    try:
        user_id = current_user["user_id"]
        
        # éªŒè¯ä¼šè¯å½’å±
        session = supabase.table("workspace_sessions").select("user_id").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        if session.data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        # æ›´æ–°å·¥ä½œæµæ­¥éª¤
        update_data = {
            "current_step": request.workflow_step,
            "updated_at": datetime.utcnow().isoformat(),
        }
        
        # å¦‚æœæä¾›äº† entry_modeï¼Œå­˜å‚¨åˆ° processing_steps JSONB å­—æ®µ
        if request.entry_mode:
            update_data["processing_steps"] = {"entry_mode": request.entry_mode, "workflow_step": request.workflow_step}
        
        supabase.table("workspace_sessions").update(update_data).eq("id", session_id).execute()
        
        logger.info(f"[Workflow] æ›´æ–°å·¥ä½œæµæ­¥éª¤: session={session_id}, step={request.workflow_step}, mode={request.entry_mode}")
        
        return {"status": "ok", "workflow_step": request.workflow_step}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Workflow] æ›´æ–°å·¥ä½œæµæ­¥éª¤å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sessions/{session_id}/workflow-step")
async def get_workflow_step(
    session_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    è·å–ä¼šè¯çš„å·¥ä½œæµæ­¥éª¤çŠ¶æ€
    
    ç”¨äºå‰ç«¯æ¢å¤åˆ°ç”¨æˆ·ä¸Šæ¬¡ç¦»å¼€çš„å·¥ä½œæµæ­¥éª¤ã€‚
    """
    try:
        user_id = current_user["user_id"]
        
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        if session.data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        data = session.data
        processing_steps = data.get("processing_steps") or {}
        
        return {
            "session_id": session_id,
            "project_id": data.get("project_id"),
            "workflow_step": processing_steps.get("workflow_step") or data.get("current_step") or "upload",
            "entry_mode": processing_steps.get("entry_mode") or "refine",
            "status": data.get("status"),
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Workflow] è·å–å·¥ä½œæµæ­¥éª¤å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sessions/by-project/{project_id}/workflow-step")
async def get_workflow_step_by_project(
    project_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    é€šè¿‡é¡¹ç›® ID è·å–ä¼šè¯çš„å·¥ä½œæµæ­¥éª¤çŠ¶æ€
    
    ç”¨äºä»é¡¹ç›®åˆ—è¡¨ç‚¹å‡»æ—¶æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„å·¥ä½œæµã€‚
    """
    try:
        user_id = current_user["user_id"]
        
        # é€šè¿‡ project_id æŸ¥æ‰¾æœ€æ–°çš„ä¼šè¯
        session = supabase.table("workspace_sessions")\
            .select("*")\
            .eq("project_id", project_id)\
            .eq("user_id", user_id)\
            .order("created_at", desc=True)\
            .limit(1)\
            .execute()
        
        if not session.data or len(session.data) == 0:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ç›¸å…³ä¼šè¯")
        
        data = session.data[0]
        processing_steps = data.get("processing_steps") or {}
        
        return {
            "session_id": data.get("id"),
            "project_id": data.get("project_id"),
            "workflow_step": processing_steps.get("workflow_step") or data.get("current_step") or "upload",
            "entry_mode": processing_steps.get("entry_mode") or "refine",
            "status": data.get("status"),
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Workflow] é€šè¿‡é¡¹ç›®è·å–å·¥ä½œæµæ­¥éª¤å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# â˜… ä¿å­˜å·¥ä½œæµé…ç½®ï¼ˆB-Roll, PiP ç­‰ï¼‰
# ============================================

class WorkflowConfigRequest(BaseModel):
    """å·¥ä½œæµé…ç½®è¯·æ±‚"""
    pip_enabled: bool = False                   # æ˜¯å¦å¯ç”¨æŒ‚è§’äººåƒ
    pip_position: Optional[str] = "bottom-right"  # äººåƒä½ç½®: bottom-right, bottom-left, top-right, top-left
    pip_size: Optional[str] = "medium"          # äººåƒå¤§å°: small, medium, large
    broll_enabled: bool = False                 # æ˜¯å¦å¯ç”¨ B-Roll
    broll_selections: Optional[List[dict]] = None  # B-Roll é€‰æ‹© [{clip_id, selected_asset_id}]
    background_preset: Optional[str] = None     # èƒŒæ™¯é¢„è®¾ ID


@router.post("/sessions/{session_id}/workflow-config")
async def save_workflow_config(
    session_id: str,
    request: WorkflowConfigRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ä¿å­˜å·¥ä½œæµé…ç½®ï¼ˆB-Roll, PiP ç­‰ï¼‰
    
    â˜… è¿™äº›é…ç½®å°†åœ¨ç¼–è¾‘å™¨åŠ è½½æ—¶ä½¿ç”¨ï¼Œè‡ªåŠ¨åº”ç”¨åˆ°ç”»å¸ƒä¸Š
    """
    try:
        user_id = current_user["user_id"]
        
        # éªŒè¯ä¼šè¯å½’å±
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        if session.data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        # æ›´æ–° processing_steps JSONB å­—æ®µï¼Œä¿ç•™å·²æœ‰é…ç½®
        existing_steps = session.data.get("processing_steps") or {}
        existing_steps.update({
            "pip_enabled": request.pip_enabled,
            "pip_position": request.pip_position,
            "pip_size": request.pip_size,
            "broll_enabled": request.broll_enabled,
            "broll_selections": request.broll_selections or [],
            "background_preset": request.background_preset,
            "config_saved_at": datetime.utcnow().isoformat(),
        })
        
        supabase.table("workspace_sessions").update({
            "processing_steps": existing_steps,
            "updated_at": datetime.utcnow().isoformat(),
        }).eq("id", session_id).execute()
        
        logger.info(f"[Workflow] ä¿å­˜å·¥ä½œæµé…ç½®: session={session_id}, pip={request.pip_enabled}, broll={request.broll_enabled}")
        
        return {
            "status": "ok",
            "message": "é…ç½®å·²ä¿å­˜",
            "config": {
                "pip_enabled": request.pip_enabled,
                "broll_enabled": request.broll_enabled,
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Workflow] ä¿å­˜å·¥ä½œæµé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sessions/{session_id}/workflow-config")
async def get_workflow_config(
    session_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    è·å–å·¥ä½œæµé…ç½®
    
    â˜… ç¼–è¾‘å™¨åŠ è½½æ—¶è°ƒç”¨ï¼Œè¯»å– PiP å’Œ B-Roll é…ç½®
    """
    try:
        user_id = current_user["user_id"]
        
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        if session.data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        processing_steps = session.data.get("processing_steps") or {}
        
        return {
            "session_id": session_id,
            "project_id": session.data.get("project_id"),
            "pip_enabled": processing_steps.get("pip_enabled", False),
            "pip_position": processing_steps.get("pip_position", "bottom-right"),
            "pip_size": processing_steps.get("pip_size", "medium"),
            "broll_enabled": processing_steps.get("broll_enabled", False),
            "broll_selections": processing_steps.get("broll_selections", []),
            "background_preset": processing_steps.get("background_preset"),
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[Workflow] è·å–å·¥ä½œæµé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# â˜… æ¸è¿›å¼ä¸¤æ­¥æµç¨‹: å¯åŠ¨ AI å¤„ç† (æ­¥éª¤2)
# ============================================

class StartAIProcessingRequest(BaseModel):
    """å¯åŠ¨ AI å¤„ç†è¯·æ±‚"""
    task_type: TaskType = TaskType.AI_CREATE
    # AI é…ç½®é€‰é¡¹ (å¯é€‰)
    output_ratio: Optional[str] = None  # è¾“å‡ºæ¯”ä¾‹: "9:16", "16:9", "1:1"
    template_id: Optional[str] = None   # æ¨¡æ¿ ID
    options: Optional[dict] = None      # å…¶ä»– AI é€‰é¡¹


class StartAIProcessingResponse(BaseModel):
    """å¯åŠ¨ AI å¤„ç†å“åº”"""
    status: str
    message: str
    credits_consumed: int
    credits_remaining: int


@router.post("/sessions/{session_id}/start-ai-processing", response_model=StartAIProcessingResponse)
async def start_ai_processing(
    session_id: str,
    request: StartAIProcessingRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """
    å¯åŠ¨ AI å¤„ç† (æ­¥éª¤2: æ£€æŸ¥ç§¯åˆ† + æ‰£é™¤ç§¯åˆ† + å¼€å§‹å¤„ç†)
    
    â˜… æ¸è¿›å¼ä¸¤æ­¥æµç¨‹:
    1. create_session - åˆ›å»ºä¼šè¯ + ä¸Šä¼ è§†é¢‘ (ä¸æ‰£ç§¯åˆ†)
    2. start-ai-processing - ç”¨æˆ·ç¡®è®¤é…ç½®åå¯åŠ¨ AI å¤„ç† (æœ¬æ¥å£ï¼Œæ‰£ç§¯åˆ†)
    
    æµç¨‹:
    1. æ ¡éªŒä¼šè¯çŠ¶æ€ (å¿…é¡»æ˜¯ä¸Šä¼ å®ŒæˆçŠ¶æ€)
    2. æ£€æŸ¥ç§¯åˆ†ä½™é¢
    3. æ‰£é™¤ç§¯åˆ†
    4. å¯åŠ¨åå°å¤„ç†ä»»åŠ¡
    """
    try:
        user_id = current_user["user_id"]
        now = datetime.utcnow().isoformat()
        
        # 1. è·å–ä¼šè¯ä¿¡æ¯
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_data = session.data
        project_id = session_data.get("project_id")
        
        # æ ¡éªŒä¼šè¯å½’å±
        if session_data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        # æ ¡éªŒä¼šè¯çŠ¶æ€: å¿…é¡»æ˜¯ completedï¼ˆfinalize-upload åçš„çŠ¶æ€ï¼‰
        if session_data.get("status") != "completed":
            raise HTTPException(
                status_code=400, 
                detail=f"ä¼šè¯çŠ¶æ€ä¸æ­£ç¡®: {session_data.get('status')}ï¼Œé¢„æœŸä¸º completedï¼ˆè¯·å…ˆå®Œæˆä¸Šä¼ ï¼‰"
            )
        
        # 2. è·å–æ‰€æœ‰å…³è”çš„ assets
        asset_ids = session_data.get("uploaded_asset_ids", [])
        if not asset_ids:
            single_asset_id = session_data.get("uploaded_asset_id")
            if single_asset_id:
                asset_ids = [single_asset_id]
        
        if not asset_ids:
            raise HTTPException(status_code=400, detail="ä¼šè¯æœªå…³è”ä»»ä½•èµ„æºï¼Œè¯·å…ˆä¸Šä¼ è§†é¢‘")
        
        # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶æ˜¯å¦éƒ½ä¸Šä¼ å®Œæˆ
        assets_result = supabase.table("assets").select("*").in_("id", asset_ids).execute()
        assets = assets_result.data or []
        
        not_ready = [a for a in assets if a.get("status") not in ("uploaded", "ready", "processing")]
        if not_ready:
            pending_names = [a.get("name", a["id"]) for a in not_ready[:3]]
            raise HTTPException(
                status_code=400, 
                detail=f"éƒ¨åˆ†æ–‡ä»¶æœªä¸Šä¼ å®Œæˆ: {', '.join(pending_names)}"
            )
        
        # 3. æ£€æŸ¥å¹¶æ‰£é™¤ç§¯åˆ† (ä»… AI åŠŸèƒ½éœ€è¦)
        credits_consumed = 0
        credits_remaining = 0
        
        if request.task_type.value == 'ai-create':
            from app.services.credit_service import get_credit_service
            credit_service = get_credit_service()
            
            # ai_create å›ºå®š 100 ç§¯åˆ†
            credits_required = 100
            
            # æ£€æŸ¥ç§¯åˆ†
            check_result = await credit_service.quick_check_credits(user_id, credits_required)
            
            if not check_result.get("allowed"):
                logger.warning(f"[AI Processing] âŒ ç§¯åˆ†ä¸è¶³: user_id={user_id}, required={credits_required}, available={check_result.get('available')}")
                raise HTTPException(
                    status_code=402,
                    detail={
                        "error": "insufficient_credits",
                        "message": f"ç§¯åˆ†ä¸è¶³ï¼Œéœ€è¦ {credits_required} ç§¯åˆ†ï¼Œå½“å‰ä½™é¢ {check_result.get('available')}",
                        "required": credits_required,
                        "available": check_result.get("available"),
                    }
                )
            
            # â˜… æ‰£é™¤ç§¯åˆ†
            consume_result = await credit_service.consume_credits(
                user_id=user_id,
                model_key="ai_create",
                credits=credits_required,  # â˜… å¿…é¡»ä¼ å…¥æ¶ˆè€—çš„ç§¯åˆ†æ•°
                ai_task_id=session_id,  # ä½¿ç”¨ session_id ä½œä¸ºä»»åŠ¡ ID
                description=f"ä¸€é”® AI æˆç‰‡ - {session_data.get('project_id', 'unknown')[:8]}",
            )
            
            if not consume_result.get("success"):
                raise HTTPException(
                    status_code=500,
                    detail=f"ç§¯åˆ†æ‰£é™¤å¤±è´¥: {consume_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                )
            
            credits_consumed = consume_result.get("credits_consumed", credits_required)
            credits_remaining = consume_result.get("credits_after", 0)
            
            logger.info(f"[AI Processing] âœ… ç§¯åˆ†æ‰£é™¤æˆåŠŸ: user_id={user_id}, consumed={credits_consumed}, remaining={credits_remaining}")
        
        # 4. æ›´æ–°ä¼šè¯é…ç½®
        update_data = {
            "selected_tasks": [request.task_type.value],
            "status": "processing",
            "current_step": "fetch",
            "progress": 0,
            "updated_at": now,
        }
        
        # ä¿å­˜ AI é…ç½®é€‰é¡¹
        if request.output_ratio or request.template_id or request.options:
            update_data["ai_config"] = {
                "output_ratio": request.output_ratio,
                "template_id": request.template_id,
                "options": request.options or {},
            }
        
        supabase.table("workspace_sessions").update(update_data).eq("id", session_id).execute()
        
        # 5. æ›´æ–°æ‰€æœ‰ assets çŠ¶æ€ä¸ºå¤„ç†ä¸­
        for asset in assets:
            supabase.table("assets").update({
                "status": "processing",
                "updated_at": now,
            }).eq("id", asset["id"]).execute()
        
        # 6. æŒ‰é¡ºåºæ’åˆ— assets
        sorted_assets = sorted(assets, key=lambda a: a.get("order_index", 0))
        
        logger.info(f"[AI Processing] ========================================")
        logger.info(f"[AI Processing] ğŸš€ å¯åŠ¨ AI å¤„ç†ä»»åŠ¡")
        logger.info(f"[AI Processing]    session_id: {session_id}")
        logger.info(f"[AI Processing]    project_id: {project_id}")
        logger.info(f"[AI Processing]    task_type: {request.task_type.value}")
        logger.info(f"[AI Processing]    credits_consumed: {credits_consumed}")
        logger.info(f"[AI Processing]    ç´ ææ•°é‡: {len(sorted_assets)}")
        logger.info(f"[AI Processing] ========================================")
        
        # 7. å¯åŠ¨åå°å¤„ç†ä»»åŠ¡
        background_tasks.add_task(
            _process_session_multi_assets,
            session_id=session_id,
            project_id=project_id,
            assets=sorted_assets,
            task_type=request.task_type.value,
        )
        
        return StartAIProcessingResponse(
            status="processing",
            message=f"AI å¤„ç†å·²å¯åŠ¨ï¼Œæ­£åœ¨å¤„ç† {len(assets)} ä¸ªç´ æ",
            credits_consumed=credits_consumed,
            credits_remaining=credits_remaining,
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"[AI Processing] âŒ å¯åŠ¨å¤±è´¥: {e}")
        logger.error(f"[AI Processing] âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# â˜… å£æ’­è§†é¢‘ç²¾ä¿®: å£ç™–/åºŸè¯æ£€æµ‹ (Defiller)
# ============================================

class FillerWord(BaseModel):
    """å£ç™–è¯æ±‡"""
    word: str                      # å£ç™–è¯æ±‡ï¼ˆå¦‚"å—¯..."ã€"é‚£ä¸ª"ï¼‰
    count: int                     # å‡ºç°æ¬¡æ•°
    total_duration_ms: int         # æ€»æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
    occurrences: List[dict]        # å‡ºç°ä½ç½® [{"start": ms, "end": ms, "clip_id": str}]


class DetectFillersRequest(BaseModel):
    """å£ç™–æ£€æµ‹è¯·æ±‚"""
    detect_fillers: bool = True      # è¯†åˆ«å£ç™–ï¼ˆå—¯ã€å•Šã€é‚£ä¸ª + é‡å¤è¯ + å¡é¡¿ï¼‰
    detect_breaths: bool = True      # è¯†åˆ«æ¢æ°”ï¼ˆé•¿æ—¶é—´åœé¡¿ï¼‰


class DetectFillersResponse(BaseModel):
    """å£ç™–æ£€æµ‹å“åº”"""
    status: str
    session_id: str
    project_id: str
    filler_words: List[FillerWord]           # æ£€æµ‹åˆ°çš„å£ç™–è¯æ±‡
    silence_segments: List[dict]             # é™éŸ³ç‰‡æ®µåˆ—è¡¨ï¼ˆå« silence_infoï¼‰
    transcript_segments: List[dict]          # å®Œæ•´è½¬å†™ç»“æœ
    total_filler_duration_ms: int            # åºŸè¯æ€»æ—¶é•¿
    original_duration_ms: int                # åŸè§†é¢‘æ—¶é•¿
    estimated_savings_percent: float         # é¢„è®¡èŠ‚çœç™¾åˆ†æ¯”


@router.post("/sessions/{session_id}/detect-fillers", response_model=DetectFillersResponse)
async def detect_fillers(
    session_id: str,
    request: DetectFillersRequest = None,
    background_tasks: BackgroundTasks = None,
    current_user: dict = Depends(get_current_user)
):
    """
    å£ç™–/åºŸè¯æ£€æµ‹ (å£æ’­è§†é¢‘ç²¾ä¿®æ¨¡å¼)
    
    â˜… å¤ç”¨ç°æœ‰ ASR + é™éŸ³æ£€æµ‹é€»è¾‘ï¼Œä¸æ‰§è¡Œ keyframe åˆ†æ
    â˜… è¿”å›åºŸè¯ç‰‡æ®µä¾›å‰ç«¯ DefillerModal ä½¿ç”¨
    â˜… æ ¹æ®é…ç½®é€‰é¡¹æ§åˆ¶æ£€æµ‹å†…å®¹
    
    æµç¨‹:
    1. è·å–ä¼šè¯å…³è”çš„è§†é¢‘èµ„æº
    2. æ‰§è¡Œ ASR è½¬å†™ï¼ˆå«é™éŸ³æ£€æµ‹ï¼‰
    3. åˆ†æå£ç™–è¯æ±‡ï¼ˆå—¯ã€å•Šã€é‚£ä¸ªã€å°±æ˜¯ç­‰ï¼‰
    4. è¿”å›ç»“æ„åŒ–çš„åºŸè¯æ•°æ®
    """
    try:
        user_id = current_user["user_id"]
        now = datetime.utcnow().isoformat()
        
        # â˜… è·å–é…ç½®é€‰é¡¹ï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
        if request is None:
            request = DetectFillersRequest()
        
        detect_fillers_enabled = request.detect_fillers  # å£ç™–+é‡å¤è¯+å¡é¡¿
        detect_breaths_enabled = request.detect_breaths   # æ¢æ°”
        
        logger.info(f"[Defiller] é…ç½®: fillers={detect_fillers_enabled}, breaths={detect_breaths_enabled}")
        
        # 1. è·å–ä¼šè¯ä¿¡æ¯
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_data = session.data
        project_id = session_data.get("project_id")
        
        # æ ¡éªŒä¼šè¯å½’å±
        if session_data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        # 2. è·å–å…³è”çš„ assets
        asset_ids = session_data.get("uploaded_asset_ids", [])
        if not asset_ids:
            single_asset_id = session_data.get("uploaded_asset_id")
            if single_asset_id:
                asset_ids = [single_asset_id]
        
        if not asset_ids:
            raise HTTPException(status_code=400, detail="ä¼šè¯æœªå…³è”ä»»ä½•èµ„æº")
        
        assets_result = supabase.table("assets").select("*").in_("id", asset_ids).execute()
        assets = assets_result.data or []
        
        if not assets:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°èµ„æºæ–‡ä»¶")
        
        # 3. æ‰§è¡Œ ASR è½¬å†™ï¼ˆå¤ç”¨ _run_asr å‡½æ•°ï¼Œæ­£ç¡®å¤„ç† Cloudflare HLSï¼‰
        from ..services.supabase_client import get_file_url
        
        all_segments = []
        total_duration_ms = 0
        
        # è¿›åº¦å›è°ƒï¼ˆdetect-fillers æ— éœ€å®æ—¶è¿›åº¦ï¼Œä½¿ç”¨ç©ºå‡½æ•°ï¼‰
        def dummy_progress(step: str, progress: int):
            logger.debug(f"[Defiller] Progress: {step} = {progress}%")
        
        for asset in assets:
            # â˜… assets è¡¨ç”¨ storage_path å­˜å‚¨ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦ç”Ÿæˆç­¾å URL
            storage_path = asset.get("storage_path")
            if not storage_path:
                logger.warning(f"[Defiller] âš ï¸ Asset {asset['id']} æ²¡æœ‰ storage_pathï¼Œè·³è¿‡")
                continue
            
            # ä½¿ç”¨ get_file_url è·å–ç­¾å URL (bucket æ˜¯ "clips")
            try:
                file_url = get_file_url("clips", storage_path, expires_in=3600)
                if not file_url:
                    logger.warning(f"[Defiller] âš ï¸ æ— æ³•è·å–ç­¾å URL: {storage_path}")
                    continue
            except Exception as url_err:
                logger.warning(f"[Defiller] âš ï¸ è·å–ç­¾å URL å¤±è´¥: {url_err}")
                continue
            
            asset_duration = float(asset.get("duration") or 0)
            total_duration_ms += int(asset_duration * 1000)
            
            logger.info(f"[Defiller] å¼€å§‹è½¬å†™: {file_url[:80]}...")
            
            # â˜…â˜…â˜… å¤ç”¨ _run_asr å‡½æ•°ï¼Œå…³é—­ DDC ä»¥ä¿ç•™è¯­æ°”è¯ â˜…â˜…â˜…
            # enable_ddc=False: ä¸å¯ç”¨è¯­ä¹‰é¡ºæ»‘ï¼Œä¿ç•™"å—¯"ã€"å•Š"ç­‰åŸå§‹è¯­æ°”è¯
            segments = await _run_asr(
                file_url=file_url,
                update_progress=dummy_progress,
                current_progress=0,
                step_progress=100,
                asset_id=asset["id"],
                video_duration_sec=asset_duration,
                enable_ddc=False  # â˜… å£ç™–æ£€æµ‹éœ€è¦ä¿ç•™åŸå§‹è¯­æ°”è¯
            )
            
            # ä¸ºæ¯ä¸ª segment æ·»åŠ  asset_id
            for seg in segments:
                seg["asset_id"] = asset["id"]
            
            all_segments.extend(segments)
            logger.info(f"[Defiller] è½¬å†™å®Œæˆ: {len(segments)} ä¸ªç‰‡æ®µ")
        
        # 4. â˜… ä½¿ç”¨æ™ºèƒ½å£ç™–æ£€æµ‹æœåŠ¡
        from ..services.filler_detector import detect_all_fillers, FillerType
        
        filler_words_map = {}  # word -> {count, total_duration_ms, occurrences}
        silence_segments = []
        
        logger.info(f"[Defiller] ğŸ¤– å¼€å§‹æ™ºèƒ½å£ç™–æ£€æµ‹: {len(all_segments)} ä¸ªç‰‡æ®µ")
        
        # ç›´æ¥ä½¿ç”¨ ASR ç»“æœï¼Œæ— éœ€é¢å¤–ä¸‹è½½
        # - é™éŸ³ç‰‡æ®µï¼šä» ASR ç»“æœçš„ silence_info æå–ï¼ˆtranscribe.py å·²åˆ†ç±»ï¼‰
        # - è¯­ä¹‰åˆ†æï¼šLLM åˆ†ææ–‡æœ¬
        analysis_result = await detect_all_fillers(
            segments=all_segments,  # åŒ…å«é™éŸ³å’Œæ–‡æœ¬ç‰‡æ®µ
            detect_silences=detect_breaths_enabled,
            detect_semantics=detect_fillers_enabled,
        )
        
        logger.info(f"[Defiller] ğŸ¤– æ£€æµ‹å®Œæˆ: {len(analysis_result.detections)} ä¸ªé—®é¢˜")
        logger.info(f"[Defiller] ğŸ¤– åˆ†ç±»: {analysis_result.filler_count_by_type}")
        
        # å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸º filler_words_map æ ¼å¼
        for detection in analysis_result.detections:
            # æ ¹æ®ç±»å‹ç”Ÿæˆæ˜¾ç¤ºåç§°
            type_names = {
                FillerType.BREATH: "[æ¢æ°”]",
                FillerType.HESITATION: "[å¡é¡¿]",
                FillerType.DEAD_AIR: "[æ­»å¯‚]",
                FillerType.FILLER_WORD: detection.text or "[å£ç™–è¯]",
                FillerType.REPEAT_WORD: f"[é‡å¤] {detection.text}",
                FillerType.NG_TAKE: "[NGç‰‡æ®µ]",
            }
            filler_type = type_names.get(detection.filler_type, detection.text)
            
            duration_ms = detection.duration_ms
            
            if filler_type not in filler_words_map:
                filler_words_map[filler_type] = {"count": 0, "total_duration_ms": 0, "occurrences": []}
            
            filler_words_map[filler_type]["count"] += 1
            filler_words_map[filler_type]["total_duration_ms"] += duration_ms
            filler_words_map[filler_type]["occurrences"].append({
                "start": detection.start,
                "end": detection.end,
                "asset_id": detection.asset_id,
                "text": detection.text,
                "reason": detection.reason,
                "confidence": detection.confidence,
                "segment_id": detection.segment_id,
            })
            
            # å¦‚æœæ˜¯é™éŸ³ç±»å‹ï¼Œæ·»åŠ åˆ° silence_segments
            if detection.filler_type in (FillerType.BREATH, FillerType.HESITATION, FillerType.DEAD_AIR):
                silence_segments.append({
                    "id": detection.segment_id,
                    "text": "",
                    "start": detection.start,
                    "end": detection.end,
                    "asset_id": detection.asset_id,
                    "silence_info": {
                        "classification": detection.filler_type.value,
                        "duration_ms": duration_ms,
                        "reason": detection.reason,
                    }
                })
        
        # 5. æ„å»ºå“åº”
        filler_words = [
            FillerWord(
                word=word,
                count=data["count"],
                total_duration_ms=data["total_duration_ms"],
                occurrences=data["occurrences"]
            )
            for word, data in sorted(filler_words_map.items(), key=lambda x: -x[1]["count"])
        ]
        
        total_filler_duration = sum(f.total_duration_ms for f in filler_words)
        savings_percent = (total_filler_duration / total_duration_ms * 100) if total_duration_ms > 0 else 0
        
        logger.info(f"[Defiller] âœ… æ£€æµ‹å®Œæˆ: {len(filler_words)} ç±»å£ç™–, æ€»æ—¶é•¿ {total_filler_duration}ms")
        logger.info(f"[Defiller]    é¢„è®¡èŠ‚çœ {savings_percent:.1f}%")
        
        return DetectFillersResponse(
            status="completed",
            session_id=session_id,
            project_id=project_id,
            filler_words=filler_words,
            silence_segments=silence_segments,
            transcript_segments=all_segments,
            total_filler_duration_ms=total_filler_duration,
            original_duration_ms=total_duration_ms,
            estimated_savings_percent=round(savings_percent, 1),
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"[Defiller] âŒ æ£€æµ‹å¤±è´¥: {e}")
        logger.error(f"[Defiller] âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# â˜… å£æ’­è§†é¢‘ç²¾ä¿®: åº”ç”¨ä¿®å‰ª (Apply Trimming)
# ============================================

class TrimSegment(BaseModel):
    """éœ€è¦åˆ é™¤çš„ç‰‡æ®µ"""
    start: int                # å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    end: int                  # ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    asset_id: Optional[str] = None  # æ‰€å±èµ„æº ID
    reason: Optional[str] = None    # åˆ é™¤åŸå› ï¼ˆå¦‚ "filler_word:å—¯"ï¼‰


class ApplyTrimmingRequest(BaseModel):
    """åº”ç”¨ä¿®å‰ªè¯·æ±‚"""
    removed_fillers: List[str]        # ç”¨æˆ·é€‰æ‹©åˆ é™¤çš„å£ç™–è¯æ±‡
    trim_segments: Optional[List[TrimSegment]] = None  # å¯é€‰ï¼šå…·ä½“è¦åˆ é™¤çš„ç‰‡æ®µ
    create_clips_from_segments: bool = True  # æ˜¯å¦æ ¹æ®ä¿ç•™ç‰‡æ®µåˆ›å»º clips


class ApplyTrimmingResponse(BaseModel):
    """åº”ç”¨ä¿®å‰ªå“åº”"""
    status: str
    session_id: str
    project_id: str
    clips_created: int                # åˆ›å»ºçš„ clip æ•°é‡
    total_duration_ms: int            # ä¿®å‰ªåçš„æ€»æ—¶é•¿
    removed_duration_ms: int          # è¢«åˆ é™¤çš„æ—¶é•¿
    clips: List[dict]                 # åˆ›å»ºçš„ clips åˆ—è¡¨


@router.post("/sessions/{session_id}/apply-trimming", response_model=ApplyTrimmingResponse)
async def apply_trimming(
    session_id: str,
    request: ApplyTrimmingRequest,
    background_tasks: BackgroundTasks,
    current_user: dict = Depends(get_current_user)
):
    """
    åº”ç”¨å£ç™–ä¿®å‰ª (å£æ’­è§†é¢‘ç²¾ä¿®æ¨¡å¼)
    
    â˜… æ ¹æ®ç”¨æˆ·åœ¨ DefillerModal ä¸­çš„é€‰æ‹©ï¼Œæ‰§è¡Œå®é™…çš„ä¿®å‰ªæ“ä½œ
    â˜… åˆ›å»ºæ–°çš„ clips å¹¶æ›´æ–° project
    
    æµç¨‹:
    1. è·å–ä¼šè¯å…³è”çš„è§†é¢‘èµ„æºå’Œè½¬å†™ç»“æœ
    2. æ ¹æ®é€‰ä¸­çš„å£ç™–è¯æ±‡è¿‡æ»¤å‡ºéœ€è¦åˆ é™¤çš„ç‰‡æ®µ
    3. è®¡ç®—ä¿ç•™ç‰‡æ®µå¹¶åˆ›å»º clips
    4. æ›´æ–°æ•°æ®åº“
    """
    try:
        user_id = current_user["user_id"]
        now = datetime.utcnow().isoformat()
        
        # 1. è·å–ä¼šè¯ä¿¡æ¯
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_data = session.data
        project_id = session_data.get("project_id")
        
        # æ ¡éªŒä¼šè¯å½’å±
        if session_data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        # 2. è·å–å…³è”çš„ assets
        asset_ids = session_data.get("uploaded_asset_ids", [])
        if not asset_ids:
            single_asset_id = session_data.get("uploaded_asset_id")
            if single_asset_id:
                asset_ids = [single_asset_id]
        
        if not asset_ids:
            raise HTTPException(status_code=400, detail="ä¼šè¯æœªå…³è”ä»»ä½•èµ„æº")
        
        assets_result = supabase.table("assets").select("*").in_("id", asset_ids).execute()
        assets = assets_result.data or []
        
        if not assets:
            raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°èµ„æºæ–‡ä»¶")
        
        # 3. è·å–å·²æœ‰çš„è½¬å†™ç»“æœï¼ˆä» clips æˆ– transcripts è¡¨ï¼‰
        # å…ˆæ£€æŸ¥æ˜¯å¦å·²æ‰§è¡Œè¿‡ detect-fillers (clips è¡¨æ²¡æœ‰ project_idï¼Œç”¨ asset_id æŸ¥è¯¢)
        existing_clips = supabase.table("clips").select("*").in_("asset_id", asset_ids).execute()
        existing_clips_data = existing_clips.data or []
        
        # å¦‚æœæœ‰ trim_segmentsï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™éœ€è¦é‡æ–°åˆ†æ
        trim_segments = request.trim_segments or []
        removed_fillers = set(request.removed_fillers)
        
        logger.info(f"[ApplyTrimming] å¼€å§‹ä¿®å‰ª: session={session_id}, project={project_id}")
        logger.info(f"[ApplyTrimming] åˆ é™¤çš„å£ç™–: {removed_fillers}")
        
        # 4. å¦‚æœæ²¡æœ‰æä¾›å…·ä½“çš„ trim_segmentsï¼Œéœ€è¦é‡æ–°æ‰§è¡Œ ASR åˆ†æ
        if not trim_segments:
            from ..tasks.transcribe import transcribe_audio
            
            all_segments = []
            total_duration_ms = 0
            
            for asset in assets:
                file_url = asset.get("file_url") or asset.get("url")
                if not file_url:
                    continue
                
                asset_duration = int((asset.get("duration") or 0) * 1000)
                total_duration_ms += asset_duration
                
                # ä½¿ç”¨ç¼“å­˜çš„è½¬å†™ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
                cached_transcript = asset.get("metadata", {}).get("transcript_segments")
                if cached_transcript:
                    segments = cached_transcript
                else:
                    result = await transcribe_audio(
                        audio_url=file_url,
                        language="zh",
                    )
                    segments = result.get("segments", [])
                
                for seg in segments:
                    seg["asset_id"] = asset["id"]
                all_segments.extend(segments)
            
            # æ ¹æ® removed_fillers æ ‡è®°éœ€è¦åˆ é™¤çš„ç‰‡æ®µ
            FILLER_PATTERNS = list(removed_fillers)
            
            for seg in all_segments:
                silence_info = seg.get("silence_info")
                text = seg.get("text", "").strip()
                should_remove = False
                reason = None
                
                # æ£€æŸ¥é™éŸ³ç‰‡æ®µ
                if silence_info:
                    classification = silence_info.get("classification")
                    filler_type = f"[{classification}]"
                    if filler_type in removed_fillers:
                        should_remove = True
                        reason = f"silence:{classification}"
                
                # æ£€æŸ¥æ–‡æœ¬ä¸­çš„å£ç™–è¯æ±‡
                if text and not should_remove:
                    for pattern in FILLER_PATTERNS:
                        if pattern in text and not pattern.startswith("["):
                            should_remove = True
                            reason = f"filler_word:{pattern}"
                            break
                
                if should_remove:
                    trim_segments.append(TrimSegment(
                        start=int(seg.get("start", 0)),
                        end=int(seg.get("end", 0)),
                        asset_id=seg.get("asset_id"),
                        reason=reason,
                    ))
        
        # 5. è®¡ç®—ä¿ç•™ç‰‡æ®µå¹¶åˆ›å»º clips
        # æŒ‰ asset åˆ†ç»„å¤„ç†
        asset_segments: dict = {}
        for asset in assets:
            asset_segments[asset["id"]] = {
                "asset": asset,
                "trim_segments": [],
                "duration_ms": int((asset.get("duration") or 0) * 1000),
            }
        
        for trim_seg in trim_segments:
            if trim_seg.asset_id and trim_seg.asset_id in asset_segments:
                asset_segments[trim_seg.asset_id]["trim_segments"].append({
                    "start": trim_seg.start,
                    "end": trim_seg.end,
                })
        
        # åˆå¹¶é‡å çš„ä¿®å‰ªç‰‡æ®µ
        def merge_overlapping_segments(segments):
            if not segments:
                return []
            sorted_segs = sorted(segments, key=lambda x: x["start"])
            merged = [sorted_segs[0]]
            for seg in sorted_segs[1:]:
                if seg["start"] <= merged[-1]["end"]:
                    merged[-1]["end"] = max(merged[-1]["end"], seg["end"])
                else:
                    merged.append(seg)
            return merged
        
        # è®¡ç®—ä¿ç•™ç‰‡æ®µ
        created_clips = []
        total_removed_duration = 0
        clip_start_time = 0  # å…¨å±€æ—¶é—´è½´ä¸Šçš„èµ·å§‹æ—¶é—´
        
        # è·å–ä¸»è½¨é“ (tracks è¡¨æ²¡æœ‰ track_typeï¼Œé€šè¿‡ order_index=0 åˆ¤æ–­ä¸»è½¨é“)
        track_result = supabase.table("tracks").select("id").eq("project_id", project_id).eq("order_index", 0).single().execute()
        if not track_result.data:
            # åˆ›å»ºé»˜è®¤ä¸»è½¨é“ (tracks è¡¨åªæœ‰ name, order_index ç­‰å­—æ®µï¼Œæ²¡æœ‰ track_type)
            track_id = str(uuid4())
            supabase.table("tracks").insert({
                "id": track_id,
                "project_id": project_id,
                "name": "ä¸»è½¨é“",
                "order_index": 0,
                "created_at": now,
            }).execute()
        else:
            track_id = track_result.data["id"]
        
        # åˆ é™¤ç°æœ‰çš„ clipsï¼ˆé€šè¿‡ asset_ids åˆ é™¤ï¼‰
        asset_ids_list = list(asset_segments.keys())
        if asset_ids_list:
            supabase.table("clips").delete().in_("asset_id", asset_ids_list).execute()
        
        for asset_id, asset_data in asset_segments.items():
            asset = asset_data["asset"]
            duration_ms = asset_data["duration_ms"]
            trim_segs = merge_overlapping_segments(asset_data["trim_segments"])
            
            # è®¡ç®—è¢«åˆ é™¤çš„æ—¶é•¿
            for seg in trim_segs:
                total_removed_duration += seg["end"] - seg["start"]
            
            # è®¡ç®—ä¿ç•™ç‰‡æ®µ
            keep_segments = []
            current_pos = 0
            
            for trim_seg in trim_segs:
                if trim_seg["start"] > current_pos:
                    keep_segments.append({
                        "start": current_pos,
                        "end": trim_seg["start"],
                    })
                current_pos = trim_seg["end"]
            
            # æœ€åä¸€ä¸ªä¿ç•™ç‰‡æ®µ
            if current_pos < duration_ms:
                keep_segments.append({
                    "start": current_pos,
                    "end": duration_ms,
                })
            
            # ä¸ºæ¯ä¸ªä¿ç•™ç‰‡æ®µåˆ›å»º clip
            for i, keep_seg in enumerate(keep_segments):
                clip_id = str(uuid4())  # å®Œæ•´çš„ UUIDï¼Œä¸èƒ½æˆªæ–­
                clip_duration = keep_seg["end"] - keep_seg["start"]
                
                clip_data = {
                    "id": clip_id,
                    "track_id": track_id,
                    "asset_id": asset_id,
                    "clip_type": "video",
                    "start_time": clip_start_time,
                    "end_time": clip_start_time + clip_duration,
                    "source_start": keep_seg["start"],
                    "source_end": keep_seg["end"],
                    "created_at": now,
                    "updated_at": now,
                }
                
                created_clips.append(clip_data)
                clip_start_time += clip_duration
        
        # 6. æ‰¹é‡æ’å…¥ clips
        if created_clips:
            supabase.table("clips").insert(created_clips).execute()
        
        # 7. è®¡ç®—æ€»æ—¶é•¿ï¼ˆç”¨äºè¿”å›ï¼Œprojects è¡¨æ²¡æœ‰ duration å­—æ®µï¼Œä¸æ›´æ–°ï¼‰
        total_duration = sum(c["end_time"] - c["start_time"] for c in created_clips)
        supabase.table("projects").update({
            "updated_at": now,
        }).eq("id", project_id).execute()
        
        # 8. æ›´æ–°ä¼šè¯çŠ¶æ€
        supabase.table("workspace_sessions").update({
            "status": "completed",
            "updated_at": now,
        }).eq("id", session_id).execute()
        
        logger.info(f"[ApplyTrimming] âœ… ä¿®å‰ªå®Œæˆ: åˆ›å»º {len(created_clips)} ä¸ª clips")
        logger.info(f"[ApplyTrimming]    ä¿ç•™æ—¶é•¿: {total_duration}ms, åˆ é™¤æ—¶é•¿: {total_removed_duration}ms")
        
        return ApplyTrimmingResponse(
            status="completed",
            session_id=session_id,
            project_id=project_id,
            clips_created=len(created_clips),
            total_duration_ms=total_duration,
            removed_duration_ms=total_removed_duration,
            clips=[{
                "id": c["id"],
                "start": c["start_time"],
                "duration": c["end_time"] - c["start_time"],
                "source_start": c["source_start"],
                "source_end": c["source_end"],
            } for c in created_clips],
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"[ApplyTrimming] âŒ ä¿®å‰ªå¤±è´¥: {e}")
        logger.error(f"[ApplyTrimming] âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# â˜… B-Roll ç‰‡æ®µå»ºè®® API (å£æ’­è§†é¢‘ç²¾ä¿®æ¨¡å¼)
# ============================================

class BRollAsset(BaseModel):
    """B-Roll ç´ æ"""
    id: str
    thumbnail_url: str
    video_url: str
    source: str  # pexels, local, ai-generated
    duration: int  # æ¯«ç§’
    width: int
    height: int
    relevance_score: Optional[float] = None


class ClipSuggestion(BaseModel):
    """AI ç‰‡æ®µå»ºè®®"""
    clip_id: str
    clip_number: int
    text: str
    time_range: dict  # {start: int, end: int}
    suggested_assets: List[BRollAsset]
    selected_asset_id: Optional[str] = None
    
    # B-Roll Agent åˆ†æç»“æœ
    need_broll: Optional[bool] = None          # æ˜¯å¦éœ€è¦ B-Roll
    broll_type: Optional[str] = None           # video/image/none
    broll_reason: Optional[str] = None         # å†³ç­–åŸå› 
    keywords_en: Optional[List[str]] = None    # è‹±æ–‡æœç´¢å…³é”®è¯
    keywords_cn: Optional[List[str]] = None    # ä¸­æ–‡å…³é”®è¯
    suggested_duration_ms: Optional[int] = None  # å»ºè®® B-Roll æ—¶é•¿


class GetClipSuggestionsResponse(BaseModel):
    """è·å–ç‰‡æ®µå»ºè®®å“åº”"""
    status: str
    session_id: str
    project_id: str
    clips: List[ClipSuggestion]
    total_duration_ms: int
    
    # ç»Ÿè®¡ä¿¡æ¯
    broll_segments_count: Optional[int] = None      # éœ€è¦ B-Roll çš„ç‰‡æ®µæ•°
    total_broll_duration_ms: Optional[int] = None   # B-Roll æ€»æ—¶é•¿


@router.post("/sessions/{session_id}/clip-suggestions", response_model=GetClipSuggestionsResponse)
async def get_clip_suggestions(
    session_id: str,
    current_user: dict = Depends(get_current_user)
):
    """
    è·å– AI ç‰‡æ®µå»ºè®® (å£æ’­è§†é¢‘ç²¾ä¿®æ¨¡å¼)
    
    â˜… æ ¹æ® ASR è½¬å†™ç»“æœï¼Œä¸ºæ¯ä¸ªç‰‡æ®µæ¨èåˆé€‚çš„ B-Roll ç´ æ
    â˜… ä½¿ç”¨å…³é”®è¯æå– + Pexels æœç´¢å®ç°
    
    æµç¨‹:
    1. è·å–ä¼šè¯çš„è½¬å†™ç‰‡æ®µï¼ˆä» detect-fillers ç»“æœï¼‰
    2. å¯¹æ¯ä¸ªç‰‡æ®µè¿›è¡Œå…³é”®è¯æå–
    3. ä½¿ç”¨å…³é”®è¯æœç´¢ Pexels B-Roll ç´ æ
    4. è¿”å›ç‰‡æ®µ + æ¨èç´ æåˆ—è¡¨
    """
    try:
        user_id = current_user["user_id"]
        
        # 1. è·å–ä¼šè¯ä¿¡æ¯
        session = supabase.table("workspace_sessions").select("*").eq("id", session_id).single().execute()
        if not session.data:
            raise HTTPException(status_code=404, detail="ä¼šè¯ä¸å­˜åœ¨")
        
        session_data = session.data
        project_id = session_data.get("project_id")
        
        # æ ¡éªŒä¼šè¯å½’å±
        if session_data.get("user_id") != user_id:
            raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä¼šè¯")
        
        # 2. è·å–é¡¹ç›®çš„ clips
        tracks_result = supabase.table("tracks").select("id").eq("project_id", project_id).execute()
        track_ids = [t["id"] for t in (tracks_result.data or [])]
        
        if not track_ids:
            raise HTTPException(status_code=400, detail="é¡¹ç›®æ²¡æœ‰è½¨é“")
        
        clips_result = supabase.table("clips").select("*").in_("track_id", track_ids).order("start_time").execute()
        clips = clips_result.data or []
        
        if not clips:
            raise HTTPException(status_code=400, detail="é¡¹ç›®æ²¡æœ‰ç‰‡æ®µ")
        
        # 3. æ”¶é›†ç‰‡æ®µä¿¡æ¯ç”¨äº B-Roll åˆ†æ
        segments_for_analysis = []
        total_duration = 0
        
        for i, clip in enumerate(clips):
            clip_id = clip["id"]
            start_time = clip.get("start_time", 0)
            end_time = clip.get("end_time", 0)
            duration = end_time - start_time
            total_duration += duration
            
            # è·å–ç‰‡æ®µæ–‡æœ¬ï¼ˆä» content_text æˆ– metadataï¼‰
            text = clip.get("content_text", "")
            if not text:
                metadata = clip.get("metadata", {}) or {}
                text = metadata.get("transcript_text", f"ç‰‡æ®µ {i + 1}")
            
            segments_for_analysis.append({
                "id": clip_id,
                "text": text,
                "start": start_time,
                "end": end_time,
            })
        
        # â˜… ä½¿ç”¨ B-Roll Agent è¿›è¡Œæ™ºèƒ½åˆ†æ
        from app.services.broll_agent import BRollAgent
        
        agent = BRollAgent()
        broll_result = await agent.analyze(
            session_id=session_id,
            segments=segments_for_analysis,
            video_style="å£æ’­",
            total_duration_ms=total_duration,
            search_assets=True,  # è‡ªåŠ¨æœç´¢ç´ æ
        )
        
        # è½¬æ¢ä¸º API å“åº”æ ¼å¼
        suggestions = []
        for i, decision in enumerate(broll_result.decisions):
            clip = next((c for c in clips if c["id"] == decision.segment_id), None)
            if not clip:
                continue
            
            text = clip.get("content_text", "")
            if not text:
                metadata = clip.get("metadata", {}) or {}
                text = metadata.get("transcript_text", f"ç‰‡æ®µ {i + 1}")
            
            # å°†åŒ¹é…çš„ç´ æè½¬æ¢ä¸º BRollAsset
            suggested_assets = []
            for asset in decision.matched_assets:
                suggested_assets.append(BRollAsset(
                    id=asset.get("id", ""),
                    thumbnail_url=asset.get("thumbnail_url", ""),
                    video_url=asset.get("video_url", "") or asset.get("image_url", ""),
                    source=asset.get("source", "pexels"),
                    duration=asset.get("duration_ms", 5000),
                    width=asset.get("width", 1920),
                    height=asset.get("height", 1080),
                    relevance_score=asset.get("relevance_score", 0.8),
                ))
            
            suggestions.append(ClipSuggestion(
                clip_id=decision.segment_id,
                clip_number=i + 1,
                text=text[:100] if text else f"ç‰‡æ®µ {i + 1}",
                time_range={
                    "start": clip.get("start_time", 0), 
                    "end": clip.get("end_time", 0)
                },
                suggested_assets=suggested_assets,
                # æ‰©å±•å­—æ®µ
                need_broll=decision.need_broll,
                broll_type=decision.broll_type.value if decision.need_broll else "none",
                broll_reason=decision.reason,
                keywords_en=decision.keywords_en,
                keywords_cn=decision.keywords_cn,
                suggested_duration_ms=decision.suggested_duration_ms,
            ))
        
        logger.info(f"[ClipSuggestions] âœ… æ™ºèƒ½åˆ†æå®Œæˆ: {broll_result.broll_segments}/{broll_result.total_segments} ç‰‡æ®µéœ€è¦ B-Roll")
        
        return GetClipSuggestionsResponse(
            status="completed",
            session_id=session_id,
            project_id=project_id,
            clips=suggestions,
            total_duration_ms=total_duration,
            # æ‰©å±•ç»Ÿè®¡
            broll_segments_count=broll_result.broll_segments,
            total_broll_duration_ms=broll_result.total_broll_duration_ms,
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"[ClipSuggestions] âŒ è·å–å»ºè®®å¤±è´¥: {e}")
        logger.error(f"[ClipSuggestions] âŒ å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================

import re


class SessionCancelledException(Exception):
    """ä¼šè¯è¢«ç”¨æˆ·å–æ¶ˆçš„å¼‚å¸¸"""
    pass


def _check_session_cancelled(session_id: str) -> bool:
    """
    æ£€æŸ¥ä¼šè¯æ˜¯å¦è¢«å–æ¶ˆ
    
    Args:
        session_id: ä¼šè¯ ID
        
    Returns:
        True å¦‚æœä¼šè¯å·²è¢«å–æ¶ˆï¼ŒFalse å¦åˆ™
    """
    try:
        result = supabase.table("workspace_sessions").select("status").eq("id", session_id).single().execute()
        if result.data:
            return result.data.get("status") == "cancelled"
        return False
    except Exception as e:
        logger.warning(f"[Workspace] æ£€æŸ¥ä¼šè¯çŠ¶æ€å¤±è´¥: {e}")
        return False


def _raise_if_cancelled(session_id: str, step_name: str = ""):
    """
    å¦‚æœä¼šè¯å·²å–æ¶ˆï¼ŒæŠ›å‡ºå¼‚å¸¸ç»ˆæ­¢å¤„ç†
    
    Args:
        session_id: ä¼šè¯ ID
        step_name: å½“å‰æ­¥éª¤åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    if _check_session_cancelled(session_id):
        step_info = f" (æ­¥éª¤: {step_name})" if step_name else ""
        logger.info(f"[Workspace] ğŸ›‘ ä¼šè¯ {session_id} è¢«å–æ¶ˆï¼Œç»ˆæ­¢å¤„ç†{step_info}")
        raise SessionCancelledException(f"ä¼šè¯ {session_id} å·²è¢«ç”¨æˆ·å–æ¶ˆ")


def _split_by_max_length(text: str, max_length: int) -> list:
    """
    æŒ‰æœ€å¤§é•¿åº¦æ™ºèƒ½åˆ‡åˆ†æ–‡æœ¬
    
    åˆ‡åˆ†ç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. ä¼˜å…ˆåœ¨ç©ºæ ¼å¤„åˆ‡åˆ†ï¼ˆé€‚åˆè‹±æ–‡ï¼‰
    2. å…¶æ¬¡åœ¨ä¸­æ–‡å¸¸è§åœé¡¿è¯ååˆ‡åˆ†ï¼ˆçš„ã€æ˜¯ã€åœ¨ã€äº†ã€å’Œã€ä¸ã€æˆ–ï¼‰
    3. æœ€åå¼ºåˆ¶æŒ‰é•¿åº¦åˆ‡åˆ†
    
    Args:
        text: å¾…åˆ‡åˆ†çš„æ–‡æœ¬
        max_length: æ¯æ®µæœ€å¤§å­—ç¬¦æ•°
        
    Returns:
        åˆ‡åˆ†åçš„æ–‡æœ¬åˆ—è¡¨
    """
    if len(text) <= max_length:
        return [text]
    
    result = []
    remaining = text
    
    while len(remaining) > max_length:
        # åœ¨ max_length èŒƒå›´å†…å¯»æ‰¾æœ€ä½³åˆ‡åˆ†ç‚¹
        chunk = remaining[:max_length]
        
        # ç­–ç•¥1ï¼šå¯»æ‰¾ç©ºæ ¼åˆ‡åˆ†ç‚¹ï¼ˆè‹±æ–‡ï¼‰
        space_idx = chunk.rfind(' ')
        if space_idx > max_length // 2:  # ç¡®ä¿åˆ‡åˆ†ç‚¹ä¸ä¼šå¤ªé å‰
            result.append(chunk[:space_idx].strip())
            remaining = remaining[space_idx:].strip()
            continue
        
        # ç­–ç•¥2ï¼šå¯»æ‰¾ä¸­æ–‡åœé¡¿è¯åˆ‡åˆ†ç‚¹
        stop_words = ['çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'ä¹Ÿ', 'éƒ½', 'å°±', 'è€Œ', 'ä½†', 'å¾ˆ', 'æ›´']
        best_stop_idx = -1
        for word in stop_words:
            idx = chunk.rfind(word)
            if idx > max_length // 2 and idx > best_stop_idx:
                best_stop_idx = idx + len(word)  # åˆ‡åˆ†ç‚¹åœ¨åœé¡¿è¯ä¹‹å
        
        if best_stop_idx > 0:
            result.append(chunk[:best_stop_idx].strip())
            remaining = remaining[best_stop_idx:].strip()
            continue
        
        # ç­–ç•¥3ï¼šå¼ºåˆ¶æŒ‰é•¿åº¦åˆ‡åˆ†
        result.append(chunk.strip())
        remaining = remaining[max_length:].strip()
    
    if remaining:
        result.append(remaining.strip())
    
    return result


def _split_segments_by_punctuation(segments: list, max_chars_per_line: int = 20) -> list:
    """
    å°† ASR segments æŒ‰æ ‡ç‚¹ç¬¦å·è¿›ä¸€æ­¥åˆ‡åˆ†æˆæ›´ç»†çš„å­å¥
    
    åˆ‡åˆ†è§„åˆ™ï¼š
    1. ä¸­æ–‡æ ‡ç‚¹ï¼šï¼Œã€‚ï¼ï¼Ÿï¼›
    2. è‹±æ–‡æ ‡ç‚¹ï¼š,.!?;
    3. æ ¹æ®æ–‡æœ¬é•¿åº¦æŒ‰æ¯”ä¾‹åˆ†é…æ—¶é—´
    4. å•è¡Œæœ€å¤§å­—ç¬¦æ•°é™åˆ¶ï¼ˆé˜²æ­¢å­—å¹•è¿‡é•¿è¶…å‡ºå±å¹•ï¼‰
    
    Args:
        segments: ASR è¿”å›çš„ segments åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« start, end, text
        max_chars_per_line: å•è¡Œæœ€å¤§å­—ç¬¦æ•°ï¼Œ15å·å­—ä½“ä¸‹å»ºè®®20å­—ç¬¦
        
    Returns:
        ç»†åˆ†åçš„ segments åˆ—è¡¨
    """
    # æ ‡ç‚¹ç¬¦å·æ­£åˆ™ï¼šåŒ¹é…ä¸­è‹±æ–‡é€—å·ã€å¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€åˆ†å·
    punctuation_pattern = re.compile(r'([ï¼Œã€‚ï¼ï¼Ÿï¼›,.!?;])')
    
    fine_segments = []
    
    for seg in segments:
        text = seg.get("text", "").strip()
        start_ms = seg.get("start", 0)
        end_ms = seg.get("end", 0)
        total_duration = end_ms - start_ms
        
        if not text or total_duration <= 0:
            continue
        
        # æŒ‰æ ‡ç‚¹åˆ‡åˆ†æ–‡æœ¬
        parts = punctuation_pattern.split(text)
        
        # é‡æ–°ç»„åˆï¼šå°†æ ‡ç‚¹ç¬¦å·ä¸å‰é¢çš„æ–‡æœ¬åˆå¹¶
        sentences = []
        buffer = ""
        for part in parts:
            buffer += part
            if punctuation_pattern.match(part):
                if buffer.strip():
                    sentences.append(buffer.strip())
                buffer = ""
        # å¤„ç†æœ€åæ²¡æœ‰æ ‡ç‚¹çš„éƒ¨åˆ†
        if buffer.strip():
            sentences.append(buffer.strip())
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªå¥å­æˆ–æ²¡æœ‰åˆ‡åˆ†ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æŒ‰é•¿åº¦åˆ‡åˆ†
        if len(sentences) <= 1:
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é•¿åº¦
            if len(text) <= max_chars_per_line:
                fine_segments.append(seg)
                continue
            else:
                # æŒ‰æœ€å¤§é•¿åº¦åˆ‡åˆ†
                sentences = _split_by_max_length(text, max_chars_per_line)
        else:
            # å¯¹æ¯ä¸ªå¥å­æ£€æŸ¥é•¿åº¦ï¼Œè¿‡é•¿çš„è¿›ä¸€æ­¥åˆ‡åˆ†
            expanded_sentences = []
            for s in sentences:
                if len(s) > max_chars_per_line:
                    expanded_sentences.extend(_split_by_max_length(s, max_chars_per_line))
                else:
                    expanded_sentences.append(s)
            sentences = expanded_sentences
        
        # æŒ‰å­—ç¬¦æ•°æ¯”ä¾‹åˆ†é…æ—¶é—´
        total_chars = sum(len(s) for s in sentences)
        if total_chars == 0:
            fine_segments.append(seg)
            continue
        
        current_time = start_ms
        for sentence in sentences:
            char_ratio = len(sentence) / total_chars
            duration = int(total_duration * char_ratio)
            
            # ç¡®ä¿è‡³å°‘æœ‰ 100ms
            duration = max(duration, 100)
            
            fine_segments.append({
                "id": seg.get("id"),
                "text": sentence,
                "start": current_time,
                "end": current_time + duration,
                "speaker": seg.get("speaker"),
            })
            
            current_time += duration
    
    return fine_segments


def _generate_project_name(request: CreateSessionRequest) -> str:
    """ç”Ÿæˆé¡¹ç›®åç§°"""
    if request.file_name:
        name = request.file_name.rsplit(".", 1)[0]
        return name[:50]
    elif request.source_url:
        return f"YouTube è§†é¢‘ - {datetime.now().strftime('%m/%d %H:%M')}"
    else:
        return f"æ–°é¡¹ç›® - {datetime.now().strftime('%Y-%m-%d %H:%M')}"


def _create_progress_updater(session_id: str):
    """
    åˆ›å»ºè¿›åº¦æ›´æ–°å™¨å‡½æ•°
    
    å†…ç½®èŠ‚æµï¼šåªåœ¨è¿›åº¦å˜åŒ– â‰¥1% æˆ–æ­¥éª¤å˜åŒ–æ—¶æ‰çœŸæ­£æ›´æ–°æ•°æ®åº“
    é¿å…é¢‘ç¹å†™å…¥é€ æˆæ€§èƒ½é—®é¢˜
    """
    last_progress = {"step": None, "value": -1}
    
    def update_progress(step: str, progress: int):
        # èŠ‚æµï¼šåªåœ¨è¿›åº¦å˜åŒ– â‰¥1% æˆ–æ­¥éª¤å˜åŒ–æ—¶æ‰æ›´æ–°
        if step == last_progress["step"] and progress == last_progress["value"]:
            return  # å®Œå…¨ç›¸åŒï¼Œè·³è¿‡
        
        if step == last_progress["step"] and abs(progress - last_progress["value"]) < 1:
            return  # åŒä¸€æ­¥éª¤ï¼Œè¿›åº¦å˜åŒ– <1%ï¼Œè·³è¿‡
        
        last_progress["step"] = step
        last_progress["value"] = progress
        
        supabase.table("workspace_sessions").update({
            "current_step": step,
            "progress": progress,
            "updated_at": datetime.utcnow().isoformat(),
        }).eq("id", session_id).execute()
    
    return update_progress


async def _fetch_asset_metadata(asset_id: str, file_url: str) -> dict:
    """æå–èµ„æºå…ƒæ•°æ®ï¼ˆå®½é«˜ã€å¸§ç‡ã€ç¼–ç ç­‰ï¼Œduration ç”±å‰ç«¯æä¾›ï¼‰"""
    try:
        from ..tasks.asset_processing import extract_media_metadata
        logger.info(f"[Workspace] æ­£åœ¨æå–å…ƒæ•°æ®: {file_url[:80]}...")
        metadata = await extract_media_metadata(file_url)
        logger.debug(f"[Workspace] æå–åˆ°çš„å…ƒæ•°æ®: {metadata}")
        
        # â˜… æ£€æµ‹è§†é¢‘ç¼–ç ï¼Œåˆ¤æ–­æµè§ˆå™¨æ˜¯å¦æ”¯æŒ
        video_codec = metadata.get("codec", "")
        # æµè§ˆå™¨åŸç”Ÿæ”¯æŒçš„ç¼–ç æ ¼å¼
        BROWSER_SUPPORTED_CODECS = {"h264", "avc1", "vp8", "vp9", "av1", "hevc", "h265"}
        needs_transcode = video_codec and video_codec.lower() not in BROWSER_SUPPORTED_CODECS
        if needs_transcode:
            logger.warning(f"[Workspace] âš ï¸ è§†é¢‘ç¼–ç  {video_codec} éœ€è¦è½¬ç ä¸º H.264")
        
        # duration ç”±å‰ç«¯æä¾›ï¼Œè¿™é‡Œåªæ›´æ–°å®½é«˜ã€å¸§ç‡ã€ç¼–ç ç­‰ä¿¡æ¯
        supabase.table("assets").update({
            "width": metadata.get("width", 1920),
            "height": metadata.get("height", 1080),
            "fps": metadata.get("fps", 30),
            "sample_rate": metadata.get("sample_rate"),
            "channels": metadata.get("channels"),
            "status": "ready",
            "updated_at": datetime.utcnow().isoformat(),
        }).eq("id", asset_id).execute()
        
        # è¿”å›å®Œæ•´å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ç¼–ç ä¿¡æ¯
        metadata["needs_transcode"] = needs_transcode
        return metadata
    except Exception as e:
        logger.warning(f"[Workspace] âŒ æå–å…ƒæ•°æ®å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å€¼")
        supabase.table("assets").update({
            "status": "ready",
            "updated_at": datetime.utcnow().isoformat(),
        }).eq("id", asset_id).execute()
        return {"duration": 0, "width": 1920, "height": 1080, "fps": 30, "needs_transcode": False}


async def _extract_audio_for_asr(video_url: str, asset_id: str, update_progress, current_progress: int, video_duration_sec: float = None) -> str:
    """
    ä»è§†é¢‘ä¸­æå–å‹ç¼©éŸ³é¢‘ï¼Œç”¨äº ASR è½¬å†™
    
    ä¼˜åŒ–ç­–ç•¥:
    - 16kHz é‡‡æ ·ç‡ï¼ˆè¯­éŸ³è¯†åˆ«è¶³å¤Ÿï¼‰
    - å•å£°é“
    - 64kbps ç ç‡
    - 4GB è§†é¢‘ â†’ çº¦ 20MB éŸ³é¢‘ï¼Œä¸Šä¼ é€Ÿåº¦æå‡ 99%
    - â˜… ç¼“å­˜å¤ç”¨ï¼šå¦‚æœéŸ³é¢‘å·²æå–è¿‡ï¼Œç›´æ¥è¿”å›ç¼“å­˜ URL
    - â˜… å®æ—¶è¿›åº¦ï¼šè§£æ FFmpeg è¾“å‡ºæ›´æ–°è¿›åº¦
    
    Args:
        video_url: è§†é¢‘çš„ç­¾å URL
        asset_id: èµ„äº§ IDï¼ˆç”¨äºå­˜å‚¨è·¯å¾„ï¼‰
        update_progress: è¿›åº¦å›è°ƒ
        current_progress: å½“å‰è¿›åº¦ç™¾åˆ†æ¯”
        video_duration_sec: è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œç”¨äºè®¡ç®—è¿›åº¦
        
    Returns:
        æå–åéŸ³é¢‘çš„ç­¾å URL
    """
    import tempfile
    import httpx
    import asyncio
    import os
    import re
    
    audio_storage_path = f"asr_audio/{asset_id}.mp3"
    
    # â˜…â˜…â˜… ç¼“å­˜æ£€æŸ¥ï¼šå¦‚æœéŸ³é¢‘å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å› â˜…â˜…â˜…
    try:
        # å°è¯•ç›´æ¥è·å–ç­¾å URLï¼Œå¦‚æœæˆåŠŸè¯´æ˜æ–‡ä»¶å­˜åœ¨
        from ..services.supabase_client import supabase
        result = supabase.storage.from_("clips").create_signed_url(audio_storage_path, 60)
        cached_url = result.get("signedURL") or result.get("signedUrl") or result.get("signed_url")
        if cached_url:
            logger.info(f"[ASRä¼˜åŒ–] âœ… ä½¿ç”¨ç¼“å­˜éŸ³é¢‘: {audio_storage_path}")
            update_progress("extract_audio", current_progress + 15)
            return cached_url
    except Exception:
        pass  # ç¼“å­˜ä¸å­˜åœ¨æˆ–æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­æå–
    
    logger.info(f"[ASRä¼˜åŒ–] ğŸµ å¼€å§‹æå–éŸ³é¢‘ asset_id={asset_id}, duration={video_duration_sec}s")
    
    custom_temp_dir = os.getenv("ASR_TEMP_DIR")
    temp_dir = tempfile.mkdtemp(prefix="asr_", dir=custom_temp_dir)
    logger.info(f"[ASRä¼˜åŒ–] ğŸ“ ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    audio_path = os.path.join(temp_dir, "audio_for_asr.mp3")
    
    try:
        update_progress("extract_audio", current_progress)
        logger.info(f"[ASRä¼˜åŒ–] ğŸ”§ FFmpeg æµå¼æå–éŸ³é¢‘...")
        logger.info(f"[ASRä¼˜åŒ–] ğŸ“ è¾“å…¥ URL: {video_url[:120]}...")
        
        # æ£€æµ‹æ˜¯å¦æ˜¯ HLS (m3u8) æµ
        is_hls = 'm3u8' in video_url.lower()
        
        # â˜… ä¼˜åŒ–ï¼šæ·»åŠ ç½‘ç»œè¶…æ—¶å’Œæ›´å¿«çš„ç¼–ç å‚æ•°
        cmd = [
            "ffmpeg", "-y",
            "-reconnect", "1",           # æ–­çº¿é‡è¿
            "-reconnect_streamed", "1",
            "-reconnect_delay_max", "5", # æœ€å¤§é‡è¿å»¶è¿Ÿ 5 ç§’
        ]
        
        # HLS éœ€è¦é¢å¤–å‚æ•°
        if is_hls:
            cmd.extend([
                "-protocol_whitelist", "file,http,https,tcp,tls,crypto",  # å…è®¸çš„åè®®
                "-allowed_extensions", "ALL",  # å…è®¸æ‰€æœ‰æ‰©å±•å
            ])
        
        cmd.extend([
            "-i", video_url,
            "-vn",                       # ä¸è¦è§†é¢‘
            "-ar", "16000",              # 16kHz é‡‡æ ·ç‡
            "-ac", "1",                  # å•å£°é“
            "-b:a", "64k",               # 64kbps ç ç‡
            "-f", "mp3",
            "-progress", "pipe:1",       # â˜… è¾“å‡ºè¿›åº¦åˆ° stdout
            audio_path
        ])
        
        logger.info(f"[ASRä¼˜åŒ–] ğŸ”§ FFmpeg å‘½ä»¤: {' '.join(cmd[:10])}...")
        
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        # â˜…â˜…â˜… å®æ—¶è§£æ FFmpeg è¿›åº¦ â˜…â˜…â˜…
        last_progress_update = 0
        total_duration_us = (video_duration_sec or 60) * 1_000_000  # å¾®ç§’
        
        async def read_progress():
            nonlocal last_progress_update
            while True:
                line = await process.stdout.readline()
                if not line:
                    break
                line_str = line.decode().strip()
                # FFmpeg progress æ ¼å¼: out_time_us=12345678
                if line_str.startswith("out_time_us="):
                    try:
                        current_us = int(line_str.split("=")[1])
                        if total_duration_us > 0:
                            pct = min(current_us / total_duration_us, 1.0)
                            # extract_audio å  10% è¿›åº¦ï¼ˆcurrent_progress åˆ° current_progress + 10ï¼‰
                            new_progress = current_progress + int(pct * 10)
                            # é¿å…é¢‘ç¹æ›´æ–°ï¼ˆæ¯å¢åŠ  2% æ›´æ–°ä¸€æ¬¡ï¼‰
                            if new_progress >= last_progress_update + 2:
                                update_progress("extract_audio", new_progress)
                                last_progress_update = new_progress
                    except (ValueError, IndexError):
                        pass
        
        # å¹¶è¡Œè¯»å– stdout è¿›åº¦å’Œ stderr
        async def read_stderr():
            """è¯»å– stderr è·å–é”™è¯¯ä¿¡æ¯"""
            data = await process.stderr.read()
            return data.decode() if data else ""
        
        progress_task = asyncio.create_task(read_progress())
        stderr_task = asyncio.create_task(read_stderr())
        
        # ç­‰å¾…è¿›ç¨‹å®Œæˆï¼ˆä¸ç”¨ communicateï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨è¯» stdoutï¼‰
        await process.wait()
        
        # ç­‰å¾…è¯»å–ä»»åŠ¡å®Œæˆ
        await progress_task
        stderr_text = await stderr_task
        
        if process.returncode != 0:
            # æå–çœŸæ­£çš„é”™è¯¯ä¿¡æ¯ï¼ˆè·³è¿‡ FFmpeg ç‰ˆæœ¬ä¿¡æ¯ï¼‰
            error_lines = stderr_text.split('\n')
            real_errors = [line for line in error_lines if 
                          'error' in line.lower() or 
                          'failed' in line.lower() or 
                          'invalid' in line.lower() or
                          'unable' in line.lower() or
                          'no such' in line.lower()]
            error_summary = '\n'.join(real_errors[-5:]) if real_errors else stderr_text[-500:]
            
            logger.error(f"[ASRä¼˜åŒ–] âŒ FFmpeg å¤±è´¥ (returncode={process.returncode}):")
            logger.error(f"[ASRä¼˜åŒ–] âŒ é”™è¯¯æ‘˜è¦: {error_summary}")
            logger.error(f"[ASRä¼˜åŒ–] âŒ å®Œæ•´ stderr (æœ€å 1000 å­—ç¬¦): {stderr_text[-1000:]}")
            raise Exception(f"éŸ³é¢‘æå–å¤±è´¥: {error_summary[:300]}")
        
        audio_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
        logger.info(f"[ASRä¼˜åŒ–] âœ… éŸ³é¢‘æµå¼æå–å®Œæˆ: {audio_size_mb:.1f}MB")
        
        # ä¸Šä¼ éŸ³é¢‘åˆ° Supabase
        update_progress("upload_audio", current_progress + 12)
        logger.info(f"[ASRä¼˜åŒ–] â¬†ï¸ ä¸Šä¼ éŸ³é¢‘...")
        
        audio_storage_path = f"asr_audio/{asset_id}.mp3"
        
        with open(audio_path, "rb") as f:
            audio_data = f.read()
        
        # ä½¿ç”¨ asyncio.to_thread é¿å…é˜»å¡
        await asyncio.to_thread(
            lambda: supabase.storage.from_("clips").upload(
                audio_storage_path,
                audio_data,
                {"content-type": "audio/mpeg", "upsert": "true"}
            )
        )
        
        audio_url = get_file_url("clips", audio_storage_path)
        update_progress("upload_audio", current_progress + 15)
        logger.info(f"[ASRä¼˜åŒ–] âœ… éŸ³é¢‘ä¸Šä¼ å®Œæˆ: {audio_storage_path}")
        
        return audio_url
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        try:
            shutil.rmtree(temp_dir)
            logger.info(f"[ASRä¼˜åŒ–] ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        except Exception as e:
            logger.warning(f"[ASRä¼˜åŒ–] âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")


async def _run_asr(file_url: str, update_progress, current_progress: int, step_progress: int, asset_id: str = None, video_duration_sec: float = None, enable_ddc: bool = True) -> list:
    """
    æ‰§è¡Œ ASR è¯­éŸ³è½¬å†™
    
    ä¼˜åŒ–1ï¼šå¦‚æœ asset_id åœ¨ tasks è¡¨ä¸­å·²æœ‰è½¬å†™ç»“æœï¼Œç›´æ¥å¤ç”¨
    ä¼˜åŒ–2ï¼šå¦‚æœæä¾›äº† asset_id ä¸”æ˜¯å¤§æ–‡ä»¶ï¼ˆè§†é¢‘ï¼‰ï¼Œä¼šå…ˆæå–éŸ³é¢‘å†è½¬å†™
    
    Args:
        enable_ddc: æ˜¯å¦å¯ç”¨è¯­ä¹‰é¡ºæ»‘ï¼ˆDDCï¼‰ï¼Œä¼šåˆ é™¤"å—¯"ã€"å•Š"ç­‰è¯­æ°”è¯
                    â˜… å£ç™–æ£€æµ‹ï¼ˆdetect_fillersï¼‰æ—¶åº”è®¾ä¸º False
    """
    logger.info(f"[_run_asr] ğŸ¤ å¼€å§‹ ASR è½¬å†™")
    logger.info(f"[_run_asr]    file_url: {file_url[:100]}...")
    logger.info(f"[_run_asr]    current_progress: {current_progress}, step_progress: {step_progress}")
    logger.info(f"[_run_asr]    asset_id: {asset_id}, video_duration: {video_duration_sec}s")
    
    try:
        from ..tasks.transcribe import transcribe_audio
        import httpx
        import asyncio
        
        # â˜…â˜…â˜… ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰è½¬å†™ç»“æœï¼ˆå¤ç”¨ analyze-content çš„è½¬å†™ï¼‰â˜…â˜…â˜…
        # æ³¨æ„ï¼šæ’é™¤ clip çº§åˆ«çš„è½¬å†™ä»»åŠ¡ï¼ˆparams ä¸­åŒ…å« clip_id çš„æ˜¯ clip è½¬å†™ï¼‰
        # åªå¤ç”¨æ•´ä½“ asset çš„è½¬å†™ç»“æœ
        if asset_id:
            existing_tasks = supabase.table("tasks").select(
                "id, status, result, params"
            ).eq("asset_id", asset_id).eq("task_type", "transcribe").eq("status", "completed").execute()
            
            # ç­›é€‰å‡ºæ•´ä½“ asset çš„è½¬å†™ä»»åŠ¡ï¼ˆparams ä¸­æ²¡æœ‰ clip_idï¼‰
            existing_task_data = None
            if existing_tasks and existing_tasks.data:
                for task in existing_tasks.data:
                    params = task.get("params") or {}
                    if not params.get("clip_id"):  # æ•´ä½“ asset è½¬å†™ï¼Œä¸æ˜¯ clip è½¬å†™
                        existing_task_data = task
                        break
            
            if existing_task_data and existing_task_data.get("result"):
                result = existing_task_data["result"]
                segments = result.get("segments", []) if isinstance(result, dict) else result
                if segments:
                    logger.info(f"[_run_asr] âœ… å¤ç”¨å·²æœ‰è½¬å†™ç»“æœ: {len(segments)} ä¸ªç‰‡æ®µ (task_id={existing_task_data['id'][:8]})")
                    update_progress("transcribe", current_progress + step_progress)
                    return segments
        
        # â˜… Cloudflare HLS URLï¼šFFmpeg æ”¯æŒç›´æ¥è¯»å– HLSï¼Œæå–éŸ³é¢‘ç”¨äº ASR
        is_cloudflare_hls = 'videodelivery.net' in file_url and 'm3u8' in file_url
        
        actual_audio_url = file_url
        
        if is_cloudflare_hls:
            # Cloudflare HLSï¼šä» HLS æå–éŸ³é¢‘
            logger.info(f"[_run_asr] â˜ï¸ Cloudflare HLSï¼Œä½¿ç”¨ FFmpeg æå–éŸ³é¢‘")
            audio_progress = int(step_progress * 0.2)
            actual_audio_url = await _extract_audio_for_asr(
                video_url=file_url,
                asset_id=asset_id,
                update_progress=update_progress,
                current_progress=current_progress,
                video_duration_sec=video_duration_sec
            )
            current_progress += audio_progress
            step_progress -= audio_progress
            logger.info(f"[_run_asr] âœ… Cloudflare HLS éŸ³é¢‘æå–æˆåŠŸ")
        else:
            # åˆ¤æ–­æ˜¯å¦æ˜¯è§†é¢‘æ–‡ä»¶ï¼Œéœ€è¦æå–éŸ³é¢‘
            is_video = any(ext in file_url.lower() for ext in ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.m4v'])
            
            # å¦‚æœæ˜¯è§†é¢‘ä¸”æœ‰ asset_idï¼Œå…ˆæå–éŸ³é¢‘ï¼ˆå¤§å¹…æå‡é€Ÿåº¦ï¼‰
            actual_audio_url = file_url
            if is_video and asset_id:
                logger.info(f"[_run_asr] ğŸ¬ æ£€æµ‹åˆ°è§†é¢‘æ–‡ä»¶ï¼Œå…ˆæå–éŸ³é¢‘ä»¥ä¼˜åŒ–ä¸Šä¼ é€Ÿåº¦")
                try:
                    # æå–éŸ³é¢‘å ç”¨ 20% è¿›åº¦
                    audio_progress = int(step_progress * 0.2)
                    actual_audio_url = await _extract_audio_for_asr(
                        video_url=file_url,
                        asset_id=asset_id,
                        update_progress=update_progress,
                        current_progress=current_progress,
                        video_duration_sec=video_duration_sec  # â˜… ä¼ å…¥è§†é¢‘æ—¶é•¿ç”¨äºè¿›åº¦è®¡ç®—
                    )
                    # è°ƒæ•´å‰©ä½™è¿›åº¦
                    current_progress += audio_progress
                    step_progress -= audio_progress
                    logger.info(f"[_run_asr] âœ… éŸ³é¢‘æå–æˆåŠŸï¼Œä½¿ç”¨å‹ç¼©éŸ³é¢‘è¿›è¡Œè½¬å†™")
                except Exception as e:
                    logger.warning(f"[_run_asr] âš ï¸ éŸ³é¢‘æå–å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–‡ä»¶: {e}")
                    actual_audio_url = file_url
        
        # â˜… Cloudflare HLS URL ä¸éœ€è¦éªŒè¯ï¼ˆç›´æ¥å¯ç”¨ï¼‰
        is_cloudflare_hls = 'videodelivery.net' in actual_audio_url and 'm3u8' in actual_audio_url
        
        if not is_cloudflare_hls:
            # éªŒè¯æ–‡ä»¶æ˜¯å¦å¯è®¿é—®ï¼ˆç­‰å¾…ä¸Šä¼ å®Œæˆï¼‰
            max_retries = 30  # æœ€å¤šç­‰å¾… 30 ç§’
            for retry in range(max_retries):
                try:
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        resp = await client.head(actual_audio_url)
                        if resp.status_code == 200:
                            logger.info(f"[_run_asr] âœ… æ–‡ä»¶å¯è®¿é—®ï¼Œå¼€å§‹è½¬å†™")
                            break
                        else:
                            logger.warning(f"[_run_asr] â³ æ–‡ä»¶ä¸å¯è®¿é—® (HTTP {resp.status_code})ï¼Œç­‰å¾…... ({retry+1}/{max_retries})")
                except Exception as e:
                    logger.warning(f"[_run_asr] â³ æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}ï¼Œç­‰å¾…... ({retry+1}/{max_retries})")
                
                if retry < max_retries - 1:
                    await asyncio.sleep(1)
            else:
                logger.error(f"[_run_asr] âŒ æ–‡ä»¶åœ¨ {max_retries} ç§’åä»ä¸å¯è®¿é—®")
                return []
        else:
            logger.info(f"[_run_asr] â˜ï¸ Cloudflare HLS URLï¼Œè·³è¿‡éªŒè¯")
        
        def on_asr_progress(progress: int, step: str):
            mapped_progress = current_progress + int(progress * step_progress / 100)
            update_progress("transcribe", mapped_progress)
        
        asr_result = await transcribe_audio(
            audio_url=actual_audio_url,
            language="zh",
            enable_ddc=enable_ddc,  # â˜… ä¼ é€’è¯­ä¹‰é¡ºæ»‘å¼€å…³
            on_progress=on_asr_progress,
        )
        
        segments = asr_result.get("segments", [])
        logger.info(f"[_run_asr] âœ… ASR å®Œæˆï¼Œè¯†åˆ« {len(segments)} ä¸ªç‰‡æ®µ")
        
        # è¯¦ç»†æ—¥å¿—
        if segments:
            first_seg = segments[0]
            last_seg = segments[-1]
            logger.info(f"[_run_asr]    ç¬¬ä¸€ä¸ªç‰‡æ®µ: start={first_seg.get('start')}ms text='{first_seg.get('text', '')[:20]}...'")
            logger.info(f"[_run_asr]    æœ€åä¸€ä¸ªç‰‡æ®µ: end={last_seg.get('end')}ms text='{last_seg.get('text', '')[:20]}...'")
        else:
            logger.warning(f"[_run_asr] âš ï¸ ASR è¿”å›ç©ºç»“æœ!")
        
        # â˜…â˜…â˜… ä¿å­˜è½¬å†™ç»“æœåˆ° tasks è¡¨ï¼Œä¾›æ™ºèƒ½åˆ†æå¤ç”¨ â˜…â˜…â˜…
        if asset_id and segments:
            try:
                now = datetime.utcnow().isoformat()
                default_user_id = "00000000-0000-0000-0000-000000000000"
                task_id = str(uuid4())
                
                # å…ˆæŸ¥è¯¢ project_id
                asset_info = supabase.table("assets").select("project_id").eq("id", asset_id).single().execute()
                project_id = asset_info.data.get("project_id") if asset_info.data else None
                
                supabase.table("tasks").insert({
                    "id": task_id,
                    "project_id": project_id,
                    "user_id": default_user_id,
                    "task_type": "transcribe",
                    "asset_id": asset_id,
                    "status": "completed",
                    "progress": 100,
                    "params": {"language": "zh", "model": "base"},
                    "result": asr_result,
                    "created_at": now,
                    "completed_at": now,
                    "updated_at": now
                }).execute()
                logger.info(f"[_run_asr] ğŸ’¾ è½¬å†™ç»“æœå·²ä¿å­˜åˆ° tasks è¡¨ (task_id={task_id[:8]})")
            except Exception as save_err:
                logger.warning(f"[_run_asr] âš ï¸ ä¿å­˜è½¬å†™ç»“æœå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {save_err}")
            
        return segments
    except Exception as e:
        logger.error(f"[_run_asr] âŒ ASR å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


# NOTE: _run_silence_detection å·²åˆ é™¤ (2025-01-28)
# é™éŸ³æ£€æµ‹åŠŸèƒ½å·²æ•´åˆåˆ° filler_detector.py ä¸­
# detect_fillers API ç›´æ¥è°ƒç”¨ filler_detector.detect_all_fillers


# NOTE: _create_clips_from_segments å·²åˆ é™¤ (2025-01-27)
# è¯¥å‡½æ•°ä»æœªè¢«è°ƒç”¨ï¼Œ_process_session_multi_assets å†…éƒ¨ç›´æ¥å¤„ç† clips åˆ›å»º
# ä¿ç•™ _create_clips_from_segments_with_offset ä¾› assets.py ä½¿ç”¨


# NOTE: _process_session å·²åˆ é™¤ (2025-01-27)
# è¯¥å‡½æ•°ä»æœªè¢«è°ƒç”¨ï¼Œæ‰€æœ‰å¤„ç†éƒ½ä½¿ç”¨ _process_session_multi_assets
# åˆ é™¤çº¦ 410 è¡Œå†—ä½™ä»£ç 


async def _process_session_multi_assets(
    session_id: str,
    project_id: str,
    assets: list,
    task_type: str,
):
    """
    åå°å¤„ç†ä¼šè¯ - å¤šç´ æç‰ˆæœ¬
    æŒ‰é¡ºåºå¤„ç†å¤šä¸ªç´ æï¼Œæ‹¼æ¥åˆ°åŒä¸€æ—¶é—´è½´
    """
    import asyncio
    
    # â˜… ç”± task_type å†³å®šå¤„ç†æµç¨‹
    ai_create_mode = task_type == "ai-create"
    voice_extract_mode = task_type == "voice-extract"
    enable_llm = False
    
    logger.info(f"[Workspace] ğŸš€ å¼€å§‹å¤„ç†å¤šç´ æä¼šè¯: session={session_id}, project={project_id}, task={task_type}, assets={len(assets)}")
    logger.debug(f"[Workspace]    ai_create_mode: {ai_create_mode}, voice_extract_mode: {voice_extract_mode}")
    
    try:
        # ========================================
        # â˜… Step 0: æ¸…ç©ºé¡¹ç›®çš„æ‰€æœ‰ clips å’Œ keyframesï¼ˆé¿å…é‡å¤/æ®‹ç•™ï¼‰
        # ========================================
        logger.info(f"[Workspace] ğŸ§¹ æ¸…ç©ºé¡¹ç›® {project_id} çš„æ‰€æœ‰ clips å’Œ keyframes...")
        
        # å…ˆè·å–é¡¹ç›®çš„æ‰€æœ‰ track_ids
        tracks_result = supabase.table("tracks").select("id").eq("project_id", project_id).execute()
        track_ids = [t["id"] for t in (tracks_result.data or [])]
        
        if track_ids:
            # è·å–æ‰€æœ‰ clip_idsï¼ˆç”¨äºåˆ é™¤å…³è”çš„ keyframesï¼‰
            clips_result = supabase.table("clips").select("id").in_("track_id", track_ids).execute()
            clip_ids = [c["id"] for c in (clips_result.data or [])]
            
            # å…ˆåˆ é™¤ keyframesï¼ˆæœ‰å¤–é”®çº¦æŸï¼‰
            if clip_ids:
                try:
                    supabase.table("keyframes").delete().in_("clip_id", clip_ids).execute()
                    logger.debug(f"[Workspace]    åˆ é™¤ {len(clip_ids)} ä¸ª clips å…³è”çš„ keyframes")
                except Exception as e:
                    logger.warning(f"[Workspace]    åˆ é™¤ keyframes å¤±è´¥ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")
            
            # å†åˆ é™¤æ‰€æœ‰ clips
            try:
                supabase.table("clips").delete().in_("track_id", track_ids).execute()
                logger.debug(f"[Workspace]    åˆ é™¤ {len(track_ids)} ä¸ª tracks ä¸‹çš„æ‰€æœ‰ clips")
            except Exception as e:
                logger.warning(f"[Workspace]    åˆ é™¤ clips å¤±è´¥: {e}")
        
        logger.info(f"[Workspace] âœ… é¡¹ç›®æ¸…ç†å®Œæˆï¼Œå¼€å§‹å…¨æ–°å¤„ç†")
        
        update_progress = _create_progress_updater(session_id)
        now = datetime.utcnow().isoformat()
        
        # Step 1: è·å–æ‰€æœ‰ç´ æå…ƒæ•°æ® (0% â†’ 10%)
        _raise_if_cancelled(session_id, "å¼€å§‹å¤„ç†å¤šç´ æ")
        update_progress("fetch", 5)
        
        # æ”¶é›†æ‰€æœ‰ç´ æä¿¡æ¯
        asset_infos = []
        total_duration = 0
        max_width = 0
        max_height = 0
        
        for idx_asset, asset in enumerate(assets):
            asset_id = asset["id"]
            storage_path = asset.get("storage_path")
            logger.debug(f"[Workspace] ğŸ“¦ è·å–ç´ æ {idx_asset + 1}/{len(assets)} å…ƒæ•°æ®: {asset_id}")
            logger.debug(f"[Workspace]    storage_path: {storage_path}")
            file_url = get_file_url("clips", storage_path)
            logger.debug(f"[Workspace]    file_url: {file_url[:80]}...")
            
            # è·å–å…ƒæ•°æ®ï¼ˆåŒ…æ‹¬ç¼–ç ä¿¡æ¯ï¼‰
            metadata = await _fetch_asset_metadata(asset_id, file_url)
            logger.debug(f"[Workspace]    metadata: {metadata}")
            duration = asset.get("duration") or metadata.get("duration", 0)
            width = metadata.get("width", 1920)
            height = metadata.get("height", 1080)
            codec = metadata.get("codec", "unknown")
            needs_transcode = metadata.get("needs_transcode", False)
            
            asset_infos.append({
                "asset_id": asset_id,
                "storage_path": storage_path,  # â˜… ä¿å­˜ storage_path ç”¨äºè½¬ç 
                "file_url": file_url,
                "duration": duration,
                "duration_ms": int(duration * 1000),
                "width": width,
                "height": height,
                "order": asset.get("order_index", 0),
                "name": asset.get("name", "ç´ æ"),
                "codec": codec,  # â˜… ä¿å­˜ç¼–ç ä¿¡æ¯
                "needs_transcode": needs_transcode,  # â˜… æ˜¯å¦éœ€è¦è½¬ç 
            })
            
            total_duration += duration
            max_width = max(max_width, width)
            max_height = max(max_height, height)
        
        # æŒ‰é¡ºåºæ’åº
        asset_infos.sort(key=lambda x: x["order"])
        
        update_progress("fetch", 10)
        logger.info(f"[Workspace] âœ… è·å– {len(asset_infos)} ä¸ªç´ æå…ƒæ•°æ®ï¼Œæ€»æ—¶é•¿ {total_duration:.1f}s")
        
        # ========================================
        # â˜… Cloudflare Streamï¼šæ— éœ€æœ¬åœ°å¤„ç†
        # ========================================
        # Cloudflare è‡ªåŠ¨å¤„ç†ï¼šHLS è½¬ç ã€è‡ªé€‚åº”ç ç‡ã€CDN åˆ†å‘
        logger.info(f"[Workspace] â˜ï¸ {len(asset_infos)} ä¸ªç´ æï¼ˆCloudflare è‡ªåŠ¨å¤„ç†ï¼‰")
        
        # ========================================
        # Step 2: å¤ç”¨å·²æœ‰è½¨é“ï¼ˆfinalize_upload å·²åˆ›å»ºï¼‰
        # ========================================
        logger.info(f"[Workspace] ğŸ” æŸ¥æ‰¾å·²æœ‰è½¨é“...")
        existing_tracks = supabase.table("tracks").select("*").eq("project_id", project_id).execute()
        
        video_track_id = None
        text_track_id = None
        audio_track_id = None
        
        if existing_tracks.data:
            for track in existing_tracks.data:
                if track.get("order_index") == 0:
                    video_track_id = track["id"]
                    logger.debug(f"[Workspace]    æ‰¾åˆ°è§†é¢‘è½¨é“: {video_track_id}")
                elif track.get("order_index") == 1:
                    # order_index=1 å¯èƒ½æ˜¯å­—å¹•è½¨é“æˆ–éŸ³é¢‘è½¨é“
                    if "å­—å¹•" in track.get("name", "") or "text" in track.get("name", "").lower():
                        text_track_id = track["id"]
                        logger.debug(f"[Workspace]    æ‰¾åˆ°å­—å¹•è½¨é“: {text_track_id}")
                    else:
                        audio_track_id = track["id"]
                        logger.debug(f"[Workspace]    æ‰¾åˆ°éŸ³é¢‘è½¨é“: {audio_track_id}")
                elif track.get("order_index") == 2:
                    text_track_id = track["id"]
                    logger.debug(f"[Workspace]    æ‰¾åˆ°å­—å¹•è½¨é“(order=2): {text_track_id}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·²æœ‰è½¨é“ï¼Œæ‰åˆ›å»ºæ–°çš„ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
        if not video_track_id:
            logger.warning(f"[Workspace] âš ï¸ æœªæ‰¾åˆ°è§†é¢‘è½¨é“ï¼Œåˆ›å»ºæ–°çš„ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼ï¼‰")
            video_track_id = str(uuid4())
            supabase.table("tracks").insert({
                "id": video_track_id,
                "project_id": project_id,
                "name": "è§†é¢‘è½¨é“",
                "order_index": 0,
                "is_muted": False,
                "is_locked": False,
                "is_visible": True,
                "created_at": now,
                "updated_at": now,
            }).execute()
        
        main_track_id = video_track_id  # é»˜è®¤ä¸»è½¨é“æ˜¯è§†é¢‘è½¨
        
        # Voice Extract æ¨¡å¼å¤„ç†
        if voice_extract_mode:
            if not audio_track_id:
                audio_track_id = str(uuid4())
                supabase.table("tracks").insert({
                    "id": audio_track_id,
                    "project_id": project_id,
                    "name": "åŸå£°éŸ³é¢‘",
                    "order_index": 1,
                    "is_muted": False,
                    "is_locked": False,
                    "is_visible": True,
                    "created_at": now,
                    "updated_at": now,
                }).execute()
            main_track_id = audio_track_id
        
        if not text_track_id:
            logger.warning(f"[Workspace] âš ï¸ æœªæ‰¾åˆ°å­—å¹•è½¨é“ï¼Œåˆ›å»ºæ–°çš„ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼ï¼‰")
            text_track_id = str(uuid4())
            supabase.table("tracks").insert({
                "id": text_track_id,
                "project_id": project_id,
                "name": "å­—å¹•è½¨é“",
                "order_index": 2 if voice_extract_mode else 1,
                "is_muted": False,
                "is_locked": False,
                "is_visible": True,
                "created_at": now,
                "updated_at": now,
            }).execute()
        
        logger.info(f"[Workspace] âœ… è½¨é“å‡†å¤‡å®Œæˆ: video={video_track_id}, text={text_track_id}")
        
        # ========================================
        # â˜… AI-Create æ¨¡å¼ï¼šå…ˆ ASRï¼Œå†æŒ‰è¯­éŸ³åˆ‡ç‰‡
        # â˜… Voice-Extract æ¨¡å¼ï¼šæ•´ä½“ clip + å­—å¹•
        # ========================================
        
        all_video_clips = []
        all_subtitle_clips = []
        all_keyframes = []
        total_segments = 0
        timeline_position = 0  # æ—¶é—´è½´ä½ç½®ï¼ˆæ¯«ç§’ï¼‰
        
        progress_per_asset = 70 / len(asset_infos)  # æ¯ä¸ªç´ æå  70% è¿›åº¦
        
        for idx, info in enumerate(asset_infos):
            # æ¯ä¸ªç´ æå¤„ç†å‰æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            _raise_if_cancelled(session_id, f"å¤„ç†ç´ æ {idx + 1}/{len(asset_infos)} å‰")
            
            asset_id = info["asset_id"]
            file_url = info["file_url"]
            storage_path = info.get("storage_path", "")
            asset_duration_ms = info["duration_ms"]
            
            if asset_duration_ms <= 0:
                asset_duration_ms = 10000
                logger.warning(f"[Workspace] âš ï¸ Asset {asset_id} æ— æ—¶é•¿ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤ 10s")
            
            base_progress = 10 + int(idx * progress_per_asset)
            logger.info(f"[Workspace] ğŸ“¹ å¤„ç†ç´ æ {idx + 1}/{len(asset_infos)}: {info['name'][:30]}...")
            logger.debug(f"[Workspace]    asset_id: {asset_id}, æ—¶é•¿: {info['duration']:.1f}s, æ¨¡å¼: {'AIæ™ºèƒ½åˆ‡ç‰‡' if ai_create_mode else 'æ•´ä½“æå–'}")
            
            # â˜… Cloudflare è§†é¢‘ï¼šæ— éœ€ç­‰å¾… MP4ï¼ŒASR ä¼šä» HLS æå–éŸ³é¢‘
            if storage_path.startswith("cloudflare:"):
                logger.info(f"[Workspace] â˜ï¸ Cloudflare è§†é¢‘ï¼ŒASR å°†ä» HLS æå–éŸ³é¢‘")
            
            # Step 1: ASR è½¬å†™ï¼ˆè·å–è¯­éŸ³ç‰‡æ®µï¼‰
            transcript_segments = []
            logger.debug(f"[Workspace] ğŸ™ï¸ å¼€å§‹ ASR è½¬å†™ç´ æ {idx + 1}...")
            update_progress("transcribe", base_progress + 5)
            
            if idx > 0:
                logger.debug(f"[Workspace]    â³ ç­‰å¾… 2 ç§’é¿å… API é™æµ...")
                await asyncio.sleep(2)
            
            try:
                transcript_segments = await _run_asr(
                    file_url, 
                    update_progress,
                    base_progress,
                    int(progress_per_asset),
                    asset_id=asset_id,
                    video_duration_sec=info['duration']
                )
                
                logger.info(f"[Workspace] âœ… ASR å®Œæˆç´ æ {idx + 1}, è¯†åˆ« {len(transcript_segments)} ä¸ªç‰‡æ®µ")
                _raise_if_cancelled(session_id, f"ç´ æ {idx + 1} ASR å")
                
                breath_count = sum(1 for seg in transcript_segments if (seg.get("silence_info") or {}).get("classification") == "breath")
                speech_count = sum(1 for seg in transcript_segments if not seg.get("silence_info"))
                logger.debug(f"[Workspace]    å…¶ä¸­: è¯­éŸ³ç‰‡æ®µ {speech_count} ä¸ª, æ¢æ°”ç‰‡æ®µ {breath_count} ä¸ª")
                total_segments += len(transcript_segments)
            except Exception as asr_err:
                logger.error(f"[Workspace] âŒ ASR è½¬å†™ç´ æ {idx + 1} å¤±è´¥: {asr_err}")
                import traceback
                traceback.print_exc()
            
            # ========================================
            # Step 2: åˆ›å»º Video Clips
            # ========================================
            
            if ai_create_mode and transcript_segments:
                # â˜…â˜…â˜… AI-Create æ¨¡å¼ï¼šå®Œæ•´ä¸€é”®æˆç‰‡æµç¨‹ â˜…â˜…â˜…
                # ä½¿ç”¨ AIVideoCreatorService å¤„ç†ï¼š
                # 1. æ™ºèƒ½åˆ‡ç‰‡ (å·²æœ‰ ASR ç»“æœ)
                # 2. è§†è§‰åˆ†æ (äººè„¸æ£€æµ‹)
                # 3. LLM è¯­ä¹‰åˆ†æ (æƒ…ç»ª/é‡è¦æ€§)
                # 4. è¿é•œè§„åˆ™å¼•æ“ (å†³ç­–)
                # 5. åºåˆ—æ„ŸçŸ¥åå¤„ç† (å¤šæ ·æ€§)
                
                logger.debug(f"[Workspace] ğŸ¬ AIæ™ºèƒ½åˆ‡ç‰‡æ¨¡å¼ï¼šè°ƒç”¨ AIVideoCreatorService...")
                
                from app.services.ai_video_creator import ai_video_creator
                from app.services.transform_rules import ZoomStrategy
                
                try:
                    # è°ƒç”¨ AI æˆç‰‡æœåŠ¡ï¼ˆå¤ç”¨å·²æœ‰ ASR ç»“æœï¼‰
                    ai_result = await ai_video_creator.process(
                        video_path=file_url,  # ä½¿ç”¨ URLï¼ˆè§†è§‰åˆ†æä¼šä¸‹è½½ï¼‰
                        audio_url=file_url,
                        options={
                            "transcript_segments": transcript_segments,
                            "enable_llm": enable_llm,  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ LLM
                        }
                    )
                    
                    logger.info(f"[Workspace] âœ… AIVideoCreator å¤„ç†å®Œæˆ: {ai_result.clips_count} ä¸ªç‰‡æ®µ")
                    
                    # å°† AI ç»“æœè½¬æ¢ä¸º clips å’Œ keyframes
                    for seg_idx, smart_seg in enumerate(ai_result.segments):
                        seg_start = int(smart_seg.start)
                        seg_end = int(smart_seg.end)
                        seg_text = smart_seg.text.strip() if smart_seg.text else ""
                        seg_duration = seg_end - seg_start
                        
                        if seg_duration <= 0:
                            continue
                        
                        clip_id = str(uuid4())
                        
                        # â˜… æ¢æ°”ç‰‡æ®µï¼šä¿ç•™ï¼Œæ·»åŠ  silence_info è®©å‰ç«¯å‘å¯¼å¤„ç†
                        is_breath = smart_seg.is_breath
                        
                        if is_breath:
                            # æ¢æ°”ç‰‡æ®µï¼šåˆ›å»º clip ä½†æ ‡è®°ä¸ºæ¢æ°”ï¼Œä¾›å‰ç«¯å‘å¯¼å¤„ç†
                            clip_data = {
                                "id": clip_id,
                                "track_id": video_track_id,
                                "asset_id": asset_id,
                                "clip_type": "video",
                                "name": "[æ¢æ°”]",
                                "start_time": timeline_position,
                                "end_time": timeline_position + seg_duration,
                                "source_start": seg_start,
                                "source_end": seg_end,
                                "volume": 1.0,
                                "is_muted": False,
                                "transform": {
                                    "x": 0, "y": 0,
                                    "scaleX": 1, "scaleY": 1,
                                    "rotation": 0,
                                    "opacity": 1,
                                },
                                "speed": 1.0,
                                "metadata": {
                                    "segment_id": smart_seg.id,
                                    "asset_index": idx,
                                    "segment_index": seg_idx,
                                    "silence_info": {
                                        "classification": "breath",
                                        "duration_ms": seg_duration,
                                    },
                                },
                                "created_at": now,
                                "updated_at": now,
                            }
                            all_video_clips.append(clip_data)
                            
                            # æ¢æ°”ç‰‡æ®µä¸éœ€è¦å­—å¹•å’Œè¿é•œ
                            logger.debug(f"[Workspace]    Clip {seg_idx + 1} [æ¢æ°”]: {timeline_position}~{timeline_position + seg_duration}ms")
                            timeline_position += seg_duration
                            continue
                        
                        # è·³è¿‡ç©ºæ–‡æœ¬çš„éæ¢æ°”ç‰‡æ®µ
                        if not seg_text:
                            continue
                        
                        # è¯­éŸ³ç‰‡æ®µ
                        clip_data = {
                            "id": clip_id,
                            "track_id": video_track_id,
                            "asset_id": asset_id,
                            "clip_type": "video",
                            "name": seg_text[:20] + ("..." if len(seg_text) > 20 else ""),
                            "start_time": timeline_position,
                            "end_time": timeline_position + seg_duration,
                            "source_start": seg_start,
                            "source_end": seg_end,
                            "volume": 1.0,
                            "is_muted": False,
                            "transform": {
                                "x": 0, "y": 0,
                                "scaleX": 1, "scaleY": 1,
                                "rotation": 0,
                                "opacity": 1,
                            },
                            "speed": 1.0,
                            "metadata": {
                                "segment_id": smart_seg.id,
                                "asset_index": idx,
                                "segment_index": seg_idx,
                                "emotion": smart_seg.emotion.value if smart_seg.emotion else "neutral",
                                "importance": smart_seg.importance.value if smart_seg.importance else "medium",
                                "has_face": smart_seg.has_face,
                                "rule_applied": smart_seg.transform.get("_rule_applied") if smart_seg.transform else None,
                            },
                            "created_at": now,
                            "updated_at": now,
                        }
                        all_video_clips.append(clip_data)
                        
                        # å­—å¹• clipsï¼ˆç»†åˆ†ï¼‰
                        fine_subs = _split_segments_by_punctuation([{
                            "id": smart_seg.id,
                            "start": seg_start,
                            "end": seg_end,
                            "text": seg_text,
                        }])
                        for sub_idx, sub_seg in enumerate(fine_subs):
                            sub_start = sub_seg.get("start", seg_start)
                            sub_end = sub_seg.get("end", seg_end)
                            sub_text = sub_seg.get("text", "").strip()
                            sub_duration = sub_end - sub_start
                            
                            if sub_duration <= 0 or not sub_text:
                                continue
                            
                            sub_offset = sub_start - seg_start
                            
                            all_subtitle_clips.append({
                                "id": str(uuid4()),
                                "track_id": text_track_id,
                                "clip_type": "subtitle",
                                "parent_clip_id": clip_id,
                                "start_time": timeline_position + sub_offset,
                                "end_time": timeline_position + sub_offset + sub_duration,
                                "source_start": 0,
                                "source_end": sub_duration,
                                "is_muted": False,
                                "content_text": sub_text,
                                "text_style": {
                                    "fontSize": 15,
                                    "fontColor": "#FFFFFF",
                                    "backgroundColor": "transparent",
                                    "alignment": "center",
                                    "maxWidth": "95%",
                                },
                                "transform": {"x": 0, "y": 150, "scale": 1},
                                "metadata": {
                                    "segment_id": smart_seg.id,
                                    "asset_index": idx,
                                    "order_index": seg_idx * 100 + sub_idx,
                                },
                                "created_at": now,
                                "updated_at": now,
                            })
                        
                        # â˜… Keyframesï¼šæ ¹æ® AI è¿é•œå†³ç­–ç”Ÿæˆ
                        if smart_seg.transform_params:
                            params = smart_seg.transform_params
                            
                            # èµ·å§‹å…³é”®å¸§
                            all_keyframes.append({
                                "id": str(uuid4()),
                                "clip_id": clip_id,
                                "property": "scale",
                                "offset": 0.0,
                                "value": {"x": params.start_scale, "y": params.start_scale},
                                "easing": "ease_in_out",
                                "created_at": now,
                                "updated_at": now,
                            })
                            
                            # ç»“æŸå…³é”®å¸§
                            all_keyframes.append({
                                "id": str(uuid4()),
                                "clip_id": clip_id,
                                "property": "scale",
                                "offset": 1.0,
                                "value": {"x": params.end_scale, "y": params.end_scale},
                                "easing": params.easing.value if hasattr(params.easing, 'value') else str(params.easing),
                                "created_at": now,
                                "updated_at": now,
                            })
                            
                            # ä½ç§»å…³é”®å¸§ï¼ˆå¦‚æœæœ‰ä½ç§»ï¼‰
                            if abs(params.position_x) > 0.01 or abs(params.position_y) > 0.01:
                                all_keyframes.append({
                                    "id": str(uuid4()),
                                    "clip_id": clip_id,
                                    "property": "position",
                                    "offset": 0.0,
                                    "value": {"x": 0, "y": 0},
                                    "easing": "ease_in_out",
                                    "created_at": now,
                                    "updated_at": now,
                                })
                                all_keyframes.append({
                                    "id": str(uuid4()),
                                    "clip_id": clip_id,
                                    "property": "position",
                                    "offset": 1.0,
                                    "value": {"x": params.position_x, "y": params.position_y},
                                    "easing": params.easing.value if hasattr(params.easing, 'value') else str(params.easing),
                                    "created_at": now,
                                    "updated_at": now,
                                })
                            
                            logger.debug(f"[Workspace]    Clip {seg_idx + 1}: {timeline_position}~{timeline_position + seg_duration}ms, "
                                       f"rule={params.rule_applied}, scale={params.start_scale:.2f}â†’{params.end_scale:.2f}")
                        else:
                            # Fallback: ç®€å•æ…¢æ¨
                            all_keyframes.append({
                                "id": str(uuid4()),
                                "clip_id": clip_id,
                                "property": "scale",
                                "offset": 0.0,
                                "value": {"x": 1.0, "y": 1.0},
                                "easing": "ease_in_out",
                                "created_at": now,
                                "updated_at": now,
                            })
                            all_keyframes.append({
                                "id": str(uuid4()),
                                "clip_id": clip_id,
                                "property": "scale",
                                "offset": 1.0,
                                "value": {"x": 1.08, "y": 1.08},
                                "easing": "linear",
                                "created_at": now,
                                "updated_at": now,
                            })
                        
                        timeline_position += seg_duration
                    
                    logger.debug(f"[Workspace] âœ… AIåˆ‡ç‰‡å®Œæˆ: {len([c for c in all_video_clips if c.get('asset_id') == asset_id])} ä¸ª video clips")
                    
                except Exception as ai_err:
                    logger.error(f"[Workspace] âŒ AIVideoCreator å¤„ç†å¤±è´¥: {ai_err}")
                    import traceback
                    traceback.print_exc()
                    
                    # Fallback: ä½¿ç”¨ç®€å•åˆ‡ç‰‡é€»è¾‘
                    logger.debug(f"[Workspace] âš ï¸ é™çº§ä¸ºç®€å•åˆ‡ç‰‡æ¨¡å¼...")
                    speech_segments = [seg for seg in transcript_segments 
                                      if seg.get("text", "").strip() and not seg.get("silence_info")]
                    
                    for seg_idx, seg in enumerate(speech_segments):
                        seg_start = seg.get("start", 0)
                        seg_end = seg.get("end", 0)
                        seg_text = seg.get("text", "").strip()
                        seg_duration = seg_end - seg_start
                        
                        if seg_duration <= 0:
                            continue
                        
                        clip_id = str(uuid4())
                        clip_data = {
                            "id": clip_id,
                            "track_id": video_track_id,
                            "asset_id": asset_id,
                            "clip_type": "video",
                            "name": seg_text[:20] + ("..." if len(seg_text) > 20 else ""),
                            "start_time": timeline_position,
                            "end_time": timeline_position + seg_duration,
                            "source_start": seg_start,
                            "source_end": seg_end,
                            "volume": 1.0,
                            "is_muted": False,
                            "transform": {"x": 0, "y": 0, "scaleX": 1, "scaleY": 1, "rotation": 0, "opacity": 1},
                            "speed": 1.0,
                            "created_at": now,
                            "updated_at": now,
                        }
                        all_video_clips.append(clip_data)
                        
                        # ç®€å• keyframes
                        all_keyframes.append({
                            "id": str(uuid4()),
                            "clip_id": clip_id,
                            "property": "scale",
                            "offset": 0.0,
                            "value": {"x": 1.0, "y": 1.0},
                            "easing": "ease_in_out",
                            "created_at": now,
                            "updated_at": now,
                        })
                        all_keyframes.append({
                            "id": str(uuid4()),
                            "clip_id": clip_id,
                            "property": "scale",
                            "offset": 1.0,
                            "value": {"x": 1.08, "y": 1.08},
                            "easing": "linear",
                            "created_at": now,
                            "updated_at": now,
                        })
                        
                        timeline_position += seg_duration
                
            else:
                # â˜… Voice-Extract æˆ–æ—  ASR ç»“æœï¼šåˆ›å»ºæ•´ä½“ clip
                logger.debug(f"[Workspace] ğŸ¬ æ•´ä½“æ¨¡å¼ï¼šåˆ›å»ºå•ä¸ª clip...")
                
                clip_id = str(uuid4())
                clip_data = {
                    "id": clip_id,
                    "track_id": video_track_id,
                    "asset_id": asset_id,
                    "clip_type": "video",
                    "name": info.get("name", "è§†é¢‘"),
                    "start_time": timeline_position,
                    "end_time": timeline_position + asset_duration_ms,
                    "source_start": 0,
                    "source_end": asset_duration_ms,
                    "volume": 1.0,
                    "is_muted": False,
                    "transform": {
                        "x": 0, "y": 0,
                        "scaleX": 1, "scaleY": 1,
                        "rotation": 0,
                        "opacity": 1,
                    },
                    "speed": 1.0,
                    "created_at": now,
                    "updated_at": now,
                }
                all_video_clips.append(clip_data)
                
                # åˆ›å»ºå­—å¹• clipsï¼ˆå¦‚æœæœ‰ ASR ç»“æœï¼‰
                if transcript_segments:
                    subtitle_clips = await _create_subtitle_clips_only(
                        transcript_segments=transcript_segments,
                        text_track_id=text_track_id,
                        video_clip_id=clip_id,
                        timeline_offset=timeline_position,
                        asset_index=idx,
                    )
                    all_subtitle_clips.extend(subtitle_clips)
                
                logger.debug(f"[Workspace]    åˆ›å»º clip: {clip_id}, {timeline_position}~{timeline_position + asset_duration_ms}ms")
                timeline_position += asset_duration_ms
            
            logger.debug(f"[Workspace] âœ… ç´ æ {idx + 1} å¤„ç†å®Œæˆ")
        
        # ========================================
        # Step 3: æ‰¹é‡æ’å…¥ clips å’Œ keyframes
        # ========================================
        logger.debug(f"[Workspace] ğŸ“¦ æ‰¹é‡æ’å…¥æ•°æ®åº“...")
        
        if all_video_clips:
            try:
                supabase.table("clips").insert(all_video_clips).execute()
                logger.debug(f"[Workspace] âœ… åˆ›å»º {len(all_video_clips)} ä¸ª video clips")
            except Exception as e:
                logger.error(f"[Workspace] âŒ åˆ›å»º video clips å¤±è´¥: {e}")
                raise
        
        if all_keyframes:
            try:
                supabase.table("keyframes").insert(all_keyframes).execute()
                logger.debug(f"[Workspace] âœ… åˆ›å»º {len(all_keyframes)} ä¸ª keyframes")
            except Exception as e:
                logger.warning(f"[Workspace] âš ï¸ æ’å…¥ keyframes å¤±è´¥: {e}")
        
        if all_subtitle_clips:
            try:
                supabase.table("clips").insert(all_subtitle_clips).execute()
                logger.debug(f"[Workspace] âœ… åˆ›å»º {len(all_subtitle_clips)} ä¸ªå­—å¹• clips")
            except Exception as e:
                logger.warning(f"[Workspace] âš ï¸ åˆ›å»ºå­—å¹• clips å¤±è´¥: {e}")
        
        # ========================================
        # â˜… Cloudflare ç®€åŒ–ï¼šæ— éœ€ç­‰å¾… HLS ä»»åŠ¡
        # ========================================
        update_progress("prepare", 95)
        
        # æ›´æ–°é¡¹ç›®
        fps = 30
        supabase.table("projects").update({
            "status": "ready",
            "resolution": {"width": max_width, "height": max_height},
            "fps": fps,
            "updated_at": now,
        }).eq("id", project_id).execute()
        
        # æ›´æ–°æ‰€æœ‰ç´ æçŠ¶æ€
        for info in asset_infos:
            supabase.table("assets").update({
                "status": "ready",
                "updated_at": now,
            }).eq("id", info["asset_id"]).execute()
        
        update_progress("prepare", 100)
        
        # æ ‡è®°ä¼šè¯å®Œæˆ
        supabase.table("workspace_sessions").update({
            "status": "completed",
            "progress": 100,
            "current_step": "completed",
            "transcript_segments": total_segments,
            "completed_at": now,
            "updated_at": now,
        }).eq("id", session_id).execute()
        
        logger.info(f"[Workspace] âœ… å¤šç´ æä¼šè¯ {session_id} å¤„ç†å®Œæˆï¼Œå…± {len(asset_infos)} ä¸ªç´ æ")
        
    except SessionCancelledException:
        # ç”¨æˆ·ä¸»åŠ¨å–æ¶ˆï¼Œä¸éœ€è¦æ›´æ–°çŠ¶æ€ï¼ˆå·²ç»æ˜¯ cancelledï¼‰
        logger.info(f"[Workspace] ğŸ›‘ å¤šç´ æä¼šè¯ {session_id} å¤„ç†å·²è¢«ç”¨æˆ·å–æ¶ˆ")
        return
    except Exception as e:
        logger.error(f"[Workspace] âŒ å¤šç´ æå¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        supabase.table("workspace_sessions").update({
            "status": "failed",
            "error_message": str(e),
            "updated_at": datetime.utcnow().isoformat(),
        }).eq("id", session_id).execute()
        
        supabase.table("projects").update({
            "status": "draft",
            "updated_at": datetime.utcnow().isoformat(),
        }).eq("id", project_id).execute()


async def _create_clips_from_segments_with_offset(
    project_id: str,
    asset_id: str,
    transcript_segments: list,
    video_track_id: str,
    text_track_id: str,
    timeline_offset: int = 0,
    asset_index: int = 0,
    enable_llm: bool = False,
    enable_smart_camera: bool = True,
    enable_subtitle: bool = True,
) -> tuple:
    """
    æ ¹æ® ASR segments åˆ›å»ºè§†é¢‘å’Œå­—å¹• clipsï¼ˆå¸¦æ—¶é—´è½´åç§»ï¼‰
    ç”¨äºå¤šç´ ææ‹¼æ¥åœºæ™¯
    
    Args:
        timeline_offset: åœ¨æ—¶é—´è½´ä¸Šçš„èµ·å§‹ä½ç½®ï¼ˆæ¯«ç§’ï¼‰
        asset_index: ç´ æç´¢å¼•ï¼Œç”¨äºå‘½å
        enable_llm: æ˜¯å¦å¯ç”¨ LLM è¯­ä¹‰åˆ†æ
        enable_smart_camera: æ˜¯å¦å¯ç”¨æ™ºèƒ½è¿é•œï¼ˆå¦‚æœä¸º Falseï¼Œåˆ™ä¸è¿›è¡Œè£å‰ª/ç¼©æ”¾ï¼‰
        enable_subtitle: æ˜¯å¦ç”Ÿæˆå­—å¹• clipsï¼ˆå¦‚æœä¸º Falseï¼Œåªç”Ÿæˆè§†é¢‘ clipsï¼‰
        
    Returns:
        tuple: (video_clips, subtitle_clips, keyframes)
        - keyframes: å¾…æ’å…¥ keyframes è¡¨çš„è®°å½•åˆ—è¡¨
    """
    logger.info(f"[Workspace] ======== åˆ‡ç‰‡å‡½æ•°å¼€å§‹ ========")
    logger.info(f"[Workspace] ğŸ¬ _create_clips_from_segments_with_offset")
    logger.info(f"[Workspace]    asset_id: {asset_id}")
    logger.info(f"[Workspace]    asset_index: {asset_index}")
    logger.info(f"[Workspace]    timeline_offset: {timeline_offset}ms")
    logger.info(f"[Workspace]    enable_smart_camera: {enable_smart_camera}")
    logger.info(f"[Workspace]    enable_subtitle: {enable_subtitle}")
    logger.info(f"[Workspace]    è¾“å…¥ segments æ•°é‡: {len(transcript_segments)}")
    
    # â˜… å…³é”®æ—¥å¿—ï¼šæ‰“å°å‰å‡ ä¸ª segments çš„å†…å®¹ï¼ŒéªŒè¯ ASR ç»“æœå’Œ asset å¯¹åº”
    if transcript_segments:
        for i, seg in enumerate(transcript_segments[:3]):
            seg_text = seg.get("text", "")[:30]
            logger.info(f"[Workspace]    segment[{i}]: start={seg.get('start')} text='{seg_text}...'")
    
    now = datetime.utcnow().isoformat()
    
    # æŒ‰æ—¶é—´é¡ºåºæ’åºåŸå§‹ segments
    sorted_segments = sorted(transcript_segments, key=lambda s: s.get("start", 0))
    
    video_clips = []
    subtitle_clips = []
    all_keyframes = []  # â˜… æ”¶é›†æ‰€æœ‰å…³é”®å¸§è®°å½•
    timeline_position = timeline_offset
    
    # ç»Ÿè®¡è‡ªåŠ¨è·³è¿‡çš„é™éŸ³ç‰‡æ®µ
    auto_skipped = {"dead_air": 0, "long_pause": 0, "hesitation": 0}
    
    # ========== ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æœ‰æ•ˆç‰‡æ®µ ==========
    valid_segments = []  # å­˜å‚¨ (seg_idx, seg, seg_duration, clip_name, is_breath, silence_info)
    
    for seg_idx, seg in enumerate(sorted_segments):
        seg_start = seg.get("start", 0)
        seg_end = seg.get("end", 0)
        seg_text = seg.get("text", "").strip()
        seg_duration = seg_end - seg_start
        
        if seg_duration <= 0:
            continue
        
        # æ£€æŸ¥é™éŸ³ä¿¡æ¯
        silence_info = seg.get("silence_info")
        
        if silence_info:
            cls = silence_info.get("classification")
            if cls in ("dead_air", "long_pause", "hesitation"):
                logger.info(f"[Workspace]   âš ï¸ è·³è¿‡ segment[{seg_idx}]: type={cls} start={seg_start} end={seg_end}")
                auto_skipped[cls] = auto_skipped.get(cls, 0) + 1
                continue
        
        # ç¡®å®šç‰‡æ®µç±»å‹
        clip_name = f"ç´ æ{asset_index + 1}-ç‰‡æ®µ{seg_idx + 1}"
        is_breath = False
        if silence_info and silence_info.get("classification") == "breath":
            clip_name = f"ç´ æ{asset_index + 1}-æ¢æ°”"
            is_breath = True
        
        valid_segments.append((seg_idx, seg, seg_duration, clip_name, is_breath, silence_info))
    
    # ========== ç¬¬äºŒé˜¶æ®µï¼šLLM è¯­ä¹‰åˆ†æï¼ˆå¯é€‰ï¼‰==========
    # å­˜å‚¨åˆ†æç»“æœ: {segment_id: SegmentAnalysis}
    llm_results: Dict[str, Any] = {}
    if enable_llm and valid_segments:
        from ..services.llm import llm_service
        
        if llm_service.is_configured():
            logger.info(f"[Workspace] ğŸ¤– å¼€å§‹ LLM è¯­ä¹‰åˆ†æ...")
            # æ„å»ºå¾…åˆ†æçš„æ–‡æœ¬ç‰‡æ®µ
            text_segments = []
            for seg_idx, seg, seg_duration, clip_name, is_breath, silence_info in valid_segments:
                seg_text = seg.get("text", "").strip()
                if seg_text and not is_breath:
                    text_segments.append({"id": str(seg_idx), "text": seg_text})
            
            if text_segments:
                try:
                    emotion_result = await llm_service.analyze_emotions(text_segments)
                    # ç›´æ¥ä½¿ç”¨ SegmentAnalysis å¯¹è±¡
                    for seg_analysis in emotion_result.results:
                        llm_results[seg_analysis.id] = seg_analysis
                    logger.info(f"[Workspace] âœ… LLM åˆ†æå®Œæˆ: {len(llm_results)} æ¡ç»“æœ")
                except Exception as e:
                    logger.warning(f"[Workspace] âš ï¸ LLM åˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            else:
                logger.info(f"[Workspace]    æ— æ–‡æœ¬ç‰‡æ®µéœ€è¦åˆ†æ")
        else:
            logger.info(f"[Workspace] âš ï¸ LLM API æœªé…ç½®ï¼Œè·³è¿‡è¯­ä¹‰åˆ†æ")
    
    # ========== ç¬¬ä¸‰é˜¶æ®µï¼šæ‰¹é‡ç”Ÿæˆ transformï¼ˆå«åºåˆ—åå¤„ç†ï¼‰==========
    # é‡ç½®åºåˆ—å¤„ç†å™¨çŠ¶æ€
    sequence_processor.reset()
    
    # å£æ’­æ¨¡å¼é»˜è®¤äººè„¸ä½ç½®ï¼ˆä¸å•ç´ ææµç¨‹ä¸€è‡´ï¼‰
    DEFAULT_FACE_CENTER_X = 0.5   # å±…ä¸­
    DEFAULT_FACE_CENTER_Y = 0.35  # ç¨å¾®åä¸Šï¼ˆå£æ’­å¸¸è§æ„å›¾ï¼‰
    
    # æ„å»ºä¸Šä¸‹æ–‡åˆ—è¡¨
    contexts = []
    for seg_idx, seg, seg_duration, clip_name, is_breath, silence_info in valid_segments:
        seg_text = seg.get("text", "").strip()
        seg_id_str = str(seg_idx)
        
        # ä» LLM ç»“æœè·å–æƒ…ç»ªå’Œé‡è¦æ€§ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
        seg_analysis = llm_results.get(seg_id_str)
        emotion = seg_analysis.emotion if seg_analysis else EmotionType.NEUTRAL
        importance = seg_analysis.importance if seg_analysis else ImportanceLevel.MEDIUM
        
        context = SegmentContext(
            segment_id=seg_id_str,
            duration_ms=seg_duration,
            text=seg_text,
            # å£æ’­æ¨¡å¼ï¼šé»˜è®¤æœ‰äººè„¸ï¼Œå±…ä¸­åä¸Šï¼ˆä¸å•ç´ ææµç¨‹ä¸€è‡´ï¼‰
            has_face=True,
            face_center_x=DEFAULT_FACE_CENTER_X,
            face_center_y=DEFAULT_FACE_CENTER_Y,
            # ä½¿ç”¨ LLM åˆ†æç»“æœæˆ–é»˜è®¤å€¼
            emotion=emotion,
            importance=importance,
            is_breath=is_breath,
        )
        contexts.append(context)
    
    logger.info(f"[Workspace] ğŸ¥ enable_smart_camera={enable_smart_camera}, å¾…å¤„ç† {len(contexts)} ä¸ªç‰‡æ®µ")
    
    # ä½¿ç”¨è§„åˆ™å¼•æ“æ‰¹é‡å¤„ç†
    if enable_smart_camera:
        params_list = [transform_engine.process(ctx) for ctx in contexts]
        
        # åºåˆ—æ„ŸçŸ¥åå¤„ç†ï¼šç¡®ä¿è¿é•œå¤šæ ·æ€§ï¼ˆä¸å•ç´ ææµç¨‹ä¸€è‡´ï¼‰
        params_contexts = list(zip(params_list, contexts))
        processed_params = sequence_processor.process_batch(params_contexts)
    else:
        # ç¦ç”¨æ™ºèƒ½è¿é•œï¼Œç”Ÿæˆé»˜è®¤é™æ€å‚æ•°
        processed_params = [
            TransformParams(strategy=ZoomStrategy.STATIC, rule_applied="disabled_by_user")
            for _ in contexts
        ]
    
    # ========== ç¬¬ä¸‰é˜¶æ®µï¼šæ„å»º clips ==========
    for i, (seg_idx, seg, seg_duration, clip_name, is_breath, silence_info) in enumerate(valid_segments):
        seg_start = seg.get("start", 0)
        seg_end = seg.get("end", 0)
        seg_text = seg.get("text", "").strip()
        
        video_clip_id = str(uuid4())
        
        # è·å–æ‰¹å¤„ç†åçš„ transform
        transform_params = processed_params[i]
        
        # â˜… æ–° APIï¼šç›´æ¥è·å–å…ƒä¿¡æ¯å’Œå…³é”®å¸§
        transform_meta = transform_params.get_meta()
        clip_keyframes = transform_params.get_keyframes_for_db(video_clip_id, seg_duration)
        
        logger.info(f"[Workspace]   âœ… åˆ›å»º clip[{seg_idx}]: name='{clip_name}' timeline={timeline_position}~{timeline_position + seg_duration} source={seg_start}~{seg_end} rule={transform_meta.get('_rule_applied', 'none')}")
        
        video_clips.append({
            "id": video_clip_id,
            "track_id": video_track_id,
            "asset_id": asset_id,
            "clip_type": "video",
            "start_time": timeline_position,
            "end_time": timeline_position + seg_duration,
            "source_start": seg_start,
            "source_end": seg_end,
            "is_muted": False,
            "name": clip_name,
            "transform": transform_meta,  # åªå­˜å…ƒä¿¡æ¯
            "created_at": now,
            "updated_at": now,
            "metadata": {
                "silence_info": silence_info,
                "original_text": seg_text,
                "asset_index": asset_index,
            }
        })
        
        # æ”¶é›†å…³é”®å¸§ï¼ˆç»Ÿä¸€å­˜å‚¨åˆ° keyframes è¡¨ï¼‰
        all_keyframes.extend(clip_keyframes)
        
        # åˆ›å»ºå­—å¹• clipsï¼ˆä»…å½“ enable_subtitle=True æ—¶ï¼‰
        if seg_text and enable_subtitle:
            fine_subs = _split_segments_by_punctuation([seg])
            
            for sub_idx, sub_seg in enumerate(fine_subs):
                sub_start = sub_seg.get("start", seg_start)
                sub_end = sub_seg.get("end", seg_end)
                sub_text = sub_seg.get("text", "").strip()
                sub_duration = sub_end - sub_start
                
                if sub_duration <= 0 or not sub_text:
                    continue
                
                subtitle_timeline_start = timeline_position + (sub_start - seg_start)
                
                subtitle_clips.append({
                    "id": str(uuid4()),
                    "track_id": text_track_id,
                    "clip_type": "subtitle",
                    "parent_clip_id": video_clip_id,
                    "start_time": subtitle_timeline_start,
                    "end_time": subtitle_timeline_start + sub_duration,
                    "source_start": 0,
                    "source_end": sub_duration,
                    "is_muted": False,
                    "content_text": sub_text,
                    "text_style": {
                        "fontSize": 15,
                        "fontColor": "#FFFFFF",
                        "backgroundColor": "transparent",
                        "alignment": "center",
                        "maxWidth": "95%",
                    },
                    "transform": {
                        "x": 0,
                        "y": 150,
                        "scale": 1,
                    },
                    "metadata": {
                        "segment_id": seg.get("id"),
                        "asset_index": asset_index,
                        "order_index": seg_idx * 100 + sub_idx,
                        "original_start": sub_start,
                        "original_end": sub_end,
                    },
                    "created_at": now,
                    "updated_at": now,
                })
        
        timeline_position += seg_duration
    
    logger.info(f"[Workspace] ======== åˆ‡ç‰‡å‡½æ•°ç»“æŸ ========")
    logger.info(f"[Workspace] ğŸ“Š ç´ æ {asset_index + 1} ç»Ÿè®¡:")
    logger.info(f"[Workspace]    è¾“å…¥ segments: {len(sorted_segments)}")
    logger.info(f"[Workspace]    åˆ›å»ºè§†é¢‘ clips: {len(video_clips)}")
    logger.info(f"[Workspace]    åˆ›å»ºå­—å¹• clips: {len(subtitle_clips)}")
    logger.info(f"[Workspace]    æå–å…³é”®å¸§æ•°: {len(all_keyframes)}")
    logger.info(f"[Workspace]    è·³è¿‡é™éŸ³: {auto_skipped}")
    logger.info(f"[Workspace]    æœ€ç»ˆ timeline ä½ç½®: {timeline_position}ms")
    
    return video_clips, subtitle_clips, all_keyframes


async def _create_subtitle_clips_only(
    transcript_segments: list,
    text_track_id: str,
    video_clip_id: str = None,
    timeline_offset: int = 0,
    asset_index: int = 0,
) -> list:
    """
    â˜… åªåˆ›å»ºå­—å¹• clipsï¼Œä¸åˆ›å»º video clips
    ç”¨äº confirm_upload é˜¶æ®µï¼Œvideo clips å·²ç”± finalize_upload åˆ›å»º
    
    Args:
        transcript_segments: ASR è½¬å†™ç»“æœ
        text_track_id: å­—å¹•è½¨é“ ID
        video_clip_id: å…³è”çš„è§†é¢‘ clip ID (ç”¨äº parent_clip_id)
        timeline_offset: æ—¶é—´è½´åç§»ï¼ˆæ¯«ç§’ï¼‰
        asset_index: ç´ æç´¢å¼•
    
    Returns:
        list: å­—å¹• clips åˆ—è¡¨
    """
    logger.info(f"[Workspace] ğŸ“ _create_subtitle_clips_only")
    logger.info(f"[Workspace]    text_track_id: {text_track_id}")
    logger.info(f"[Workspace]    video_clip_id: {video_clip_id}")
    logger.info(f"[Workspace]    timeline_offset: {timeline_offset}ms")
    logger.info(f"[Workspace]    segments count: {len(transcript_segments)}")
    
    now = datetime.utcnow().isoformat()
    subtitle_clips = []
    
    # è¿‡æ»¤æœ‰æ•ˆçš„è¯­éŸ³ç‰‡æ®µï¼ˆè·³è¿‡é™éŸ³ï¼‰
    sorted_segments = sorted(transcript_segments, key=lambda s: s.get("start", 0))
    
    for seg_idx, seg in enumerate(sorted_segments):
        seg_start = seg.get("start", 0)
        seg_end = seg.get("end", 0)
        seg_text = seg.get("text", "").strip()
        seg_duration = seg_end - seg_start
        
        if seg_duration <= 0 or not seg_text:
            continue
        
        # è·³è¿‡é™éŸ³ç‰‡æ®µ
        silence_info = seg.get("silence_info")
        if silence_info:
            cls = silence_info.get("classification")
            if cls in ("dead_air", "long_pause", "hesitation", "breath"):
                continue
        
        # ç»†åˆ†å­—å¹•ï¼ˆæŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼‰
        fine_subs = _split_segments_by_punctuation([seg])
        
        for sub_idx, sub_seg in enumerate(fine_subs):
            sub_start = sub_seg.get("start", seg_start)
            sub_end = sub_seg.get("end", seg_end)
            sub_text = sub_seg.get("text", "").strip()
            sub_duration = sub_end - sub_start
            
            if sub_duration <= 0 or not sub_text:
                continue
            
            # â˜… è®¡ç®—æ—¶é—´è½´ä½ç½®ï¼šä½¿ç”¨ source æ—¶é—´ï¼ˆç›¸å¯¹äºè§†é¢‘å¼€å§‹ï¼‰
            subtitle_timeline_start = timeline_offset + sub_start
            
            subtitle_clips.append({
                "id": str(uuid4()),
                "track_id": text_track_id,
                "clip_type": "subtitle",
                "parent_clip_id": video_clip_id,
                "start_time": subtitle_timeline_start,
                "end_time": subtitle_timeline_start + sub_duration,
                "source_start": 0,
                "source_end": sub_duration,
                "is_muted": False,
                "content_text": sub_text,
                "text_style": {
                    "fontSize": 15,
                    "fontColor": "#FFFFFF",
                    "backgroundColor": "transparent",
                    "alignment": "center",
                    "maxWidth": "95%",
                },
                "transform": {
                    "x": 0,
                    "y": 150,
                    "scale": 1,
                },
                "metadata": {
                    "segment_id": seg.get("id"),
                    "asset_index": asset_index,
                    "order_index": seg_idx * 100 + sub_idx,
                    "original_start": sub_start,
                    "original_end": sub_end,
                },
                "created_at": now,
                "updated_at": now,
            })
    
    logger.info(f"[Workspace] âœ… åˆ›å»º {len(subtitle_clips)} ä¸ªå­—å¹• clips")
    return subtitle_clips