"""
Stage 2: ç»“æ„åˆ†æå±‚

åˆ†ææ¯ä¸ªç‰‡æ®µçš„å†…å®¹è§’è‰²å’Œç±»å‹ï¼Œæ˜¯æ•´ä¸ª Remotion Agent çš„æ ¸å¿ƒã€‚

èŒè´£:
1. åˆ’åˆ†ç« èŠ‚
2. è¯†åˆ«å†…å®¹ç±»å‹ (åˆ—è¡¨é¡¹ã€æ•°æ®ã€å…³é”®è¯ç­‰)
3. æ ‡è®°é‡ç‚¹
4. æå–ç»“æ„åŒ–æ•°æ® (æ•°å­—ã€å…³é”®è¯ã€å¼•ç”¨)
5. ğŸ†• æ£€æµ‹ B-Roll è§¦å‘ç‚¹
6. ğŸ†• å»ºè®®å¸ƒå±€æ¨¡å¼
"""

import logging
import json
import re
from typing import List, Dict, Any, Optional, Tuple
from pydantic import ValidationError

from app.services.llm.clients import get_llm
from .models import (
    SegmentRole,
    ContentType,
    ImportanceLevel,
    StructuredSegment,
    SegmentStructure,
    ListContext,
    ProcessContext,
    ExtractedData,
    ExtractedNumber,
    ExtractedKeyword,
    ExtractedQuote,
    GlobalStructure,
    ChapterInfo,
    StructureAnalysisResult,
)
from .prompts.structure import STRUCTURE_ANALYSIS_PROMPT
from .broll_trigger import detect_broll_triggers, detect_primary_trigger, BrollTriggerType
from .layout_modes import LayoutMode, LayoutModeSelector

logger = logging.getLogger(__name__)


async def analyze_content_structure(
    segments: List[Dict[str, Any]],
    content_understanding: Optional[Dict[str, Any]] = None,
    provider: str = "doubao"
) -> StructureAnalysisResult:
    """
    åˆ†æå†…å®¹ç»“æ„
    
    Args:
        segments: ASR è½¬å†™ç‰‡æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« {id, text, start_ms, end_ms}
        content_understanding: Stage 1 çš„ç†è§£ç»“æœï¼ˆå¯é€‰ï¼‰
        provider: LLM æä¾›å•†
        
    Returns:
        StructureAnalysisResult: ç»“æ„åŒ–åˆ†æç»“æœ
    """
    if not segments:
        return StructureAnalysisResult(
            segments=[],
            global_structure=GlobalStructure()
        )
    
    # å‡†å¤‡è¾“å…¥æ–‡æœ¬
    segments_text = _format_segments_for_prompt(segments)
    
    # å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    context_hint = ""
    if content_understanding:
        context_hint = f"""
## å†…å®¹èƒŒæ™¯
- ä¸»é¢˜: {content_understanding.get('topic', 'æœªçŸ¥')}
- ç±»åˆ«: {content_understanding.get('category', 'æœªçŸ¥')}
- é£æ ¼: {content_understanding.get('tone', 'æœªçŸ¥')}
"""
    
    # è°ƒç”¨ LLM
    try:
        llm = get_llm(provider=provider)
        prompt = STRUCTURE_ANALYSIS_PROMPT.format(
            segments_text=segments_text,
            context_hint=context_hint
        )
        
        response = await llm.ainvoke(prompt)
        result_text = response.content if hasattr(response, 'content') else str(response)
        
        # è§£æ JSON ç»“æœ
        parsed_result = _parse_llm_response(result_text, segments)
        
        return parsed_result
        
    except Exception as e:
        logger.error(f"Structure analysis failed: {e}")
        # è¿”å›åŸºç¡€åˆ†æç»“æœï¼ˆé™çº§å¤„ç†ï¼‰
        return _fallback_analysis(segments)


def _format_segments_for_prompt(segments: List[Dict[str, Any]]) -> str:
    """æ ¼å¼åŒ–ç‰‡æ®µä¸º Prompt è¾“å…¥"""
    lines = []
    for seg in segments:
        seg_id = seg.get('id', '')
        text = seg.get('text', '')
        start_ms = seg.get('start_ms', 0)
        lines.append(f"[{seg_id}] ({start_ms}ms) {text}")
    return "\n".join(lines)


def _parse_llm_response(
    response_text: str,
    original_segments: List[Dict[str, Any]]
) -> StructureAnalysisResult:
    """
    è§£æ LLM å“åº”
    
    LLM è¿”å›çš„ JSON ç»“æ„:
    {
        "segments": [
            {
                "id": "seg_1",
                "role": "hook",
                "content_type": "title-display",
                "importance": "high",
                "extracted_data": {
                    "numbers": [{"value": "300%", "label": "å¢é•¿", "trend": "up"}],
                    "keywords": [{"word": "æ•ˆç‡", "importance": "primary"}]
                },
                "list_context": null,
                "process_context": null
            }
        ],
        "global_structure": {
            "has_point_list": true,
            "point_list_count": 3,
            "has_process": false,
            "has_comparison": false,
            "chapters": [{"title": "å¼€åœº", "start_segment_id": "seg_1", "end_segment_id": "seg_3"}]
        }
    }
    """
    # æå– JSON
    json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
    if json_match:
        json_str = json_match.group(1)
    else:
        # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
        json_str = response_text
    
    # å°è¯•ä¿®å¤å¸¸è§çš„ JSON æ ¼å¼é—®é¢˜
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse LLM response as JSON: {e}")
        
        # å°è¯•ä¿®å¤: ç§»é™¤å°¾éƒ¨é€—å·
        fixed_json = re.sub(r',\s*}', '}', json_str)
        fixed_json = re.sub(r',\s*]', ']', fixed_json)
        
        try:
            data = json.loads(fixed_json)
            logger.info("JSON parsing succeeded after fixing trailing commas")
        except json.JSONDecodeError:
            # å°è¯•æå– { } ä¹‹é—´çš„å†…å®¹
            brace_match = re.search(r'\{.*\}', json_str, re.DOTALL)
            if brace_match:
                try:
                    data = json.loads(brace_match.group())
                    logger.info("JSON parsing succeeded after extracting braces")
                except json.JSONDecodeError:
                    logger.error("All JSON parsing attempts failed, using fallback")
                    return _fallback_analysis(original_segments)
            else:
                return _fallback_analysis(original_segments)
    
    # æ„å»ºç‰‡æ®µ ID åˆ°åŸå§‹æ•°æ®çš„æ˜ å°„
    seg_map = {seg.get('id', f'seg_{i}'): seg for i, seg in enumerate(original_segments)}
    
    # è§£æ segments
    structured_segments = []
    llm_segments = data.get('segments', [])
    
    for llm_seg in llm_segments:
        seg_id = llm_seg.get('id', '')
        original = seg_map.get(seg_id, {})
        original_text = original.get('text', '')
        
        # è§£æç»“æ„æ•°æ® (ä¼ å…¥åŸæ–‡ç”¨äº B-Roll è§¦å‘æ£€æµ‹)
        structure = _parse_segment_structure(llm_seg, text=original_text)
        
        structured_segments.append(StructuredSegment(
            id=seg_id,
            text=original_text,
            start_ms=original.get('start_ms', 0),
            end_ms=original.get('end_ms', 0),
            structure=structure
        ))
    
    # è§£æå…¨å±€ç»“æ„
    global_data = data.get('global_structure', {})
    chapters = [
        ChapterInfo(**ch) for ch in global_data.get('chapters', [])
    ]
    
    global_structure = GlobalStructure(
        has_point_list=global_data.get('has_point_list', False),
        point_list_count=global_data.get('point_list_count'),
        has_process=global_data.get('has_process', False),
        process_step_count=global_data.get('process_step_count'),
        has_comparison=global_data.get('has_comparison', False),
        chapters=chapters
    )
    
    return StructureAnalysisResult(
        segments=structured_segments,
        global_structure=global_structure
    )


def _parse_segment_structure(llm_seg: Dict[str, Any], text: str = "") -> SegmentStructure:
    """è§£æå•ä¸ªç‰‡æ®µçš„ç»“æ„"""
    # è§’è‰²
    role_str = llm_seg.get('role', 'filler')
    try:
        role = SegmentRole(role_str)
    except ValueError:
        role = SegmentRole.FILLER
    
    # å†…å®¹ç±»å‹
    content_type_str = llm_seg.get('content_type', 'none')
    try:
        content_type = ContentType(content_type_str)
    except ValueError:
        content_type = ContentType.NONE
    
    # é‡è¦ç¨‹åº¦ (å¢å¼ºæ˜ å°„)
    importance_str = llm_seg.get('importance', 'medium')
    # æ˜ å°„å¯èƒ½çš„ LLM è¿”å›å€¼åˆ°æ ‡å‡†æšä¸¾
    importance_map = {
        'critical': 'critical',
        'high': 'high',
        'medium': 'medium',
        'low': 'low',
        # LLM å¯èƒ½è¿”å›çš„éæ ‡å‡†å€¼
        'primary': 'high',
        'secondary': 'medium',
        'tertiary': 'low',
        'main': 'high',
        'normal': 'medium',
        'minor': 'low',
    }
    importance_str = importance_map.get(importance_str.lower(), 'medium')
    try:
        importance = ImportanceLevel(importance_str)
    except ValueError:
        importance = ImportanceLevel.MEDIUM
    
    # åˆ—è¡¨ä¸Šä¸‹æ–‡
    list_ctx = None
    if llm_seg.get('list_context'):
        ctx = llm_seg['list_context']
        # ç¡®ä¿å¿…éœ€çš„æ•´æ•°å­—æ®µæœ‰é»˜è®¤å€¼
        item_index = ctx.get('item_index')
        total_items = ctx.get('total_items')
        item_title = ctx.get('item_title')
        if item_index is None or not isinstance(item_index, int):
            item_index = 1
        if total_items is None or not isinstance(total_items, int):
            total_items = 1
        if item_title is None or not isinstance(item_title, str):
            item_title = ''
        list_ctx = ListContext(
            list_id=ctx.get('list_id', '') or '',
            item_index=item_index,
            total_items=total_items,
            item_title=item_title
        )
    
    # æµç¨‹ä¸Šä¸‹æ–‡
    process_ctx = None
    if llm_seg.get('process_context'):
        ctx = llm_seg['process_context']
        # ç¡®ä¿å¿…éœ€çš„æ•´æ•°å­—æ®µæœ‰é»˜è®¤å€¼
        step_index = ctx.get('step_index')
        total_steps = ctx.get('total_steps')
        step_title = ctx.get('step_title')
        if step_index is None or not isinstance(step_index, int):
            step_index = 1
        if total_steps is None or not isinstance(total_steps, int):
            total_steps = 1
        if step_title is None or not isinstance(step_title, str):
            step_title = ''
        process_ctx = ProcessContext(
            process_id=ctx.get('process_id', '') or '',
            step_index=step_index,
            total_steps=total_steps,
            step_title=step_title
        )
    
    # æå–çš„æ•°æ®
    extracted_data = None
    if llm_seg.get('extracted_data'):
        ed = llm_seg['extracted_data']
        
        # è§£ææ•°å­— (å¸¦å®¹é”™)
        numbers = []
        for n in ed.get('numbers', []):
            try:
                # éªŒè¯å¹¶ä¿®æ­£ trend å€¼
                trend = n.get('trend', 'neutral')
                if trend not in ('up', 'down', 'neutral'):
                    trend = 'neutral'
                numbers.append(ExtractedNumber(
                    value=str(n.get('value', '')),
                    label=n.get('label', ''),
                    trend=trend,
                ))
            except Exception as e:
                logger.warning(f"Failed to parse ExtractedNumber: {e}")
        
        # è§£æå…³é”®è¯ (å¸¦å®¹é”™)
        keywords = []
        for k in ed.get('keywords', []):
            try:
                if isinstance(k, str):
                    # LLM å¯èƒ½åªè¿”å›å­—ç¬¦ä¸²
                    keywords.append(ExtractedKeyword(word=k, importance="primary"))
                elif isinstance(k, dict):
                    # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
                    word = k.get('word') or k.get('text') or k.get('keyword', '')
                    importance = k.get('importance', 'primary')
                    if importance not in ('primary', 'secondary'):
                        importance = 'primary'
                    if word:
                        keywords.append(ExtractedKeyword(word=word, importance=importance))
                else:
                    logger.warning(f"Unexpected keyword format: {type(k)}")
            except Exception as e:
                logger.warning(f"Failed to parse ExtractedKeyword: {e}")
        
        quote = None
        if ed.get('quote'):
            try:
                quote = ExtractedQuote(**ed['quote'])
            except Exception as e:
                logger.warning(f"Failed to parse ExtractedQuote: {e}")
        
        extracted_data = ExtractedData(
            numbers=numbers,
            keywords=keywords,
            quote=quote
        )
    
    # ğŸ†• B-Roll è§¦å‘æ£€æµ‹ (å¢å¼ºç‰ˆ)
    needs_broll = llm_seg.get('needs_broll', False)
    broll_keywords = llm_seg.get('broll_keywords', [])
    broll_trigger_type = None
    broll_trigger_text = None
    broll_suggested_content = None
    broll_importance = "medium"
    
    # ä½¿ç”¨è§„åˆ™å¼•æ“æ£€æµ‹è§¦å‘ç‚¹
    if text:
        primary_trigger = detect_primary_trigger(text)
        if primary_trigger:
            needs_broll = True
            broll_trigger_type = primary_trigger.trigger_type.value
            broll_trigger_text = primary_trigger.matched_text
            broll_suggested_content = primary_trigger.suggested_broll
            broll_importance = primary_trigger.importance
            
            # è¡¥å……å…³é”®è¯
            if not broll_keywords and primary_trigger.matched_text:
                broll_keywords = [primary_trigger.matched_text]
    
    # ğŸ†• å»ºè®®å¸ƒå±€æ¨¡å¼
    suggested_layout_mode = LayoutModeSelector.select_mode(
        has_broll=needs_broll,
        broll_importance=broll_importance,
        content_type=content_type_str,
        template_id="talking-head",  # é»˜è®¤æ¨¡ç‰ˆ
    ).value
    
    return SegmentStructure(
        role=role,
        content_type=content_type,
        importance=importance,
        list_context=list_ctx,
        process_context=process_ctx,
        extracted_data=extracted_data,
        needs_broll=needs_broll,
        broll_keywords=broll_keywords,
        broll_trigger_type=broll_trigger_type,
        broll_trigger_text=broll_trigger_text,
        broll_suggested_content=broll_suggested_content,
        broll_importance=broll_importance,
        suggested_layout_mode=suggested_layout_mode,
    )


def _fallback_analysis(segments: List[Dict[str, Any]]) -> StructureAnalysisResult:
    """
    é™çº§åˆ†æ - ä½¿ç”¨è§„åˆ™è¿›è¡ŒåŸºç¡€åˆ†æ
    
    å½“ LLM è°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨
    """
    structured_segments = []
    
    # ç®€å•çš„è§„åˆ™åˆ†æ
    list_pattern = re.compile(r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+|é¦–å…ˆ|å…¶æ¬¡|æœ€å|ç„¶å|\d+[\.ã€])')
    number_pattern = re.compile(r'(\d+(?:\.\d+)?)\s*(%|å€|ä¸‡|äº¿|ä¸ª|æ¬¡)')
    keyword_triggers = ['é‡è¦', 'å…³é”®', 'æ ¸å¿ƒ', 'å¿…é¡»', 'ä¸€å®š', 'è®°ä½']
    
    has_list = False
    list_items = []
    
    for i, seg in enumerate(segments):
        seg_id = seg.get('id', f'seg_{i}')
        text = seg.get('text', '')
        
        # é»˜è®¤å€¼
        role = SegmentRole.FILLER
        content_type = ContentType.NONE
        importance = ImportanceLevel.MEDIUM
        extracted_data = None
        
        # ğŸ†• B-Roll è§¦å‘æ£€æµ‹
        needs_broll = False
        broll_keywords = []
        broll_trigger_type = None
        broll_trigger_text = None
        broll_suggested_content = None
        broll_importance = "medium"
        
        primary_trigger = detect_primary_trigger(text)
        if primary_trigger:
            needs_broll = True
            broll_trigger_type = primary_trigger.trigger_type.value
            broll_trigger_text = primary_trigger.matched_text
            broll_suggested_content = primary_trigger.suggested_broll
            broll_importance = primary_trigger.importance
            broll_keywords = [primary_trigger.matched_text]
        
        # æ£€æµ‹å¼€åœº
        if i < 2 and any(kw in text for kw in ['ä½ çŸ¥é“', 'ä»Šå¤©', 'å¤§å®¶å¥½', 'ï¼Ÿ']):
            role = SegmentRole.HOOK
            content_type = ContentType.TITLE_DISPLAY
            importance = ImportanceLevel.HIGH
        
        # æ£€æµ‹åˆ—è¡¨é¡¹
        elif list_pattern.search(text):
            has_list = True
            role = SegmentRole.POINT
            content_type = ContentType.LIST_ITEM
            importance = ImportanceLevel.HIGH
            list_items.append(seg_id)
        
        # æ£€æµ‹æ•°å­—
        number_match = number_pattern.search(text)
        if number_match:
            role = SegmentRole.DATA
            content_type = ContentType.DATA_HIGHLIGHT
            importance = ImportanceLevel.HIGH
            extracted_data = ExtractedData(
                numbers=[ExtractedNumber(
                    value=number_match.group(0),
                    label="æ•°æ®",
                    trend="neutral"
                )]
            )
        
        # æ£€æµ‹å…³é”®è¯å¼ºè°ƒ
        if any(kw in text for kw in keyword_triggers):
            content_type = ContentType.KEYWORD_EMPHASIS
            importance = ImportanceLevel.HIGH
        
        # æ£€æµ‹æ€»ç»“
        if any(kw in text for kw in ['æ€»ç»“', 'æ€»ä¹‹', 'æ‰€ä»¥', 'ç»¼ä¸Š']):
            role = SegmentRole.SUMMARY
            importance = ImportanceLevel.HIGH
        
        # ğŸ†• å»ºè®®å¸ƒå±€æ¨¡å¼
        suggested_layout_mode = LayoutModeSelector.select_mode(
            has_broll=needs_broll,
            broll_importance=broll_importance,
            content_type=content_type.value if content_type else "none",
            template_id="talking-head",
        ).value
        
        structure = SegmentStructure(
            role=role,
            content_type=content_type,
            importance=importance,
            extracted_data=extracted_data,
            needs_broll=needs_broll,
            broll_keywords=broll_keywords,
            broll_trigger_type=broll_trigger_type,
            broll_trigger_text=broll_trigger_text,
            broll_suggested_content=broll_suggested_content,
            broll_importance=broll_importance,
            suggested_layout_mode=suggested_layout_mode,
        )
        
        structured_segments.append(StructuredSegment(
            id=seg_id,
            text=text,
            start_ms=seg.get('start_ms', 0),
            end_ms=seg.get('end_ms', 0),
            structure=structure
        ))
    
    global_structure = GlobalStructure(
        has_point_list=has_list,
        point_list_count=len(list_items) if has_list else None
    )
    
    return StructureAnalysisResult(
        segments=structured_segments,
        global_structure=global_structure
    )
