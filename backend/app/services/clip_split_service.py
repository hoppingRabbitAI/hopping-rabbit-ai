"""
Clip æ™ºèƒ½æ‹†åˆ†æœåŠ¡

åˆ†æ Clip å†…å®¹ï¼Œæ™ºèƒ½æ‹†åˆ†ä¸ºæ›´å°çš„ç‰‡æ®µ
æ”¯æŒåŸºäº transcriptï¼ˆåˆ†å¥ï¼‰å’Œåœºæ™¯å˜åŒ–ï¼ˆåˆ†é•œï¼‰çš„æ‹†åˆ†

è®¾è®¡åŸåˆ™ï¼š
1. ä¼˜å…ˆä½¿ç”¨ transcript è¿›è¡Œåˆ†å¥æ‹†åˆ†
2. å¦‚æœæ—  transcriptï¼Œä¸´æ—¶è°ƒç”¨ ASR æœåŠ¡è·å–
3. è¿”å›æ‹†åˆ†å»ºè®®ï¼Œç”¨æˆ·ç¡®è®¤åæ‰§è¡Œ
4. ä¿æŒçˆ¶å­å…³ç³»è¿½æº¯ (parent_clip_id)
"""

import logging
from typing import Optional, List, Tuple
from dataclasses import dataclass
from uuid import uuid4

logger = logging.getLogger(__name__)


# ==========================================
# æ•°æ®æ¨¡å‹
# ==========================================

@dataclass
class SplitPoint:
    """æ‹†åˆ†ç‚¹"""
    time_ms: int          # æ‹†åˆ†æ—¶é—´ç‚¹ï¼ˆæ¯«ç§’ï¼‰
    confidence: float     # ç½®ä¿¡åº¦ 0-1
    reason: str           # æ‹†åˆ†åŸå› 
    transcript: str = ""  # è¯¥ç‰‡æ®µçš„è½¬å†™æ–‡æœ¬


@dataclass
class SplitSegment:
    """æ‹†åˆ†åçš„ç‰‡æ®µ"""
    start_ms: int
    end_ms: int
    transcript: str
    confidence: float
    
    @property
    def duration_ms(self) -> int:
        return self.end_ms - self.start_ms


@dataclass
class SplitAnalysisResult:
    """æ‹†åˆ†åˆ†æç»“æœ"""
    can_split: bool
    reason: str
    segments: List[SplitSegment]
    split_strategy: str  # 'sentence' | 'scene' | 'none'
    
    @property
    def segment_count(self) -> int:
        return len(self.segments)


# ==========================================
# åˆ†æé€»è¾‘
# ==========================================

MIN_SEGMENT_DURATION_MS = 3000  # æœ€å°ç‰‡æ®µæ—¶é•¿ 3 ç§’ï¼ˆé¿å…æ‹†åˆ†å¤ªç»†ï¼‰
MIN_CLIP_DURATION_MS = 5000     # å¯æ‹†åˆ†çš„æœ€å° clip æ—¶é•¿ 5 ç§’
TARGET_SEGMENT_DURATION_MS = 10000  # ç›®æ ‡ç‰‡æ®µæ—¶é•¿ 10 ç§’ï¼ˆç”¨äºåˆå¹¶çŸ­å¥ï¼‰


def analyze_transcript_for_split(
    transcript: str,
    clip_start_ms: int,
    clip_end_ms: int,
    words_with_timing: Optional[List[dict]] = None
) -> Tuple[bool, List[SplitSegment]]:
    """
    åˆ†æ transcript å¹¶ç¡®å®šæ‹†åˆ†ç‚¹
    
    Args:
        transcript: å®Œæ•´è½¬å†™æ–‡æœ¬
        clip_start_ms: clip èµ·å§‹æ—¶é—´
        clip_end_ms: clip ç»“æŸæ—¶é—´
        words_with_timing: å¸¦æ—¶é—´æˆ³çš„è¯åˆ—è¡¨ [{"word": "...", "start": ms, "end": ms}]
        
    Returns:
        (can_split, segments)
    """
    if not transcript or not transcript.strip():
        return False, []
    
    clip_duration = clip_end_ms - clip_start_ms
    
    # â˜… æ”¹è¿›çš„å¥å­åˆ†å‰²é€»è¾‘ï¼š
    # 1. åªæŒ‰å¥æœ«æ ‡ç‚¹åˆ†å‰²ï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰
    # 2. é€—å·ä¸ä½œä¸ºåˆ†å¥ç‚¹ï¼ˆé¿å…æ‹†åˆ†å¤ªç»†ï¼‰
    import re
    
    # åªåŒ¹é…å¥æœ«æ ‡ç‚¹ï¼šä¸­è‹±æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·
    sentence_pattern = r'[ã€‚ï¼ï¼Ÿ!?]+'
    
    # åˆ†å‰²å¥å­
    sentences = re.split(sentence_pattern, transcript.strip())
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]
    
    logger.info(f"[ClipSplit] åˆæ­¥åˆ†å¥: {len(sentences)} ä¸ªå¥å­")
    
    if len(sentences) <= 1:
        return False, []
    
    # â˜… åˆå¹¶çŸ­å¥ï¼šå¦‚æœå¥å­å¤ªçŸ­ï¼ˆ< 20 å­—ç¬¦ï¼‰ï¼Œåˆå¹¶åˆ°ä¸‹ä¸€å¥
    merged_sentences = []
    buffer = ""
    for s in sentences:
        buffer += s
        # å¦‚æœç´¯ç§¯æ–‡æœ¬è¶³å¤Ÿé•¿ï¼ˆ> 30 å­—ç¬¦ï¼‰ï¼Œä½œä¸ºä¸€ä¸ªç‹¬ç«‹å¥å­
        if len(buffer) >= 30:
            merged_sentences.append(buffer)
            buffer = ""
        else:
            buffer += "ã€‚"  # ä¿ç•™åˆ†éš”
    if buffer:
        if merged_sentences:
            merged_sentences[-1] += buffer  # å¹¶å…¥æœ€åä¸€å¥
        else:
            merged_sentences.append(buffer)
    
    sentences = merged_sentences
    logger.info(f"[ClipSplit] åˆå¹¶çŸ­å¥å: {len(sentences)} ä¸ªå¥å­")
    
    # å¦‚æœæœ‰è¯çº§æ—¶é—´æˆ³ï¼Œä½¿ç”¨ç²¾ç¡®æ‹†åˆ†
    if words_with_timing and len(words_with_timing) > 0:
        return _split_by_word_timing(sentences, words_with_timing, clip_start_ms, clip_end_ms)
    
    # æ²¡æœ‰è¯çº§æ—¶é—´æˆ³ï¼ŒæŒ‰å¥å­æ•°é‡å‡åˆ†æ—¶é—´
    return _split_by_even_distribution(sentences, clip_start_ms, clip_end_ms)


def _split_by_word_timing(
    sentences: List[str],
    words: List[dict],
    clip_start_ms: int,
    clip_end_ms: int
) -> Tuple[bool, List[SplitSegment]]:
    """
    åŸºäºè¯çº§æ—¶é—´æˆ³ç²¾ç¡®æ‹†åˆ†
    
    æ”¹è¿›ç­–ç•¥ï¼š
    1. æŒ‰å¥å­è¾¹ç•Œåˆ†å‰²
    2. å¦‚æœåˆ†å‡ºçš„ç‰‡æ®µå¤ªçŸ­ï¼ˆ< MIN_SEGMENT_DURATION_MSï¼‰ï¼Œåˆå¹¶åˆ°ä¸‹ä¸€å¥
    3. ç¡®ä¿ç‰‡æ®µæ•°é‡åˆç†ï¼ˆä¸è¶…è¿‡ clip æ—¶é•¿ / 5ç§’ï¼‰
    """
    clip_duration = clip_end_ms - clip_start_ms
    max_segments = max(2, clip_duration // 5000)  # æœ€å¤šæ¯ 5 ç§’ä¸€ä¸ªç‰‡æ®µ
    
    logger.info(f"[ClipSplit] å¼€å§‹ç²¾ç¡®åˆ†å‰²: {len(sentences)} ä¸ªå¥å­, clip æ—¶é•¿ {clip_duration/1000:.1f}s, æœ€å¤š {max_segments} ä¸ªç‰‡æ®µ")
    
    segments = []
    current_sentence_idx = 0
    current_segment_start = clip_start_ms
    accumulated_text = ""
    accumulated_sentences = []
    
    for i, word_info in enumerate(words):
        # å…¼å®¹ä¸¤ç§æ ¼å¼: {"word": "...", "end": ...} æˆ– {"text": "...", "end_time": ...}
        word = word_info.get("word") or word_info.get("text", "")
        word_end = int(word_info.get("end") or word_info.get("end_time", 0))
        
        accumulated_text += word
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ°å½“å‰å¥å­çš„ç»“å°¾
        if current_sentence_idx < len(sentences):
            target_sentence = sentences[current_sentence_idx]
            # ç®€åŒ–åŒ¹é…ï¼šæ£€æŸ¥ç´¯ç§¯æ–‡æœ¬æ˜¯å¦åŒ…å«ç›®æ ‡å¥å­çš„ä¸»è¦å†…å®¹
            if len(accumulated_text) >= len(target_sentence) * 0.8:
                accumulated_sentences.append(target_sentence)
                segment_duration = word_end - current_segment_start
                
                # â˜… åªæœ‰å½“æ—¶é•¿ >= MIN_SEGMENT_DURATION_MS æ—¶æ‰åˆ›å»ºæ–°ç‰‡æ®µ
                # å¦åˆ™ç»§ç»­ç´¯ç§¯åˆ°ä¸‹ä¸€å¥
                if segment_duration >= MIN_SEGMENT_DURATION_MS:
                    segments.append(SplitSegment(
                        start_ms=current_segment_start,
                        end_ms=word_end,
                        transcript="".join(accumulated_sentences),
                        confidence=0.9
                    ))
                    current_segment_start = word_end
                    accumulated_text = ""
                    accumulated_sentences = []
                
                current_sentence_idx += 1
    
    # å¤„ç†æœ€åä¸€ä¸ªç‰‡æ®µ
    if current_segment_start < clip_end_ms:
        remaining_sentences = accumulated_sentences + sentences[current_sentence_idx:]
        if remaining_sentences and clip_end_ms - current_segment_start >= MIN_SEGMENT_DURATION_MS:
            segments.append(SplitSegment(
                start_ms=current_segment_start,
                end_ms=clip_end_ms,
                transcript="".join(remaining_sentences),
                confidence=0.8
            ))
        elif segments and remaining_sentences:
            # å¹¶å…¥æœ€åä¸€ä¸ªç‰‡æ®µ
            segments[-1] = SplitSegment(
                start_ms=segments[-1].start_ms,
                end_ms=clip_end_ms,
                transcript=segments[-1].transcript + "".join(remaining_sentences),
                confidence=0.8
            )
    
    # â˜… å¦‚æœç‰‡æ®µå¤ªå¤šï¼Œè¿›ä¸€æ­¥åˆå¹¶
    if len(segments) > max_segments:
        logger.info(f"[ClipSplit] ç‰‡æ®µæ•° {len(segments)} > æœ€å¤§ {max_segments}ï¼Œè¿›è¡Œåˆå¹¶...")
        segments = _merge_short_segments(segments, max_segments)
    
    logger.info(f"[ClipSplit] æœ€ç»ˆåˆ†å‰²: {len(segments)} ä¸ªç‰‡æ®µ")
    return len(segments) > 1, segments


def _merge_short_segments(segments: List[SplitSegment], max_count: int) -> List[SplitSegment]:
    """åˆå¹¶çŸ­ç‰‡æ®µï¼Œç¡®ä¿ç‰‡æ®µæ•°ä¸è¶…è¿‡ max_count"""
    if len(segments) <= max_count:
        return segments
    
    # è®¡ç®—éœ€è¦åˆå¹¶çš„æ¬¡æ•°
    merge_count = len(segments) - max_count
    
    # æ‰¾åˆ°æœ€çŸ­çš„ç‰‡æ®µè¿›è¡Œåˆå¹¶
    for _ in range(merge_count):
        if len(segments) <= max_count:
            break
        
        # æ‰¾åˆ°æ—¶é•¿æœ€çŸ­çš„ç‰‡æ®µï¼ˆä¸æ˜¯ç¬¬ä¸€ä¸ªä¹Ÿä¸æ˜¯æœ€åä¸€ä¸ªæ›´å¥½ï¼‰
        min_duration = float('inf')
        min_idx = 1
        for i in range(1, len(segments)):
            duration = segments[i].duration_ms
            if duration < min_duration:
                min_duration = duration
                min_idx = i
        
        # åˆå¹¶åˆ°å‰ä¸€ä¸ªç‰‡æ®µ
        if min_idx > 0:
            prev = segments[min_idx - 1]
            curr = segments[min_idx]
            merged = SplitSegment(
                start_ms=prev.start_ms,
                end_ms=curr.end_ms,
                transcript=prev.transcript + curr.transcript,
                confidence=min(prev.confidence, curr.confidence)
            )
            segments = segments[:min_idx-1] + [merged] + segments[min_idx+1:]
    
    return segments


def _split_by_even_distribution(
    sentences: List[str],
    clip_start_ms: int,
    clip_end_ms: int
) -> Tuple[bool, List[SplitSegment]]:
    """åŸºäºå¥å­æ•°é‡å‡åˆ†æ—¶é—´"""
    clip_duration = clip_end_ms - clip_start_ms
    segment_duration = clip_duration // len(sentences)
    
    # å¦‚æœå‡åˆ†åç‰‡æ®µå¤ªçŸ­ï¼Œå‡å°‘æ‹†åˆ†æ•°é‡
    if segment_duration < MIN_SEGMENT_DURATION_MS:
        # è®¡ç®—æœ€å¤šèƒ½æ‹†æˆå‡ ä¸ªç‰‡æ®µ
        max_segments = clip_duration // MIN_SEGMENT_DURATION_MS
        if max_segments <= 1:
            return False, []
        # åˆå¹¶å¥å­
        merged_sentences = []
        sentences_per_segment = len(sentences) // max_segments + 1
        for i in range(0, len(sentences), sentences_per_segment):
            merged = " ".join(sentences[i:i+sentences_per_segment])
            merged_sentences.append(merged)
        sentences = merged_sentences[:max_segments]
        segment_duration = clip_duration // len(sentences)
    
    segments = []
    for i, sentence in enumerate(sentences):
        start = clip_start_ms + i * segment_duration
        end = clip_start_ms + (i + 1) * segment_duration if i < len(sentences) - 1 else clip_end_ms
        
        segments.append(SplitSegment(
            start_ms=start,
            end_ms=end,
            transcript=sentence,
            confidence=0.6  # å‡åˆ†çš„ç½®ä¿¡åº¦è¾ƒä½
        ))
    
    return len(segments) > 1, segments


async def _fetch_asr_for_clip(clip: dict, supabase_client) -> Tuple[str, List[dict]]:
    """
    ä¸´æ—¶è°ƒç”¨ ASR æœåŠ¡è·å– clip çš„è¯­éŸ³è½¬å†™ç»“æœ
    
    æµç¨‹ï¼šä¸‹è½½è§†é¢‘ â†’ æå–éŸ³é¢‘ â†’ ä¸Šä¼ åˆ° Supabase â†’ è°ƒç”¨ ASR
    
    Args:
        clip: Clip æ•°æ®
        supabase_client: Supabase å®¢æˆ·ç«¯
        
    Returns:
        (transcript, words_with_timing)
    """
    import os
    import asyncio
    import tempfile
    import hashlib
    from app.tasks.transcribe import transcribe_audio
    from app.config import get_settings
    
    settings = get_settings()
    asset_id = clip.get("asset_id")
    if not asset_id:
        return "", []
    
    # è·å– asset ä¿¡æ¯
    asset_result = supabase_client.table("assets").select(
        "storage_path, hls_path, cloudflare_uid, project_id"
    ).eq("id", asset_id).single().execute()
    
    if not asset_result.data:
        logger.warning(f"[ClipSplit] Asset {asset_id} ä¸å­˜åœ¨")
        return "", []
    
    asset = asset_result.data
    storage_path = asset.get("storage_path", "")
    
    # 1. ç¡®å®šè§†é¢‘ä¸‹è½½ URL
    if storage_path.startswith("cloudflare:"):
        video_uid = storage_path.replace("cloudflare:", "")
        download_url = f"https://videodelivery.net/{video_uid}/manifest/video.m3u8"
    elif storage_path.startswith("http"):
        download_url = storage_path
    else:
        # Supabase Storage
        from app.services.supabase_client import get_file_url
        download_url = get_file_url("clips", storage_path, expires_in=3600)
        if not download_url:
            logger.warning(f"[ClipSplit] æ— æ³•è·å–è§†é¢‘ URL: {storage_path}")
            return "", []
    
    logger.info(f"[ClipSplit] è§†é¢‘ URL: {download_url[:60]}...")
    
    # 2. æ£€æŸ¥æ˜¯å¦å·²æœ‰æå–çš„éŸ³é¢‘ç¼“å­˜
    audio_storage_path = f"asr_audio/{asset_id}.mp3"
    try:
        result = supabase_client.storage.from_("clips").create_signed_url(audio_storage_path, 3600)
        cached_url = result.get("signedURL") or result.get("signedUrl") or result.get("signed_url")
        if cached_url:
            logger.info(f"[ClipSplit] âœ… ä½¿ç”¨ç¼“å­˜éŸ³é¢‘")
            audio_url = cached_url
        else:
            audio_url = None
    except Exception:
        audio_url = None
    
    # 3. å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œä¸‹è½½è§†é¢‘å¹¶æå–éŸ³é¢‘
    if not audio_url:
        temp_dir = settings.cache_dir or tempfile.gettempdir()
        path_hash = hashlib.md5(asset_id.encode()).hexdigest()[:12]
        temp_video_path = os.path.join(temp_dir, f"clip_split_{path_hash}.mp4")
        temp_audio_path = os.path.join(temp_dir, f"asr_{asset_id}.mp3")
        
        try:
            # ä¸‹è½½è§†é¢‘
            if not os.path.exists(temp_video_path):
                logger.info(f"[ClipSplit] å¼€å§‹ä¸‹è½½è§†é¢‘ (URL: {download_url[:80]}...)")
                process = await asyncio.create_subprocess_exec(
                    "ffmpeg", "-y",
                    "-i", download_url,
                    "-c", "copy",
                    "-movflags", "+faststart",
                    temp_video_path,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                try:
                    stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=300)
                except asyncio.TimeoutError:
                    process.kill()
                    logger.error(f"[ClipSplit] âŒ è§†é¢‘ä¸‹è½½è¶…æ—¶ (5åˆ†é’Ÿ)")
                    raise ValueError("è§†é¢‘ä¸‹è½½è¶…æ—¶")
                if process.returncode != 0:
                    raise ValueError(f"è§†é¢‘ä¸‹è½½å¤±è´¥: {stderr.decode()[:200]}")
                logger.info(f"[ClipSplit] âœ… è§†é¢‘ä¸‹è½½å®Œæˆ -> {temp_video_path}")
            else:
                logger.info(f"[ClipSplit] âœ… ä½¿ç”¨ç¼“å­˜è§†é¢‘: {temp_video_path}")
            
            # æå–éŸ³é¢‘
            logger.info(f"[ClipSplit] æå–éŸ³é¢‘...")
            process = await asyncio.create_subprocess_exec(
                "ffmpeg", "-y",
                "-i", temp_video_path,
                "-vn", "-ar", "16000", "-ac", "1", "-b:a", "64k", "-f", "mp3",
                temp_audio_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            try:
                stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=60)
            except asyncio.TimeoutError:
                process.kill()
                logger.error(f"[ClipSplit] âŒ éŸ³é¢‘æå–è¶…æ—¶ (1åˆ†é’Ÿ)")
                raise ValueError("éŸ³é¢‘æå–è¶…æ—¶")
            if process.returncode != 0:
                raise ValueError(f"éŸ³é¢‘æå–å¤±è´¥: {stderr.decode()[:200]}")
            logger.info(f"[ClipSplit] âœ… éŸ³é¢‘æå–å®Œæˆ -> {temp_audio_path}")
            
            # ä¸Šä¼ éŸ³é¢‘åˆ° Supabase
            with open(temp_audio_path, "rb") as f:
                audio_data = f.read()
            logger.info(f"[ClipSplit] ä¸Šä¼ éŸ³é¢‘ ({len(audio_data) / 1024:.1f} KB)...")
            
            try:
                supabase_client.storage.from_("clips").upload(
                    audio_storage_path, audio_data,
                    {"content-type": "audio/mpeg", "upsert": "true"}
                )
            except Exception as e:
                if "Duplicate" not in str(e):
                    raise
            
            # è·å–ç­¾å URL
            result = supabase_client.storage.from_("clips").create_signed_url(audio_storage_path, 3600)
            audio_url = result.get("signedURL") or result.get("signedUrl") or result.get("signed_url")
            logger.info(f"[ClipSplit] âœ… éŸ³é¢‘ä¸Šä¼ å®Œæˆ")
            
        except Exception as e:
            logger.error(f"[ClipSplit] å‡†å¤‡éŸ³é¢‘å¤±è´¥: {e}")
            return "", []
    
    # 4. è°ƒç”¨ ASR æœåŠ¡
    logger.info(f"[ClipSplit] è°ƒç”¨ ASR æœåŠ¡...")
    try:
        asr_result = await transcribe_audio(
            audio_url=audio_url,
            language="zh",
            audio_format="mp3",
            enable_word_timestamps=True,
        )
        
        segments = asr_result.get("segments", [])
        
        # â˜… å…ˆä¿å­˜ ASR ç»“æœåˆ° asset.metadataï¼ˆä¸ç®¡æ˜¯å¦æœ‰ç›¸å…³ç‰‡æ®µï¼‰
        if segments:
            try:
                existing_metadata = supabase_client.table("assets").select("metadata").eq("id", asset_id).single().execute()
                current_metadata = existing_metadata.data.get("metadata") or {}
                if not current_metadata.get("transcript_segments"):
                    current_metadata["transcript_segments"] = segments
                    supabase_client.table("assets").update({
                        "metadata": current_metadata
                    }).eq("id", asset_id).execute()
                    logger.info(f"[ClipSplit] âœ… ASR ç»“æœå·²ä¿å­˜ ({len(segments)} ä¸ªç‰‡æ®µ)")
            except Exception as e:
                logger.warning(f"[ClipSplit] ä¿å­˜ ASR ç»“æœå¤±è´¥: {e}")
        
        if not segments:
            return "", []
        
        # ç­›é€‰ä¸ clip æ—¶é—´èŒƒå›´é‡å çš„ segments
        # æ³¨æ„ï¼šclip çš„ source_start/source_end å•ä½æ˜¯æ¯«ç§’
        # ASR è¿”å›çš„ start/end å•ä½ä¹Ÿæ˜¯æ¯«ç§’
        source_start = clip.get("source_start", 0)
        source_end = clip.get("source_end", clip.get("end_time", 0) - clip.get("start_time", 0))
        
        logger.info(f"[ClipSplit] ç­›é€‰ segments: clip èŒƒå›´ {source_start}ms - {source_end}ms ({source_start/1000:.1f}s - {source_end/1000:.1f}s)")
        
        relevant_segments = []
        for seg in segments:
            # ASR è¿”å›çš„æ—¶é—´å•ä½æ˜¯æ¯«ç§’
            seg_start_ms = int(seg.get("start", 0))
            seg_end_ms = int(seg.get("end", 0))
            if seg_start_ms < source_end and seg_end_ms > source_start:
                relevant_segments.append(seg)
        
        logger.info(f"[ClipSplit] ç­›é€‰åˆ° {len(relevant_segments)}/{len(segments)} ä¸ªç›¸å…³ç‰‡æ®µ")
        
        if not relevant_segments:
            return "", []
        
        transcript = " ".join([s.get("text", "") for s in relevant_segments])
        all_words = []
        for seg in relevant_segments:
            if seg.get("words"):
                all_words.extend(seg["words"])
        
        return transcript, all_words
        
    except Exception as e:
        logger.error(f"[ClipSplit] ASR è°ƒç”¨å¤±è´¥: {e}")
        return "", []


async def analyze_clip_for_split(
    clip_id: str,
    supabase_client,
    strategy: str = "sentence"
) -> SplitAnalysisResult:
    """
    åˆ†æ clip æ˜¯å¦å¯ä»¥æ‹†åˆ†
    
    Args:
        clip_id: Clip ID
        supabase_client: Supabase å®¢æˆ·ç«¯
        strategy: æ‹†åˆ†ç­–ç•¥ (sentence | scene | paragraph)
        
    Returns:
        SplitAnalysisResult
    """
    logger.info(f"[ClipSplit] åˆ†æ clip {clip_id[:8]}... ç­–ç•¥: {strategy}")
    
    # 1. è·å– clip ä¿¡æ¯
    clip_result = supabase_client.table("clips").select("*").eq("id", clip_id).single().execute()
    
    if not clip_result.data:
        return SplitAnalysisResult(
            can_split=False,
            reason="ç‰‡æ®µä¸å­˜åœ¨",
            segments=[],
            split_strategy="none"
        )
    
    clip = clip_result.data
    clip_start = clip.get("start_time", 0)
    clip_end = clip.get("end_time", 0)
    clip_duration = clip_end - clip_start
    
    # â˜… æºè§†é¢‘æ—¶é—´ï¼ˆç”¨äºä¸ ASR æ—¶é—´æˆ³åŒ¹é…ï¼‰
    source_start = clip.get("source_start", 0)
    source_end = clip.get("source_end", clip_duration)
    
    # 2. æ£€æŸ¥æ—¶é•¿æ˜¯å¦è¶³å¤Ÿ
    if clip_duration < MIN_CLIP_DURATION_MS:
        return SplitAnalysisResult(
            can_split=False,
            reason=f"ç‰‡æ®µæ—¶é•¿å¤ªçŸ­ï¼ˆ{clip_duration/1000:.1f}ç§’ï¼‰ï¼Œæ— æ³•æ‹†åˆ†",
            segments=[],
            split_strategy="none"
        )
    
    # 3. è·å– transcriptï¼ˆä» clip metadata æˆ–å…³è”çš„ transcriptï¼‰
    transcript = clip.get("metadata", {}).get("transcript") or clip.get("content_text") or ""
    words_with_timing = clip.get("metadata", {}).get("words") or []
    
    # å¦‚æœ clip æœ¬èº«æ²¡æœ‰ transcriptï¼Œå°è¯•ä» asset çš„ ASR ç»“æœè·å–
    if not transcript and clip.get("asset_id"):
        asset_id = clip["asset_id"]
        
        try:
            # â˜… ä» assets.metadata.transcript_segments è·å– ASR ç»“æœ
            asset_result = supabase_client.table("assets").select(
                "metadata"
            ).eq("id", asset_id).single().execute()
            
            if asset_result.data:
                asset_metadata = asset_result.data.get("metadata") or {}
                transcript_segments = asset_metadata.get("transcript_segments") or []
                
                # ç­›é€‰ä¸ clip æ—¶é—´èŒƒå›´é‡å çš„ segments
                # æ³¨æ„ï¼šASR è¿”å›çš„ start/end å•ä½å·²ç»æ˜¯æ¯«ç§’
                relevant_segments = []
                for seg in transcript_segments:
                    # ASR è¿”å›çš„æ—¶é—´å•ä½æ˜¯æ¯«ç§’
                    seg_start = int(seg.get("start", 0))
                    seg_end = int(seg.get("end", 0))
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ clip æ—¶é—´èŒƒå›´æœ‰é‡å 
                    if seg_start < source_end and seg_end > source_start:
                        relevant_segments.append(seg)
                
                if relevant_segments:
                    transcript = " ".join([s.get("text", "") for s in relevant_segments])
                    # åˆå¹¶æ‰€æœ‰ words
                    all_words = []
                    for seg in relevant_segments:
                        if seg.get("words"):
                            all_words.extend(seg["words"])
                    words_with_timing = all_words
                    logger.info(f"[ClipSplit] ä» asset.metadata è·å– {len(relevant_segments)} ä¸ª transcript segments")
        except Exception as e:
            logger.warning(f"[ClipSplit] è·å– transcript å¤±è´¥: {e}")
    
    # â˜… æ²»æ ‡æ²»æœ¬ï¼šå¦‚æœæ²¡æœ‰ ASR æ•°æ®ï¼Œä¸”éœ€è¦åˆ†å¥/åˆ†æ®µè½ï¼Œä¸´æ—¶è°ƒç”¨ ASR æœåŠ¡
    if not transcript and strategy in ("sentence", "paragraph") and clip.get("asset_id"):
        logger.info(f"[ClipSplit] æ²¡æœ‰ ASR æ•°æ®ï¼Œä¸´æ—¶è°ƒç”¨ ASR æœåŠ¡...")
        try:
            transcript, words_with_timing = await _fetch_asr_for_clip(
                clip, supabase_client
            )
            if transcript:
                logger.info(f"[ClipSplit] ASR å®Œæˆï¼Œè·å–åˆ° {len(words_with_timing)} ä¸ªè¯")
        except Exception as e:
            logger.warning(f"[ClipSplit] ä¸´æ—¶ ASR è°ƒç”¨å¤±è´¥: {e}")
    
    # 4. æ ¹æ®ç­–ç•¥è¿›è¡Œæ‹†åˆ†
    if strategy == "scene":
        # åœºæ™¯æ‹†åˆ† - åŸºäºç”»é¢å˜åŒ–
        # TODO: å®ç°åœºæ™¯å˜åŒ–æ£€æµ‹ï¼Œç›®å‰è¿”å›ä¸æ”¯æŒ
        return SplitAnalysisResult(
            can_split=False,
            reason="åˆ†é•œæ‹†åˆ†åŠŸèƒ½å¼€å‘ä¸­ï¼Œè¯·ä½¿ç”¨åˆ†å¥æˆ–åˆ†æ®µè½",
            segments=[],
            split_strategy="scene"
        )
    
    elif strategy == "paragraph":
        # æ®µè½æ‹†åˆ† - åŸºäºè¯­ä¹‰åˆ†æ
        if not transcript:
            return SplitAnalysisResult(
                can_split=False,
                reason="è¯¥ç‰‡æ®µæ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³å†…å®¹",
                segments=[],
                split_strategy="paragraph"
            )
        # TODO: ä½¿ç”¨ LLM è¿›è¡Œè¯­ä¹‰æ®µè½åˆ’åˆ†ï¼Œç›®å‰ä½¿ç”¨åˆ†å¥ä½œä¸ºå›é€€
        # â˜… ä½¿ç”¨æºè§†é¢‘æ—¶é—´ï¼ˆä¸ ASR æ—¶é—´æˆ³åŒ¹é…ï¼‰
        can_split, segments = analyze_transcript_for_split(
            transcript, source_start, source_end, words_with_timing
        )
        if can_split and len(segments) > 1:
            return SplitAnalysisResult(
                can_split=True,
                reason=f"æ£€æµ‹åˆ° {len(segments)} ä¸ªè¯­ä¹‰æ®µè½ï¼Œå¯ä»¥æ‹†åˆ†",
                segments=segments,
                split_strategy="paragraph"
            )
        else:
            return SplitAnalysisResult(
                can_split=False,
                reason="è¯¥ç‰‡æ®µå†…å®¹è¾ƒçŸ­ï¼Œæ— æ³•åˆ’åˆ†æ®µè½",
                segments=[],
                split_strategy="paragraph"
            )
    
    else:
        # é»˜è®¤ï¼šåˆ†å¥æ‹†åˆ†
        if not transcript:
            return SplitAnalysisResult(
                can_split=False,
                reason="è¯¥ç‰‡æ®µæ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³å†…å®¹",
                segments=[],
                split_strategy="sentence"
            )
        
        # â˜… ä½¿ç”¨æºè§†é¢‘æ—¶é—´ï¼ˆä¸ ASR æ—¶é—´æˆ³åŒ¹é…ï¼‰
        can_split, segments = analyze_transcript_for_split(
            transcript, source_start, source_end, words_with_timing
        )
        
        if can_split and len(segments) > 1:
            return SplitAnalysisResult(
                can_split=True,
                reason=f"æ£€æµ‹åˆ° {len(segments)} ä¸ªå¥å­ï¼Œå¯ä»¥æ‹†åˆ†",
                segments=segments,
                split_strategy="sentence"
            )
        else:
            return SplitAnalysisResult(
                can_split=False,
                reason="è¯¥ç‰‡æ®µå†…å®¹ä¸ºå•ä¸€è¯­å¥ï¼Œæ— æ³•è¿›ä¸€æ­¥æ‹†åˆ†",
                segments=[],
                split_strategy="sentence"
            )


async def execute_clip_split(
    clip_id: str,
    segments: List[SplitSegment],
    supabase_client
) -> List[dict]:
    """
    æ‰§è¡Œ clip æ‹†åˆ†
    
    Args:
        clip_id: åŸå§‹ clip ID
        segments: æ‹†åˆ†åçš„ç‰‡æ®µåˆ—è¡¨
        supabase_client: Supabase å®¢æˆ·ç«¯
        
    Returns:
        æ–°åˆ›å»ºçš„ clips åˆ—è¡¨
    """
    from datetime import datetime
    
    # 1. è·å–åŸå§‹ clip
    clip_result = supabase_client.table("clips").select("*").eq("id", clip_id).single().execute()
    if not clip_result.data:
        raise ValueError(f"Clip {clip_id} ä¸å­˜åœ¨")
    
    original_clip = clip_result.data
    now = datetime.utcnow().isoformat()
    
    # â˜… åŸå§‹ clip çš„æ—¶é—´ä¿¡æ¯
    orig_start_time = original_clip["start_time"]  # timeline ä¸Šçš„å¼€å§‹ä½ç½®
    orig_end_time = original_clip["end_time"]      # timeline ä¸Šçš„ç»“æŸä½ç½®
    orig_source_start = original_clip.get("source_start", 0)  # æºè§†é¢‘å¼€å§‹ç‚¹
    orig_duration = orig_end_time - orig_start_time
    
    # â˜… åŸå§‹ clip çš„ metadataï¼ˆç”¨äºç»§æ‰¿ç¼©ç•¥å›¾ï¼‰
    original_metadata = original_clip.get("metadata", {}) or {}
    
    # 2. åˆ›å»ºæ–° clips
    # â˜… segment çš„ start_ms/end_ms æ˜¯ç›¸å¯¹äºæºè§†é¢‘çš„æ—¶é—´ï¼ˆå³ source_start åŸºå‡†ï¼‰
    # éœ€è¦è½¬æ¢ä¸º timeline ä¸Šçš„ä½ç½®
    new_clips = []
    current_timeline_pos = orig_start_time  # æ–° clip åœ¨ timeline ä¸Šçš„ä½ç½®
    
    for i, segment in enumerate(segments):
        # è®¡ç®—ç‰‡æ®µæ—¶é•¿
        segment_duration = segment.end_ms - segment.start_ms
        
        new_clip = {
            "id": str(uuid4()),
            "track_id": original_clip["track_id"],
            "asset_id": original_clip.get("asset_id"),
            "parent_clip_id": clip_id,
            "clip_type": original_clip.get("clip_type", "video"),
            # â˜… timeline ä¸Šçš„ä½ç½®ï¼šé¡ºåºæ’åˆ—
            "start_time": current_timeline_pos,
            "end_time": current_timeline_pos + segment_duration,
            # â˜… æºè§†é¢‘ä¸­çš„ä½ç½®ï¼šä¿æŒä¸ ASR ä¸€è‡´
            "source_start": segment.start_ms,
            "source_end": segment.end_ms,
            "volume": original_clip.get("volume", 1.0),
            "is_muted": original_clip.get("is_muted", False),
            "speed": original_clip.get("speed", 1.0),
            "cached_url": original_clip.get("cached_url"),
            "metadata": {
                "transcript": segment.transcript,
                "split_index": i,
                "split_confidence": segment.confidence,
                "split_from": clip_id,
                # â˜… æ²»æ ‡æ²»æœ¬ï¼šå…ˆç»§æ‰¿åŸå§‹ clip çš„ç¼©ç•¥å›¾
                "thumbnail_url": original_metadata.get("thumbnail_url"),
            },
            "created_at": now,
            "updated_at": now,
        }
        new_clips.append(new_clip)
        current_timeline_pos += segment_duration  # ä¸‹ä¸€ä¸ª clip ç´§æ¥ç€æ’åˆ—
    
    # 3. æ‰¹é‡æ’å…¥æ–° clipsï¼ˆå…ˆä¿å­˜ï¼Œè®©ç”¨æˆ·ç«‹å³çœ‹åˆ°ç»“æœï¼‰
    if new_clips:
        result = supabase_client.table("clips").insert(new_clips).execute()
        
        # 4. åˆ é™¤åŸå§‹ clip
        supabase_client.table("clips").delete().eq("id", clip_id).execute()
        
        logger.info(f"[ClipSplit] âœ… æ‹†åˆ†å®Œæˆ: {clip_id} -> {len(new_clips)} ä¸ªç‰‡æ®µ")
        
        # 5. â˜… æ²»æ ‡æ²»æœ¬ï¼šåŒæ­¥ç”Ÿæˆç¼©ç•¥å›¾ï¼ˆç¡®ä¿å‰ç«¯åˆ·æ–°åèƒ½çœ‹åˆ°æ–°ç¼©ç•¥å›¾ï¼‰
        asset_id = original_clip.get("asset_id")
        track_id = original_clip.get("track_id")
        if asset_id and track_id:
            try:
                await _generate_thumbnails_sync(
                    result.data, asset_id, track_id, supabase_client
                )
            except Exception as e:
                logger.warning(f"[ClipSplit] ç¼©ç•¥å›¾ç”Ÿæˆå¤±è´¥ï¼Œä½†æ‹†åˆ†å·²å®Œæˆ: {e}")
        
        return result.data
    
    return []


async def _generate_thumbnails_sync(
    clips: List[dict],
    asset_id: str,
    track_id: str,
    supabase_client
):
    """
    â˜… æ²»æ ‡æ²»æœ¬ï¼šåŒæ­¥ä¸ºæ‹†åˆ†åçš„ clips ç”Ÿæˆç²¾ç¡®ç¼©ç•¥å›¾
    
    åœ¨æ‹†åˆ†å®ŒæˆååŒæ­¥æ‰§è¡Œï¼Œç¡®ä¿å‰ç«¯åˆ·æ–°æ—¶èƒ½çœ‹åˆ°æ–°ç¼©ç•¥å›¾
    """
    import tempfile
    import subprocess
    import os
    import shutil
    
    try:
        # 1. è·å–è§†é¢‘ URL
        asset_result = supabase_client.table("assets").select("*").eq("id", asset_id).single().execute()
        if not asset_result.data:
            logger.warning(f"[ClipSplit Thumbnail] Asset {asset_id} ä¸å­˜åœ¨")
            return
        
        asset = asset_result.data
        video_url = asset.get("cf_stream_url") or asset.get("storage_url") or asset.get("cached_url")
        if not video_url:
            logger.warning(f"[ClipSplit Thumbnail] æ— æ³•è·å–è§†é¢‘ URL")
            return
        
        # 2. è·å– session_id å’Œé¡¹ç›®æ¯”ä¾‹
        track_result = supabase_client.table("tracks").select("project_id").eq("id", track_id).single().execute()
        if not track_result.data:
            return
        
        project_id = track_result.data.get("project_id")
        session_result = supabase_client.table("workspace_sessions").select("id").eq(
            "project_id", project_id
        ).order("created_at", desc=True).limit(1).execute()
        
        session_id = session_result.data[0].get("id") if session_result.data else "unknown"
        
        # â˜… è·å–é¡¹ç›®ç›®æ ‡æ¯”ä¾‹
        target_aspect = None
        try:
            project_result = supabase_client.table("projects").select("resolution").eq("id", project_id).single().execute()
            if project_result.data and project_result.data.get("resolution"):
                resolution = project_result.data["resolution"]
                if resolution.get("width") and resolution.get("height"):
                    if resolution["width"] > resolution["height"]:
                        target_aspect = "16:9"
                    else:
                        target_aspect = "9:16"
                    logger.info(f"[ClipSplit Thumbnail] ğŸ“ ç›®æ ‡æ¯”ä¾‹: {target_aspect}")
        except Exception as e:
            logger.warning(f"[ClipSplit Thumbnail] è·å–é¡¹ç›®æ¯”ä¾‹å¤±è´¥: {e}")
        
        # 3. ä¸‹è½½è§†é¢‘ï¼ˆå¦‚æœæ˜¯ HLSï¼‰
        temp_dir = tempfile.mkdtemp(prefix="clip_thumb_sync_")
        video_path = video_url
        
        if 'videodelivery.net' in video_url or 'm3u8' in video_url:
            temp_video = os.path.join(temp_dir, "video.mp4")
            cmd = [
                "ffmpeg", "-y", "-i", video_url,
                "-c", "copy", "-bsf:a", "aac_adtstoasc",
                temp_video
            ]
            logger.info(f"[ClipSplit Thumbnail] ä¸‹è½½è§†é¢‘...")
            result = subprocess.run(cmd, capture_output=True, timeout=300)
            if result.returncode == 0 and os.path.exists(temp_video):
                video_path = temp_video
            else:
                logger.warning(f"[ClipSplit Thumbnail] è§†é¢‘ä¸‹è½½å¤±è´¥")
                shutil.rmtree(temp_dir, ignore_errors=True)
                return
        
        # â˜… è·å–è§†é¢‘å°ºå¯¸ï¼ˆç”¨äºè£å‰ªï¼‰
        src_width, src_height = 1920, 1080
        crop_filter = None
        if target_aspect:
            try:
                probe_cmd = [
                    "ffprobe", "-v", "quiet",
                    "-select_streams", "v:0",
                    "-show_entries", "stream=width,height",
                    "-of", "csv=p=0",
                    video_path
                ]
                result = subprocess.run(probe_cmd, capture_output=True, text=True, timeout=10)
                if result.returncode == 0 and result.stdout.strip():
                    parts = result.stdout.strip().split(',')
                    if len(parts) == 2:
                        src_width, src_height = int(parts[0]), int(parts[1])
                        src_ratio = src_width / src_height
                        target_ratio = 16/9 if target_aspect == "16:9" else 9/16
                        if abs(src_ratio - target_ratio) / target_ratio > 0.05:
                            if src_ratio > target_ratio:
                                new_w = int(src_height * target_ratio)
                                new_h = src_height
                                x = (src_width - new_w) // 2
                                y = 0
                            else:
                                new_w = src_width
                                new_h = int(src_width / target_ratio)
                                x = 0
                                y = (src_height - new_h) // 2
                            crop_filter = f"crop={new_w}:{new_h}:{x}:{y}"
                            logger.info(f"[ClipSplit Thumbnail] âœ‚ï¸ è£å‰ª: {crop_filter}")
            except Exception as e:
                logger.warning(f"[ClipSplit Thumbnail] è·å–å°ºå¯¸å¤±è´¥: {e}")
        
        # 4. ä¸ºæ¯ä¸ª clip ç”Ÿæˆç¼©ç•¥å›¾
        STORAGE_BUCKET = "ai-creations"
        success_count = 0
        
        for i, clip in enumerate(clips):
            clip_id = clip.get("id")
            source_start = clip.get("source_start", 0)
            source_end = clip.get("source_end", source_start + 1000)
            mid_time_sec = (source_start + source_end) / 2 / 1000
            
            local_filename = f"clip_{i:03d}_{clip_id[:8]}.jpg"
            output_path = os.path.join(temp_dir, local_filename)
            
            try:
                # æ„å»ºæ»¤é•œé“¾
                filter_parts = []
                if crop_filter:
                    filter_parts.append(crop_filter)
                if target_aspect == "9:16":
                    filter_parts.append("scale=-2:'min(568,ih)'")
                else:
                    filter_parts.append("scale='min(320,iw)':-2")
                
                video_filter = ",".join(filter_parts) if filter_parts else None
                
                # æå–å¸§
                cmd = [
                    "ffmpeg", "-y",
                    "-ss", str(mid_time_sec),
                    "-i", video_path,
                    "-vframes", "1",
                ]
                if video_filter:
                    cmd.extend(["-vf", video_filter])
                cmd.extend(["-q:v", "2", output_path])
                
                result = subprocess.run(cmd, capture_output=True, timeout=30)
                
                if result.returncode != 0 or not os.path.exists(output_path):
                    continue
                
                # ä¸Šä¼ åˆ° Supabase
                storage_path = f"shot_thumbnails/{session_id}/{local_filename}"
                
                with open(output_path, "rb") as f:
                    file_data = f.read()
                
                try:
                    supabase_client.storage.from_(STORAGE_BUCKET).remove([storage_path])
                except:
                    pass
                
                supabase_client.storage.from_(STORAGE_BUCKET).upload(
                    storage_path, file_data, {"content-type": "image/jpeg"}
                )
                
                public_url = supabase_client.storage.from_(STORAGE_BUCKET).get_public_url(storage_path)
                
                # æ›´æ–°æ•°æ®åº“ä¸­çš„ clip
                current_clip = supabase_client.table("clips").select("metadata").eq("id", clip_id).single().execute()
                if current_clip.data:
                    metadata = current_clip.data.get("metadata", {}) or {}
                    metadata["thumbnail_url"] = public_url
                    supabase_client.table("clips").update({"metadata": metadata}).eq("id", clip_id).execute()
                
                success_count += 1
                os.remove(output_path)
                
            except Exception as e:
                logger.warning(f"[ClipSplit Thumbnail] clip {clip_id[:8]} å¤±è´¥: {e}")
                continue
        
        # æ¸…ç†
        shutil.rmtree(temp_dir, ignore_errors=True)
        logger.info(f"[ClipSplit Thumbnail] âœ… åŒæ­¥ç”Ÿæˆå®Œæˆ: {success_count}/{len(clips)} ä¸ª")
        
    except Exception as e:
        logger.error(f"[ClipSplit Thumbnail] åŒæ­¥ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def _async_generate_thumbnails(
    clips: List[dict],
    asset_id: str,
    track_id: str,
    supabase_client
):
    """
    åå°å¼‚æ­¥ä¸ºæ‹†åˆ†åçš„ clips ç”Ÿæˆç²¾ç¡®ç¼©ç•¥å›¾
    
    å®Œæˆåç›´æ¥æ›´æ–°æ•°æ®åº“ï¼Œå‰ç«¯ä¸‹æ¬¡åˆ·æ–°å³å¯çœ‹åˆ°
    """
    import tempfile
    import subprocess
    import os
    import shutil
    
    try:
        # 1. è·å–è§†é¢‘ URL
        asset_result = supabase_client.table("assets").select("*").eq("id", asset_id).single().execute()
        if not asset_result.data:
            logger.warning(f"[ClipSplit Thumbnail] Asset {asset_id} ä¸å­˜åœ¨")
            return
        
        asset = asset_result.data
        video_url = asset.get("cf_stream_url") or asset.get("storage_url") or asset.get("cached_url")
        if not video_url:
            logger.warning(f"[ClipSplit Thumbnail] æ— æ³•è·å–è§†é¢‘ URL")
            return
        
        # 2. è·å– session_id å’Œé¡¹ç›®æ¯”ä¾‹
        track_result = supabase_client.table("tracks").select("project_id").eq("id", track_id).single().execute()
        if not track_result.data:
            return
        
        project_id = track_result.data.get("project_id")
        session_result = supabase_client.table("workspace_sessions").select("id").eq(
            "project_id", project_id
        ).order("created_at", desc=True).limit(1).execute()
        
        session_id = session_result.data[0].get("id") if session_result.data else "unknown"
        
        # â˜…â˜…â˜… è·å–é¡¹ç›®ç›®æ ‡æ¯”ä¾‹ â˜…â˜…â˜…
        target_aspect = None
        try:
            project_result = supabase_client.table("projects").select("resolution").eq("id", project_id).single().execute()
            if project_result.data and project_result.data.get("resolution"):
                resolution = project_result.data["resolution"]
                if resolution.get("width") and resolution.get("height"):
                    if resolution["width"] > resolution["height"]:
                        target_aspect = "16:9"
                    else:
                        target_aspect = "9:16"
                    logger.info(f"[ClipSplit Thumbnail] ğŸ“ ç›®æ ‡æ¯”ä¾‹: {target_aspect}")
        except Exception as e:
            logger.warning(f"[ClipSplit Thumbnail] è·å–é¡¹ç›®æ¯”ä¾‹å¤±è´¥: {e}")
        
        # 3. ä¸‹è½½è§†é¢‘ï¼ˆå¦‚æœæ˜¯ HLSï¼‰
        temp_dir = tempfile.mkdtemp(prefix="clip_thumb_")
        video_path = video_url
        
        if 'videodelivery.net' in video_url or 'm3u8' in video_url:
            temp_video = os.path.join(temp_dir, "video.mp4")
            cmd = [
                "ffmpeg", "-y", "-i", video_url,
                "-c", "copy", "-bsf:a", "aac_adtstoasc",
                temp_video
            ]
            logger.info(f"[ClipSplit Thumbnail] ä¸‹è½½è§†é¢‘...")
            result = subprocess.run(cmd, capture_output=True, timeout=300)
            if result.returncode == 0 and os.path.exists(temp_video):
                video_path = temp_video
            else:
                logger.warning(f"[ClipSplit Thumbnail] è§†é¢‘ä¸‹è½½å¤±è´¥")
                shutil.rmtree(temp_dir, ignore_errors=True)
                return
        
        # â˜…â˜…â˜… è·å–è§†é¢‘å°ºå¯¸ï¼ˆç”¨äºè£å‰ªï¼‰ â˜…â˜…â˜…
        src_width, src_height = 1920, 1080
        crop_filter = None
        if target_aspect:
            try:
                probe_cmd = [
                    "ffprobe", "-v", "quiet",
                    "-select_streams", "v:0",
                    "-show_entries", "stream=width,height",
                    "-of", "csv=p=0",
                    video_path
                ]
                result = subprocess.run(probe_cmd, capture_output=True, text=True, timeout=10)
                if result.returncode == 0 and result.stdout.strip():
                    parts = result.stdout.strip().split(',')
                    if len(parts) == 2:
                        src_width, src_height = int(parts[0]), int(parts[1])
                        src_ratio = src_width / src_height
                        target_ratio = 16/9 if target_aspect == "16:9" else 9/16
                        if abs(src_ratio - target_ratio) / target_ratio > 0.05:
                            if src_ratio > target_ratio:
                                new_w = int(src_height * target_ratio)
                                new_h = src_height
                                x = (src_width - new_w) // 2
                                y = 0
                            else:
                                new_w = src_width
                                new_h = int(src_width / target_ratio)
                                x = 0
                                y = (src_height - new_h) // 2
                            crop_filter = f"crop={new_w}:{new_h}:{x}:{y}"
                            logger.info(f"[ClipSplit Thumbnail] âœ‚ï¸ è£å‰ª: {crop_filter}")
            except Exception as e:
                logger.warning(f"[ClipSplit Thumbnail] è·å–å°ºå¯¸å¤±è´¥: {e}")
        
        # 4. ä¸ºæ¯ä¸ª clip ç”Ÿæˆç¼©ç•¥å›¾
        STORAGE_BUCKET = "ai-creations"
        
        for i, clip in enumerate(clips):
            clip_id = clip.get("id")
            source_start = clip.get("source_start", 0)
            source_end = clip.get("source_end", source_start + 1000)
            mid_time_sec = (source_start + source_end) / 2 / 1000
            
            local_filename = f"clip_{i:03d}_{clip_id[:8]}.jpg"
            output_path = os.path.join(temp_dir, local_filename)
            
            try:
                # æ„å»ºæ»¤é•œé“¾
                filter_parts = []
                if crop_filter:
                    filter_parts.append(crop_filter)
                if target_aspect == "9:16":
                    filter_parts.append("scale=-2:'min(568,ih)'")
                else:
                    filter_parts.append("scale='min(320,iw)':-2")
                
                video_filter = ",".join(filter_parts) if filter_parts else None
                
                # æå–å¸§
                cmd = [
                    "ffmpeg", "-y",
                    "-ss", str(mid_time_sec),
                    "-i", video_path,
                    "-vframes", "1",
                ]
                if video_filter:
                    cmd.extend(["-vf", video_filter])
                cmd.extend(["-q:v", "2", output_path])
                
                result = subprocess.run(cmd, capture_output=True, timeout=30)
                
                if result.returncode != 0 or not os.path.exists(output_path):
                    continue
                
                # ä¸Šä¼ åˆ° Supabase
                storage_path = f"shot_thumbnails/{session_id}/{local_filename}"
                
                with open(output_path, "rb") as f:
                    file_data = f.read()
                
                try:
                    supabase_client.storage.from_(STORAGE_BUCKET).remove([storage_path])
                except:
                    pass
                
                supabase_client.storage.from_(STORAGE_BUCKET).upload(
                    storage_path, file_data, {"content-type": "image/jpeg"}
                )
                
                public_url = supabase_client.storage.from_(STORAGE_BUCKET).get_public_url(storage_path)
                
                # æ›´æ–°æ•°æ®åº“ä¸­çš„ clip
                current_clip = supabase_client.table("clips").select("metadata").eq("id", clip_id).single().execute()
                if current_clip.data:
                    metadata = current_clip.data.get("metadata", {}) or {}
                    metadata["thumbnail_url"] = public_url
                    supabase_client.table("clips").update({"metadata": metadata}).eq("id", clip_id).execute()
                
                logger.info(f"[ClipSplit Thumbnail] âœ… {clip_id[:8]} ç¼©ç•¥å›¾å·²æ›´æ–°")
                os.remove(output_path)
                
            except Exception as e:
                logger.warning(f"[ClipSplit Thumbnail] clip {clip_id[:8]} å¤±è´¥: {e}")
                continue
        
        # æ¸…ç†
        shutil.rmtree(temp_dir, ignore_errors=True)
        logger.info(f"[ClipSplit Thumbnail] âœ… å…¨éƒ¨å®Œæˆï¼Œå…± {len(clips)} ä¸ª")
        
    except Exception as e:
        logger.error(f"[ClipSplit Thumbnail] å¼‚æ­¥ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()



