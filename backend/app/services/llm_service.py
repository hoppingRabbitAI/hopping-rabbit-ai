"""
LLM æœåŠ¡ - æ”¯æŒè±†åŒ…å¤§æ¨¡åž‹ & Google Gemini
ç”¨äºŽæ–‡æœ¬æƒ…ç»ªåˆ†æžä¸Žå‰ªè¾‘å†³ç­–

è±†åŒ… API æ–‡æ¡£: https://www.volcengine.com/docs/82379/1263482
Gemini API æ–‡æ¡£: https://ai.google.dev/gemini-api/docs
"""

import os
import json
import logging
import httpx
from typing import Dict, List, Optional
from app.config import get_settings

logger = logging.getLogger(__name__)

# ============================================
# é…ç½®å¸¸é‡
# ============================================

# LLM å‚æ•°
LLM_DEFAULT_MAX_TOKENS = 2000
LLM_DEFAULT_TEMPERATURE = 0.3  # ä½Žæ¸©åº¦ï¼Œè¾“å‡ºæ›´ç¨³å®š
LLM_REQUEST_TIMEOUT_SECONDS = 60.0

# æ—¥å¿—é¢„è§ˆé•¿åº¦
LOG_PROMPT_PREVIEW_LENGTH = 200
LOG_RESPONSE_ERROR_LENGTH = 500

# ============================================
# é…ç½® (ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–)
# ============================================

settings = get_settings()

# LLM Provider
LLM_PROVIDER = settings.llm_provider  # "doubao" æˆ– "gemini"

# ç«å±±æ–¹èˆŸ API (è±†åŒ…)
ARK_API_BASE = "https://ark.cn-beijing.volces.com/api/v3"
ARK_API_KEY = settings.volcengine_ark_api_key
DOUBAO_MODEL_ENDPOINT = settings.doubao_model_endpoint

# Google Gemini API
GEMINI_API_BASE = "https://generativelanguage.googleapis.com/v1beta"
GEMINI_API_KEY = settings.gemini_api_key
GEMINI_MODEL = settings.gemini_model


# ============================================
# æƒ…ç»ªåˆ†æž Prompt
# ============================================

EMOTION_ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å‰ªè¾‘åŠ©æ‰‹ã€‚åˆ†æžä»¥ä¸‹è§†é¢‘å°è¯ç‰‡æ®µï¼Œåˆ¤æ–­å…¶æƒ…ç»ªå’Œé‡è¦æ€§ã€‚

å°è¯åˆ—è¡¨:
{segments_text}

è¯·ä¸ºæ¯ä¸ªç‰‡æ®µè¾“å‡º JSON æ ¼å¼çš„åˆ†æžç»“æžœï¼Œæ ¼å¼å¦‚ä¸‹:
```json
{{
  "results": [
    {{
      "id": "ç‰‡æ®µID",
      "emotion": "neutral/excited/serious/happy/sad",
      "importance": "low/medium/high",
      "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
      "focus_word": "çªå‡ºçš„å…³é”®è¯(å¯é€‰)"
    }}
  ]
}}
```

åˆ¤æ–­è§„åˆ™:
- emotion (æƒ…ç»ª): 
  - excited: æ¿€åŠ¨ã€å…´å¥‹ã€å¼ºè°ƒé‡ç‚¹
  - serious: ä¸¥è‚ƒã€è®¤çœŸã€è®²é“ç†
  - happy: è½»æ¾ã€æ„‰å¿«ã€çŽ©ç¬‘
  - sad: æ‚²ä¼¤ã€é—æ†¾ã€æƒ‹æƒœ
  - neutral: å¹³æ·¡å™è¿°
- importance (é‡è¦æ€§):
  - high: æ ¸å¿ƒè§‚ç‚¹ã€æ€»ç»“æ€§è¯­å¥ã€å«"é‡è¦/å…³é”®/å¿…é¡»"ç­‰è¯
  - medium: æ™®é€šå†…å®¹
  - low: è¿‡æ¸¡å¥ã€å£å¤´ç¦…ã€æ— æ„ä¹‰çš„è¯­æ°”è¯
- focus_word (ç„¦ç‚¹è¯):
  - åªæœ‰åœ¨è¯­æ°”çªç„¶è½¬æŠ˜æˆ–å¼ºçƒˆå¼ºè°ƒæ—¶æ‰å¡«å†™
  - ä¾‹å¦‚: "ä½†æ˜¯", "ä¸è¿‡", "ç„¶è€Œ", "å¿…é¡»", "ç»å¯¹", "å“‡"
  - å¿…é¡»æ˜¯åŽŸæ–‡ä¸­å­˜åœ¨çš„è¯

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""


# ============================================
# API è°ƒç”¨
# ============================================

async def call_llm(
    prompt: str,
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å‰ªè¾‘åŠ©æ‰‹ã€‚",
    max_tokens: int = LLM_DEFAULT_MAX_TOKENS
) -> Optional[str]:
    """
    ç»Ÿä¸€ LLM è°ƒç”¨å…¥å£ï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹© provider
    """
    if LLM_PROVIDER == "gemini" and GEMINI_API_KEY:
        return await call_gemini_llm(prompt, system_prompt, max_tokens)
    else:
        return await call_doubao_llm(prompt, system_prompt, max_tokens)


async def call_gemini_llm(
    prompt: str,
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å‰ªè¾‘åŠ©æ‰‹ã€‚",
    max_tokens: int = LLM_DEFAULT_MAX_TOKENS
) -> Optional[str]:
    """
    è°ƒç”¨ Google Gemini API
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥
        system_prompt: ç³»ç»Ÿæç¤º
        max_tokens: æœ€å¤§è¾“å‡º token æ•°
    
    Returns:
        æ¨¡åž‹è¾“å‡ºæ–‡æœ¬ï¼Œå¤±è´¥è¿”å›ž None
    """
    if not GEMINI_API_KEY:
        logger.warning("      âš ï¸ GEMINI_API_KEY æœªé…ç½®")
        return None
    
    # Gemini API æ ¼å¼
    url = f"{GEMINI_API_BASE}/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    
    headers = {
        "Content-Type": "application/json"
    }
    
    body = {
        "contents": [
            {
                "role": "user",
                "parts": [{"text": f"{system_prompt}\n\n{prompt}"}]
            }
        ],
        "generationConfig": {
            "temperature": LLM_DEFAULT_TEMPERATURE,
            "maxOutputTokens": max_tokens,
        }
    }
    
    # æ‰“å° LLM è°ƒç”¨ä¿¡æ¯
    prompt_preview = prompt[:LOG_PROMPT_PREVIEW_LENGTH] + '...' if len(prompt) > LOG_PROMPT_PREVIEW_LENGTH else prompt
    logger.info(f"      ðŸ¤– [LLM] è°ƒç”¨ Gemini æ¨¡åž‹")
    logger.info(f"         æ¨¡åž‹: {GEMINI_MODEL}")
    logger.info(f"         è¾“å…¥é•¿åº¦: {len(prompt)} å­—ç¬¦")
    logger.debug(f"         Prompt é¢„è§ˆ: {prompt_preview}")
    
    try:
        async with httpx.AsyncClient(timeout=LLM_REQUEST_TIMEOUT_SECONDS) as client:
            logger.info(f"         â†’ å‘é€è¯·æ±‚åˆ° Gemini API ...")
            
            response = await client.post(url, headers=headers, json=body)
            
            if response.status_code != 200:
                logger.error(f"         âŒ Gemini API é”™è¯¯: {response.status_code}")
                logger.error(f"         å“åº”: {response.text[:LOG_RESPONSE_ERROR_LENGTH]}")
                return None
            
            result = response.json()
            
            # è§£æž Gemini å“åº”æ ¼å¼
            candidates = result.get("candidates", [])
            if not candidates:
                logger.error("         âŒ Gemini æ— å“åº”å†…å®¹")
                return None
            
            content = candidates[0].get("content", {}).get("parts", [{}])[0].get("text", "")
            
            # æ‰“å° token ä½¿ç”¨æƒ…å†µ
            usage = result.get("usageMetadata", {})
            logger.info(f"         âœ“ å“åº”æˆåŠŸ!")
            logger.info(f"         Token ä½¿ç”¨: prompt={usage.get('promptTokenCount', '?')}, completion={usage.get('candidatesTokenCount', '?')}, total={usage.get('totalTokenCount', '?')}")
            logger.info(f"         è¾“å‡ºé•¿åº¦: {len(content)} å­—ç¬¦")
            
            return content
            
    except Exception as e:
        logger.error(f"         âŒ Gemini API è°ƒç”¨å¤±è´¥: {e}")
        return None


async def call_doubao_llm(
    prompt: str,
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å‰ªè¾‘åŠ©æ‰‹ã€‚",
    max_tokens: int = LLM_DEFAULT_MAX_TOKENS
) -> Optional[str]:
    """
    è°ƒç”¨è±†åŒ…å¤§æ¨¡åž‹ API
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥
        system_prompt: ç³»ç»Ÿæç¤º
        max_tokens: æœ€å¤§è¾“å‡º token æ•°
    
    Returns:
        æ¨¡åž‹è¾“å‡ºæ–‡æœ¬ï¼Œå¤±è´¥è¿”å›ž None
    """
    if not ARK_API_KEY:
        logger.warning("      âš ï¸ VOLCENGINE_ARK_API_KEY æœªé…ç½®")
        return None
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {ARK_API_KEY}"
    }
    
    body = {
        "model": DOUBAO_MODEL_ENDPOINT,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": LLM_DEFAULT_TEMPERATURE
    }
    
    # æ‰“å° LLM è°ƒç”¨ä¿¡æ¯
    prompt_preview = prompt[:LOG_PROMPT_PREVIEW_LENGTH] + '...' if len(prompt) > LOG_PROMPT_PREVIEW_LENGTH else prompt
    logger.info(f"      ðŸ¤– [LLM] è°ƒç”¨è±†åŒ…æ¨¡åž‹")
    logger.info(f"         æ¨¡åž‹: {DOUBAO_MODEL_ENDPOINT}")
    logger.info(f"         è¾“å…¥é•¿åº¦: {len(prompt)} å­—ç¬¦")
    logger.debug(f"         Prompt é¢„è§ˆ: {prompt_preview}")
    
    try:
        async with httpx.AsyncClient(timeout=LLM_REQUEST_TIMEOUT_SECONDS) as client:
            logger.info(f"         â†’ å‘é€è¯·æ±‚åˆ° {ARK_API_BASE}/chat/completions ...")
            
            response = await client.post(
                f"{ARK_API_BASE}/chat/completions",
                headers=headers,
                json=body
            )
            
            if response.status_code != 200:
                logger.error(f"         âŒ LLM API é”™è¯¯: {response.status_code}")
                logger.error(f"         å“åº”: {response.text[:LOG_RESPONSE_ERROR_LENGTH]}")
                return None
            
            result = response.json()
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            # æ‰“å° token ä½¿ç”¨æƒ…å†µ
            usage = result.get("usage", {})
            logger.info(f"         âœ“ å“åº”æˆåŠŸ!")
            logger.info(f"         Token ä½¿ç”¨: prompt={usage.get('prompt_tokens', '?')}, completion={usage.get('completion_tokens', '?')}, total={usage.get('total_tokens', '?')}")
            logger.info(f"         è¾“å‡ºé•¿åº¦: {len(content)} å­—ç¬¦")
            
            return content
            
    except Exception as e:
        logger.error(f"         âŒ LLM API è°ƒç”¨å¤±è´¥: {e}")
        return None


async def analyze_segments_batch(
    segments: List[Dict],
    batch_size: int = 20
) -> Dict[str, Dict]:
    """
    æ‰¹é‡åˆ†æžç‰‡æ®µçš„æƒ…ç»ªå’Œé‡è¦æ€§
    
    Args:
        segments: [{"id": "xxx", "text": "å°è¯å†…å®¹"}, ...]
        batch_size: æ¯æ‰¹å¤„ç†çš„ç‰‡æ®µæ•°
    
    Returns:
        {segment_id: {"emotion": "...", "importance": "...", "keywords": [...]}}
    """
    if not ARK_API_KEY:
        logger.warning("      âš ï¸ LLM API Key æœªé…ç½®ï¼Œè¿”å›žç©ºç»“æžœ")
        return {}
    
    results = {}
    total_batches = (len(segments) + batch_size - 1) // batch_size
    
    logger.info(f"      ðŸ“¦ æ‰¹é‡åˆ†æž: {len(segments)} ä¸ªç‰‡æ®µï¼Œåˆ† {total_batches} æ‰¹å¤„ç† (æ¯æ‰¹ {batch_size} ä¸ª)")
    
    # åˆ†æ‰¹å¤„ç†
    for batch_idx, i in enumerate(range(0, len(segments), batch_size)):
        batch = segments[i:i + batch_size]
        
        logger.info(f"      â†’ å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ ({len(batch)} ä¸ªç‰‡æ®µ)...")
        
        # æž„å»ºè¾“å…¥æ–‡æœ¬
        segments_text = "\n".join([
            f"[{seg['id']}] {seg['text']}" 
            for seg in batch
        ])
        
        prompt = EMOTION_ANALYSIS_PROMPT.format(segments_text=segments_text)
        
        response = await call_llm(prompt)
        
        if response:
            try:
                # æå– JSON
                json_str = response
                if "```json" in response:
                    json_str = response.split("```json")[1].split("```")[0]
                elif "```" in response:
                    json_str = response.split("```")[1].split("```")[0]
                
                data = json.loads(json_str.strip())
                
                batch_results = 0
                for item in data.get("results", []):
                    seg_id = item.get("id", "")
                    if seg_id:
                        results[seg_id] = {
                            "emotion": item.get("emotion", "neutral"),
                            "importance": item.get("importance", "medium"),
                            "keywords": item.get("keywords", [])
                        }
                        batch_results += 1
                
                logger.info(f"         âœ“ ç¬¬ {batch_idx + 1} æ‰¹è§£æžæˆåŠŸ: {batch_results} æ¡ç»“æžœ")
                        
            except json.JSONDecodeError as e:
                logger.error(f"         âŒ JSON è§£æžå¤±è´¥: {e}")
                logger.error(f"         åŽŸå§‹å“åº”: {response[:500]}...")
        else:
            logger.warning(f"         âš ï¸ ç¬¬ {batch_idx + 1} æ‰¹ LLM æ— å“åº”")
    
    logger.info(f"      âœ… æ‰¹é‡åˆ†æžå®Œæˆ: å…± {len(results)} æ¡ç»“æžœ")
    
    return results


# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================

def is_llm_configured() -> bool:
    """æ£€æŸ¥ LLM API æ˜¯å¦å·²é…ç½®"""
    if LLM_PROVIDER == "gemini":
        return bool(GEMINI_API_KEY)
    else:
        return bool(ARK_API_KEY and DOUBAO_MODEL_ENDPOINT != "ep-xxxxxxxx")


def get_current_llm_provider() -> str:
    """èŽ·å–å½“å‰ä½¿ç”¨çš„ LLM provider"""
    if LLM_PROVIDER == "gemini" and GEMINI_API_KEY:
        return f"gemini ({GEMINI_MODEL})"
    else:
        return f"doubao ({DOUBAO_MODEL_ENDPOINT})"


# ============================================
# å›¾åƒ Prompt å¢žå¼º
# ============================================

IMAGE_PROMPT_ENHANCEMENT_SYSTEM = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI å›¾åƒç”Ÿæˆæç¤ºè¯ä¸“å®¶ã€‚
ç”¨æˆ·ä¼šç»™ä½ ä¸€ä¸ªç®€çŸ­çš„æè¿°ï¼Œä½ éœ€è¦å°†å…¶æ‰©å±•ä¸ºæ›´è¯¦ç»†ã€æ›´é€‚åˆ AI å›¾åƒç”Ÿæˆçš„ promptã€‚

é‡è¦è§„åˆ™ï¼š
1. å¦‚æžœç”¨æˆ·æåˆ°"æ¢èƒŒæ™¯"ã€"æ”¹èƒŒæ™¯"ç­‰ï¼Œè¦å¼ºè°ƒ"ä¿æŒäººç‰©ä¸å˜ï¼Œåªæ”¹å˜èƒŒæ™¯"
2. æ·»åŠ ç”»è´¨æè¿°è¯ï¼šå¦‚ high quality, detailed, professional photography
3. ä¿æŒç”¨æˆ·åŽŸæ„ï¼Œä¸è¦æ”¹å˜ä¸»é¢˜
4. è¾“å‡ºåº”è¯¥æ˜¯è‹±æ–‡ï¼ˆAI å›¾åƒæ¨¡åž‹æ›´æ“…é•¿è‹±æ–‡ï¼‰
5. å¦‚æžœç”¨æˆ·å·²ç»å†™äº†å¾ˆè¯¦ç»†çš„ promptï¼Œåªéœ€å¾®è°ƒæ¶¦è‰²
6. é•¿åº¦æŽ§åˆ¶åœ¨ 50-150 è¯

åªè¾“å‡ºå¢žå¼ºåŽçš„ promptï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""


async def enhance_image_prompt(
    user_prompt: str,
    is_image_to_image: bool = False
) -> str:
    """
    å¢žå¼ºç”¨æˆ·çš„å›¾åƒç”Ÿæˆ prompt
    
    Args:
        user_prompt: ç”¨æˆ·è¾“å…¥çš„åŽŸå§‹ prompt
        is_image_to_image: æ˜¯å¦æ˜¯å›¾ç”Ÿå›¾æ¨¡å¼
    
    Returns:
        å¢žå¼ºåŽçš„ promptï¼Œå¦‚æžœ LLM å¤±è´¥åˆ™è¿”å›žåŽŸå§‹ prompt
    """
    if not is_llm_configured():
        logger.info("[PromptEnhance] LLM æœªé…ç½®ï¼Œè·³è¿‡å¢žå¼º")
        return user_prompt
    
    # æž„å»ºè¯·æ±‚
    context = ""
    if is_image_to_image:
        context = "ï¼ˆæ³¨æ„ï¼šç”¨æˆ·ä¸Šä¼ äº†å‚è€ƒå›¾ç‰‡ï¼Œè¿™æ˜¯å›¾ç”Ÿå›¾æ¨¡å¼ï¼Œè¯·ç¡®ä¿ prompt ä¸­å¼ºè°ƒä¿ç•™åŽŸå›¾çš„äººç‰©/ä¸»ä½“ç‰¹å¾ï¼‰\n\n"
    
    prompt = f"{context}ç”¨æˆ·è¾“å…¥ï¼š{user_prompt}"
    
    try:
        enhanced = await call_llm(
            prompt=prompt,
            system_prompt=IMAGE_PROMPT_ENHANCEMENT_SYSTEM,
            max_tokens=300
        )
        
        if enhanced and len(enhanced.strip()) > 0:
            logger.info(f"[PromptEnhance] åŽŸå§‹: {user_prompt[:50]}... -> å¢žå¼º: {enhanced[:50]}...")
            return enhanced.strip()
        else:
            return user_prompt
            
    except Exception as e:
        logger.warning(f"[PromptEnhance] å¢žå¼ºå¤±è´¥: {e}")
        return user_prompt
