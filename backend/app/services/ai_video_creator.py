"""
AI ä¸€é”®æˆç‰‡æ ¸å¿ƒæœåŠ¡
æ•´åˆ VAD + ASR + CV + LLM å®ç°æ™ºèƒ½å‰ªè¾‘

é‡æ„è¯´æ˜ (2026-01):
- è¿é•œè§„åˆ™å·²è¿ç§»è‡³ transform_rules.py
- ä½¿ç”¨å¯æ‰©å±•çš„è§„åˆ™å¼•æ“æ¶æ„
- èšç„¦"AIä¿®æ”¹è§†é¢‘æ¯”ä¾‹"èƒ½åŠ›
"""

import logging
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from enum import Enum

# ä»è§„åˆ™å¼•æ“å¯¼å…¥ç±»å‹å’Œå¼•æ“
from app.services.transform_rules import (
    EmotionType,
    ImportanceLevel,
    EasingType,
    SegmentContext,
    TransformParams,
    transform_engine,
    sequence_processor,
    DEFAULT_CENTER_X,
    DEFAULT_CENTER_Y,
)

logger = logging.getLogger(__name__)


# ============================================
# é…ç½®å¸¸é‡
# ============================================

# ç‰‡æ®µæ—¶é•¿é˜ˆå€¼ (æ¯«ç§’)
MIN_SEGMENT_DURATION_MS = 500  # ç‰‡æ®µæœ€å°æœ‰æ•ˆæ—¶é•¿
SHORT_GAP_MERGE_THRESHOLD_MS = 300  # çŸ­é—´éš™åˆå¹¶é˜ˆå€¼
FOCUS_WORD_EXTENSION_MS = 500  # ç„¦ç‚¹è¯æ‰©å±•æ—¶é•¿

# äººè„¸æ£€æµ‹é»˜è®¤å€¼
DEFAULT_FACE_CENTER_Y_OFFSET = 0.4  # å£æ’­å¸¸è§æ„å›¾ï¼Œç¨å¾®åä¸Š

# ç¼©æ”¾é˜ˆå€¼
MIN_SCALE_DELTA_THRESHOLD = 0.03  # ç¼©æ”¾å˜åŒ–æœ€å°é˜ˆå€¼

# ä½ç§»è®¡ç®—ç³»æ•°
POSITION_OFFSET_FACTOR = 0.8  # ä½ç½®åç§»ç³»æ•°

# æ¯«ç§’è½¬ç§’
MS_TO_SECONDS = 1000.0


@dataclass
class SmartSegment:
    """æ™ºèƒ½åˆ‡ç‰‡ç»“æ„"""
    id: str
    start: float  # æ¯«ç§’
    end: float    # æ¯«ç§’
    text: str
    
    # è§†è§‰åˆ†æç»“æœ
    has_face: bool = False
    face_center_x: float = DEFAULT_CENTER_X
    face_center_y: float = DEFAULT_CENTER_Y
    face_ratio: float = 0.0
    
    # LLM åˆ†æç»“æœ (Phase 4)
    emotion: EmotionType = EmotionType.NEUTRAL
    importance: ImportanceLevel = ImportanceLevel.MEDIUM
    keywords: List[str] = field(default_factory=list)
    focus_word: str = ""  # çªå‡ºçš„å…³é”®è¯ï¼ˆç”¨äºçªç„¶æ”¾å¤§ï¼‰
    
    # è¯çº§æ—¶é—´æˆ³ä¿¡æ¯ (ASR 2.0)
    words: List[Dict] = field(default_factory=list)
    
    # ç”Ÿæˆçš„è¿é•œå‚æ•°
    transform: Optional[Dict] = None  # å…ƒä¿¡æ¯ dictï¼ˆå‘åå…¼å®¹ï¼‰
    transform_params: Optional[TransformParams] = None  # å®Œæ•´çš„ TransformParams å¯¹è±¡
    
    # å…ƒæ•°æ® (ä¿ç•™é™éŸ³åˆ†çº§ç­‰ä¿¡æ¯)
    metadata: Optional[Dict] = field(default_factory=dict)
    
    @property
    def duration(self) -> float:
        """æ—¶é•¿ (æ¯«ç§’)"""
        return self.end - self.start
    
    @property
    def duration_seconds(self) -> float:
        """æ—¶é•¿ (ç§’)"""
        return self.duration / MS_TO_SECONDS
    
    @property
    def is_breath(self) -> bool:
        """æ˜¯å¦ä¸ºæ¢æ°”ç‰‡æ®µ"""
        return self.metadata.get("is_breath", False) if self.metadata else False
    
    @property
    def is_silence(self) -> bool:
        """æ˜¯å¦ä¸ºé™éŸ³ç‰‡æ®µ"""
        return self.metadata.get("silence_info") is not None if self.metadata else False


@dataclass
class AIEditingResult:
    """AI å‰ªè¾‘ç»“æœ"""
    segments: List[SmartSegment]
    total_duration: float
    speech_duration: float
    clips_count: int
    subtitles: List[Dict]
    metadata: Dict = field(default_factory=dict)


# ============================================
# æ ¸å¿ƒæœåŠ¡ç±»
# ============================================

class AIVideoCreatorService:
    """
    ä¸€é”® AI æˆç‰‡æœåŠ¡
    
    Pipeline:
    1. é¢„å¤„ç† -> 2. æ™ºèƒ½åˆ‡ç‰‡ (VAD+ASR) -> 3. è§†è§‰åˆ†æ -> 4. è¿é•œå†³ç­– -> 5. è¾“å‡º
    """
    
    def __init__(self) -> None:
        self._vision_service: Optional[Any] = None
        self._llm_service: Optional[Any] = None
    
    @property
    def vision_service(self) -> Any:
        """æ‡’åŠ è½½è§†è§‰æœåŠ¡"""
        if self._vision_service is None:
            from app.features.vision import vision_service
            self._vision_service = vision_service
        return self._vision_service
    
    async def process(
        self,
        video_path: str,
        audio_url: str,
        options: Optional[Dict] = None
    ) -> AIEditingResult:
        """
        æ‰§è¡Œä¸€é”®æˆç‰‡æµç¨‹
        
        Args:
            video_path: æœ¬åœ°è§†é¢‘æ–‡ä»¶è·¯å¾„
            audio_url: éŸ³é¢‘æ–‡ä»¶çš„å…¬ç½‘ URL (ç”¨äº ASR)
            options: å¯é€‰é…ç½® (å¦‚ style, enable_llm, transcript_segments ç­‰)
        
        Returns:
            AIEditingResult: åŒ…å«æ‰€æœ‰åˆ‡ç‰‡å’Œè¿é•œæ•°æ®
        """
        options = options or {}
        enable_llm = options.get("enable_llm", False)
        existing_segments = options.get("transcript_segments")  # å¤ç”¨å·²æœ‰çš„ ASR ç»“æœ
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ [AI Creator] å¼€å§‹ä¸€é”®æˆç‰‡æµç¨‹")
        logger.info(f"{'='*60}")
        logger.info(f"ğŸ“ è§†é¢‘æ–‡ä»¶: {video_path}")
        logger.info(f"ğŸ”§ LLM å¯ç”¨: {enable_llm}")
        
        # Step 1: æ™ºèƒ½åˆ‡ç‰‡ (å¤ç”¨å·²æœ‰ ASR æˆ–é‡æ–°è°ƒç”¨)
        logger.info(f"\nğŸ“ Step 1: æ™ºèƒ½åˆ‡ç‰‡ (ASR)")
        logger.info("-" * 40)
        if existing_segments:
            logger.info(f"   âœ“ å¤ç”¨å·²æœ‰ ASR ç»“æœ: {len(existing_segments)} ä¸ªç‰‡æ®µ")
            segments = self._convert_to_smart_segments(existing_segments)
        else:
            logger.info("   â†’ è°ƒç”¨ ASR æœåŠ¡è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
            segments = await self._step1_smart_segmentation(audio_url)
        
        # æ‰“å°ç‰‡æ®µæ‘˜è¦
        total_text_len = sum(len(s.text) for s in segments)
        breath_count = sum(1 for s in segments if s.is_breath)
        logger.info(f"   âœ“ æœ‰æ•ˆç‰‡æ®µ: {len(segments)} ä¸ª")
        logger.info(f"   âœ“ æ¢æ°”ç‰‡æ®µ: {breath_count} ä¸ª")
        logger.info(f"   âœ“ æ€»æ–‡æœ¬é•¿åº¦: {total_text_len} å­—ç¬¦")
        
        # Step 2: è§†è§‰åˆ†æ (MediaPipe)
        logger.info(f"\nğŸ“ Step 2: è§†è§‰åˆ†æ (äººè„¸æ£€æµ‹)")
        logger.info("-" * 40)
        segments = await self._step2_visual_analysis(video_path, segments)
        
        # æ‰“å°è§†è§‰åˆ†æç»“æœ
        face_segments = [s for s in segments if s.has_face]
        logger.info(f"   âœ“ æœ‰äººè„¸ç‰‡æ®µ: {len(face_segments)}/{len(segments)} ä¸ª")
        
        # Step 3: LLM è¯­ä¹‰åˆ†æ (å¯é€‰)
        if enable_llm:
            logger.info(f"\nğŸ“ Step 3: LLM è¯­ä¹‰åˆ†æ (è±†åŒ…å¤§æ¨¡å‹)")
            logger.info("-" * 40)
            segments = await self._step3_llm_analysis(segments)
        else:
            logger.info(f"\nğŸ“ Step 3: LLM è¯­ä¹‰åˆ†æ [å·²è·³è¿‡]")
            logger.info("   âš ï¸ enable_llm=Falseï¼Œä½¿ç”¨é»˜è®¤æƒ…ç»ªå’Œé‡è¦æ€§")
        
        # Step 4: ç”Ÿæˆè¿é•œå†³ç­–
        logger.info(f"\nğŸ“ Step 4: è¿é•œå†³ç­– (è§„åˆ™å¼•æ“)")
        logger.info("-" * 40)
        segments = self._step4_generate_transform(segments)
        
        # Step 5: ç”Ÿæˆå­—å¹•æ•°æ®
        logger.info(f"\nğŸ“ Step 5: ç”Ÿæˆå­—å¹•æ•°æ®")
        logger.info("-" * 40)
        subtitles = self._generate_subtitles(segments)
        logger.info(f"   âœ“ å­—å¹•æ¡æ•°: {len(subtitles)} æ¡")
        
        # æ±‡æ€»ç»Ÿè®¡
        total_duration = segments[-1].end if segments else 0
        speech_duration = sum(s.duration for s in segments)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… [AI Creator] ä¸€é”®æˆç‰‡å®Œæˆ!")
        logger.info(f"{'='*60}")
        logger.info(f"ğŸ“Š æ€»æ—¶é•¿: {total_duration/1000:.1f}s")
        logger.info(f"ğŸ“Š è¯­éŸ³æ—¶é•¿: {speech_duration/1000:.1f}s")
        logger.info(f"ğŸ“Š ç‰‡æ®µæ•°: {len(segments)} ä¸ª")
        logger.info(f"{'='*60}\n")
        
        return AIEditingResult(
            segments=segments,
            total_duration=total_duration,
            speech_duration=speech_duration,
            clips_count=len(segments),
            subtitles=subtitles,
            metadata={
                "enable_llm": enable_llm,
                "video_path": video_path
            }
        )
    
    async def _step1_smart_segmentation(self, audio_url: str) -> List[SmartSegment]:
        """
        Step 1: ä½¿ç”¨ ASR è¿›è¡Œæ™ºèƒ½åˆ‡ç‰‡
        """
        from app.tasks.transcribe import transcribe_audio
        
        result = await transcribe_audio(
            audio_url=audio_url,
            enable_word_timestamps=True
        )
        
        segments = []
        for seg in result.get("segments", []):
            # è¿‡æ»¤æ— æ•ˆç‰‡æ®µ
            if seg.get("is_deleted"):
                continue
            
            # è¿‡æ»¤é™éŸ³ç‰‡æ®µ
            if seg.get("silence_info"):
                continue
            
            text = seg.get("text", "").strip()
            if not text:
                continue
            
            smart_seg = SmartSegment(
                id=seg.get("id", ""),
                start=seg.get("start", 0),
                end=seg.get("end", 0),
                text=text,
                words=seg.get("words", [])
            )
            
            # è¿‡æ»¤è¿‡çŸ­çš„ç‰‡æ®µ (< MIN_SEGMENT_DURATION_MS)
            if smart_seg.duration >= MIN_SEGMENT_DURATION_MS:
                segments.append(smart_seg)
        
        # åˆå¹¶é—´éš”è¿‡çŸ­çš„ç›¸é‚»ç‰‡æ®µ (< SHORT_GAP_MERGE_THRESHOLD_MS)
        segments = self._merge_short_gaps(segments, min_gap_ms=SHORT_GAP_MERGE_THRESHOLD_MS)
        
        return segments
    
    def _refine_segments_with_focus(self, segments: List[SmartSegment]) -> List[SmartSegment]:
        """
        æ ¹æ® LLM è¯†åˆ«çš„ focus_word å’Œ ASR çš„ words æ—¶é—´æˆ³ï¼Œç»†åŒ–åˆ‡åˆ†ç‰‡æ®µ
        å®ç°"çªç„¶æ”¾å¤§"çš„æ•ˆæœï¼šå°†ä¸€ä¸ªé•¿ç‰‡æ®µåˆ‡åˆ†ä¸º Pre -> Focus(Instant Zoom) -> Post
        """
        refined_segments = []
        count_refined = 0
        
        for seg in segments:
            # 1. åŸºç¡€æ ¡éªŒï¼šæ— ç„¦ç‚¹è¯æˆ–æ— è¯çº§æ—¶é—´æˆ³ï¼Œç›´æ¥ä¿ç•™
            if not seg.focus_word or not seg.words:
                refined_segments.append(seg)
                continue
            
            # 2. æŸ¥æ‰¾ç„¦ç‚¹è¯ (Focus Word) åœ¨ words åˆ—è¡¨ä¸­çš„ä½ç½®
            # focus_word å¯èƒ½æ˜¯çŸ­è¯­ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼ŒåŒ¹é…å•ä¸ªè¯
            focus_text = seg.focus_word.strip()
            found_idx = -1
            found_word = None
            
            # åŒ¹é…é€»è¾‘ï¼šåŒ…å«åŒ¹é…
            for i, w in enumerate(seg.words):
                w_text = w.get("text", "")
                # ç§»é™¤æ ‡ç‚¹åæ¯”è¾ƒ
                clean_w = w_text.strip(".,?!ï¼Œã€‚ï¼Ÿï¼")
                clean_f = focus_text.strip(".,?!ï¼Œã€‚ï¼Ÿï¼")
                
                if clean_f and (clean_f in clean_w or clean_w in clean_f):
                    found_idx = i
                    found_word = w
                    break
            
            if not found_word:
                # æ²¡æ‰¾åˆ°å¯¹åº”è¯çš„æ—¶é—´æˆ³ï¼Œæ— æ³•åˆ‡åˆ†ï¼Œä¿ç•™åŸæ ·
                refined_segments.append(seg)
                continue
                
            # 3. æ‰§è¡Œåˆ‡åˆ†
            count_refined += 1
            
            w_start = found_word.get("start", 0)
            w_end = found_word.get("end", 0)
            
            # A. å‰æ®µ (Pre-focus)
            if w_start > seg.start + 100: # æœ€å°é—´éš” 100ms
                pre_text = "".join([w.get("text","") for w in seg.words[:found_idx]])
                pre_seg = SmartSegment(
                    id=f"{seg.id}_pre",
                    start=seg.start,
                    end=w_start,
                    text=pre_text or "...",
                    has_face=seg.has_face,
                    face_center_x=seg.face_center_x,
                    face_center_y=seg.face_center_y,
                    emotion=seg.emotion,
                    importance=ImportanceLevel.MEDIUM, # é™çº§ä¸ºæ™®é€š
                    words=seg.words[:found_idx]
                )
                refined_segments.append(pre_seg)
            
            # B. ç„¦ç‚¹æ®µ (Focus) - æ ¸å¿ƒéƒ¨åˆ†
            # ç¨å¾®å»¶é•¿ä¸€ç‚¹ç»“æŸæ—¶é—´ä»¥å±•ç¤ºæ•ˆæœï¼Œä½†ä¸èƒ½è¶…è¿‡åŸç‰‡æ®µç»“æŸæ—¶é—´
            focus_seg_end = min(w_end + FOCUS_WORD_EXTENSION_MS, seg.end) 
            
            focus_seg = SmartSegment(
                id=f"{seg.id}_focus",
                start=w_start,
                end=focus_seg_end,
                text=found_word.get("text", focus_text),
                has_face=seg.has_face,
                face_center_x=seg.face_center_x,
                face_center_y=seg.face_center_y,
                emotion=seg.emotion,
                importance=ImportanceLevel.HIGH, # æå‡ä¸ºé«˜é‡è¦æ€§
                metadata={"is_emphasis": True, "focus_word": focus_text}, # æ ‡è®°ï¼Œä¾›è§„åˆ™å¼•æ“ä½¿ç”¨
                words=[found_word] 
            )
            refined_segments.append(focus_seg)
            
            # C. åæ®µ (Post-focus)
            if focus_seg_end < seg.end - 100:
                post_text = "".join([w.get("text","") for w in seg.words[found_idx+1:]])
                post_seg = SmartSegment(
                    id=f"{seg.id}_post",
                    start=focus_seg_end,
                    end=seg.end,
                    text=post_text or "...",
                    has_face=seg.has_face,
                    face_center_x=seg.face_center_x,
                    face_center_y=seg.face_center_y,
                    emotion=seg.emotion, # ä¿æŒåŸæƒ…ç»ªï¼Œæˆ–é‡ç½®
                    importance=ImportanceLevel.MEDIUM,
                    words=seg.words[found_idx+1:]
                )
                refined_segments.append(post_seg)

        if count_refined > 0:
            logger.info(f"   âš¡ï¸ [SmartRefine] åŸºäºç„¦ç‚¹è¯ç»†åŒ–äº† {count_refined} ä¸ªåˆ‡ç‰‡ (Sudden Zoom)")
            
        return refined_segments

    def _convert_to_smart_segments(self, asr_segments: List[Dict]) -> List[SmartSegment]:
        """
        å°†å·²æœ‰çš„ ASR segments è½¬æ¢ä¸º SmartSegment
        
        ä¿ç•™ç²¾ç»†çš„é™éŸ³åˆ†çº§é€»è¾‘:
        - dead_air (æ­»å¯‚ >3s): è·³è¿‡
        - long_pause (å¥æœ«é•¿åœé¡¿ >2s): è·³è¿‡
        - hesitation (å¥ä¸­å¡é¡¿ >500ms): è·³è¿‡
        - breath (æ¢æ°”): ä¿ç•™ (ç”¨æˆ·å¯é€‰æ‹©åˆ é™¤)
        - uncertain: ä¿ç•™
        - è¯­éŸ³ç‰‡æ®µ: æ­£å¸¸å¤„ç†
        """
        segments = []
        
        # ç»Ÿè®¡
        skipped_count = {"dead_air": 0, "long_pause": 0, "hesitation": 0}
        breath_count = 0
        
        for seg in asr_segments:
            silence_info = seg.get("silence_info")
            
            # å¤„ç†é™éŸ³ç‰‡æ®µ
            if silence_info:
                classification = silence_info.get("classification")
                
                # æ­»å¯‚ã€é•¿åœé¡¿ã€å¡é¡¿ â†’ è‡ªåŠ¨è·³è¿‡
                if classification in ("dead_air", "long_pause", "hesitation"):
                    skipped_count[classification] = skipped_count.get(classification, 0) + 1
                    continue
                
                # æ¢æ°” â†’ ä¿ç•™ï¼ˆåˆ›å»ºä¸€ä¸ªç©ºæ–‡æœ¬çš„ segmentï¼Œåç»­ç”Ÿæˆè§†é¢‘ clip æ—¶ä¼šä¿ç•™æ—¶é•¿ï¼‰
                if classification == "breath":
                    breath_count += 1
                    smart_seg = SmartSegment(
                        id=seg.get("id", ""),
                        start=seg.get("start", 0),
                        end=seg.get("end", 0),
                        text="",  # æ¢æ°”æ²¡æœ‰æ–‡å­—
                    )
                    # æ ‡è®°ä¸ºæ¢æ°”ï¼Œåç»­å¯ç”¨äºè¿é•œå†³ç­–
                    smart_seg.metadata = {"is_breath": True, "silence_info": silence_info}
                    segments.append(smart_seg)
                    continue
                
                # uncertain â†’ ä¿ç•™
                if classification == "uncertain":
                    smart_seg = SmartSegment(
                        id=seg.get("id", ""),
                        start=seg.get("start", 0),
                        end=seg.get("end", 0),
                        text="",
                    )
                    smart_seg.metadata = {"is_uncertain": True, "silence_info": silence_info}
                    segments.append(smart_seg)
                    continue
            
            # è¯­éŸ³ç‰‡æ®µ
            text = seg.get("text", "").strip()
            if not text:
                continue
            
            seg_start = seg.get("start", 0)
            seg_end = seg.get("end", 0)
            duration = seg_end - seg_start
            
            # è¿‡æ»¤è¿‡çŸ­çš„ç‰‡æ®µ (< 200ms)
            if duration < 200:
                continue

            smart_seg = SmartSegment(
                id=seg.get("id", ""),
                start=seg_start,
                end=seg_end,
                text=text,
                words=seg.get("words", [])
            )
            segments.append(smart_seg)
        
        logger.info(f"[AI Creator] Converted ASR segments: {len(segments)} kept, "
                   f"skipped: {skipped_count}, breaths: {breath_count}")
        
        return segments
    
    def _merge_short_gaps(
        self, 
        segments: List[SmartSegment], 
        min_gap_ms: float = SHORT_GAP_MERGE_THRESHOLD_MS
    ) -> List[SmartSegment]:
        """åˆå¹¶é—´éš”è¿‡çŸ­çš„ç›¸é‚»ç‰‡æ®µ"""
        if len(segments) <= 1:
            return segments
        
        merged = [segments[0]]
        
        for seg in segments[1:]:
            prev = merged[-1]
            gap = seg.start - prev.end
            
            if gap < min_gap_ms:
                # åˆå¹¶ï¼šæ‰©å±•å‰ä¸€ä¸ªç‰‡æ®µ
                prev.end = seg.end
                prev.text = prev.text + " " + seg.text
            else:
                merged.append(seg)
        
        return merged
    
    async def _step2_visual_analysis(
        self, 
        video_path: str, 
        segments: List[SmartSegment]
    ) -> List[SmartSegment]:
        """
        Step 2: è§†è§‰åˆ†æï¼ˆä½¿ç”¨ MediaPipe äººè„¸æ£€æµ‹ï¼‰
        
        æ£€æµ‹æ¯ä¸ªç‰‡æ®µä¸­çš„äººè„¸ä½ç½®ï¼Œç”¨äºç²¾å‡†çš„è¿é•œæ¨è¿›æ•ˆæœã€‚
        """
        logger.info("   â†’ å¼€å§‹è§†è§‰åˆ†æï¼ˆäººè„¸æ£€æµ‹ï¼‰...")
        
        try:
            vision = self.vision_service
            detected_count = 0
            
            for seg in segments:
                if seg.is_breath:
                    # æ¢æ°”ç‰‡æ®µè·³è¿‡æ£€æµ‹ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    seg.has_face = True
                    seg.face_center_x = DEFAULT_CENTER_X
                    seg.face_center_y = DEFAULT_FACE_CENTER_Y_OFFSET
                    seg.face_ratio = 0.0
                    continue
                
                try:
                    # è°ƒç”¨è§†è§‰æœåŠ¡æ£€æµ‹äººè„¸
                    result = vision.analyze_clip_region(
                        video_path=video_path,
                        start_time=seg.start / MS_TO_SECONDS,
                        end_time=seg.end / MS_TO_SECONDS,
                        sample_rate=1.0  # æ¯ç§’é‡‡æ · 1 å¸§
                    )
                    
                    seg.has_face = result.get("has_face", False)
                    seg.face_center_x = result.get("center_x", DEFAULT_CENTER_X)
                    seg.face_center_y = result.get("center_y", DEFAULT_FACE_CENTER_Y_OFFSET)
                    seg.face_ratio = result.get("face_ratio", 0.0)
                    
                    if seg.has_face:
                        detected_count += 1
                        
                except Exception as e:
                    # å•ä¸ªç‰‡æ®µæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    logger.debug(f"      ç‰‡æ®µ {seg.id} äººè„¸æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    seg.has_face = True
                    seg.face_center_x = DEFAULT_CENTER_X
                    seg.face_center_y = DEFAULT_FACE_CENTER_Y_OFFSET
                    seg.face_ratio = 0.0
            
            logger.info(f"   âœ“ è§†è§‰åˆ†æå®Œæˆ: {detected_count}/{len(segments)} ä¸ªç‰‡æ®µæ£€æµ‹åˆ°äººè„¸")
            
        except Exception as e:
            # æ•´ä½“æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            logger.warning(f"   âš ï¸ è§†è§‰åˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤äººè„¸ä½ç½®")
            for seg in segments:
                seg.has_face = True
                seg.face_center_x = DEFAULT_CENTER_X
                seg.face_center_y = DEFAULT_FACE_CENTER_Y_OFFSET
                seg.face_ratio = 0.0
        
        return segments
    
    async def _step3_llm_analysis(
        self, 
        segments: List[SmartSegment]
    ) -> List[SmartSegment]:
        """
        Step 3: ä½¿ç”¨ LLM åˆ†ææ–‡æœ¬æƒ…ç»ªå’Œé‡è¦æ€§
        """
        from app.services.llm_service import analyze_segments_batch, is_llm_configured
        
        if not is_llm_configured():
            logger.warning("   âš ï¸ LLM API æœªé…ç½®ï¼Œè·³è¿‡è¯­ä¹‰åˆ†æ")
            return segments
        
        # è¿‡æ»¤å‡ºæœ‰æ–‡æœ¬çš„ç‰‡æ®µ
        text_segments = [{"id": s.id, "text": s.text} for s in segments if s.text.strip()]
        logger.info(f"   â†’ å¾…åˆ†æç‰‡æ®µ: {len(text_segments)} ä¸ª (æœ‰æ–‡æœ¬)")
        
        if not text_segments:
            logger.info("   âš ï¸ æ²¡æœ‰éœ€è¦åˆ†æçš„æ–‡æœ¬ç‰‡æ®µ")
            return segments
        
        # æ‰“å°éƒ¨åˆ†æ–‡æœ¬é¢„è§ˆ
        preview_count = min(3, len(text_segments))
        logger.info(f"   â†’ æ–‡æœ¬é¢„è§ˆ (å‰{preview_count}æ¡):")
        for i, seg in enumerate(text_segments[:preview_count]):
            text_preview = seg['text'][:50] + '...' if len(seg['text']) > 50 else seg['text']
            logger.info(f"      [{i+1}] {text_preview}")
        
        try:
            logger.info(f"   â†’ è°ƒç”¨è±†åŒ… LLM è¿›è¡Œæƒ…ç»ªåˆ†æ...")
            analyzed = await analyze_segments_batch(text_segments)
            
            logger.info(f"   âœ“ LLM è¿”å› {len(analyzed)} æ¡åˆ†æç»“æœ")
            
            # ç»Ÿè®¡æƒ…ç»ªåˆ†å¸ƒ
            emotion_counts = {}
            importance_counts = {}
            
            # æ›´æ–°åˆ†æç»“æœ
            for seg in segments:
                if seg.id in analyzed:
                    result = analyzed[seg.id]
                    seg.emotion = EmotionType(result.get("emotion", "neutral"))
                    seg.importance = ImportanceLevel(result.get("importance", "medium"))
                    seg.keywords = result.get("keywords", [])
                    seg.focus_word = result.get("focus_word", "")
                    
                    # ç»Ÿè®¡
                    emotion_counts[seg.emotion.value] = emotion_counts.get(seg.emotion.value, 0) + 1
                    importance_counts[seg.importance.value] = importance_counts.get(seg.importance.value, 0) + 1
            
            # æ‰“å°åˆ†æç»“æœç»Ÿè®¡
            logger.info(f"   ğŸ“Š æƒ…ç»ªåˆ†å¸ƒ: {emotion_counts}")
            logger.info(f"   ğŸ“Š é‡è¦æ€§åˆ†å¸ƒ: {importance_counts}")
            
            # æ‰“å°éƒ¨åˆ†è¯¦ç»†ç»“æœ
            analyzed_segs = [s for s in segments if s.id in analyzed]
            logger.info(f"   â†’ è¯¦ç»†ç»“æœé¢„è§ˆ (å‰5æ¡):")
            for seg in analyzed_segs[:5]:
                text_preview = seg.text[:30] + '...' if len(seg.text) > 30 else seg.text
                keywords_str = ', '.join(seg.keywords[:3]) if seg.keywords else '-'
                logger.info(f"      [{seg.emotion.value:8}|{seg.importance.value:6}] \"{text_preview}\" å…³é”®è¯: {keywords_str}")
            
            # ç»†åŒ–ç„¦ç‚¹è¯åˆ‡ç‰‡ (New)
            segments = self._refine_segments_with_focus(segments)
                    
        except Exception as e:
            logger.warning(f"   âŒ LLM åˆ†æå¤±è´¥: {e}")
            logger.info("   â†’ ä½¿ç”¨é»˜è®¤æƒ…ç»ªå’Œé‡è¦æ€§")
        
        return segments
    
    def _step4_generate_transform(
        self, 
        segments: List[SmartSegment]
    ) -> List[SmartSegment]:
        """
        Step 4: æ ¹æ®åˆ†æç»“æœç”Ÿæˆè¿é•œå‚æ•°
        
        ä½¿ç”¨å¯æ‰©å±•çš„è§„åˆ™å¼•æ“ (transform_rules.py)ï¼š
        - EmotionZoomRule: æƒ…ç»ªé©±åŠ¨çš„ç¼©æ”¾è§„åˆ™
        - NoFaceZoomRule: æ— äººè„¸æ—¶çš„ Ken Burns æ•ˆæœ
        - ShortClipRule: çŸ­ç‰‡æ®µå¤„ç†
        - BreathClipRule: æ¢æ°”ç‰‡æ®µå¤„ç†
        
        æ–°å¢ï¼šåºåˆ—æ„ŸçŸ¥åå¤„ç†å™¨ (SequenceAwarePostProcessor)
        - é¿å…è¿ç»­ç‰‡æ®µä½¿ç”¨ç›¸åŒè¿é•œæ•ˆæœ
        - é«˜æ½®åè‡ªåŠ¨æ’å…¥"å‘¼å¸"ç‰‡æ®µ
        - ç¡®ä¿è§†è§‰èŠ‚å¥å¤šæ ·æ€§
        
        è§„åˆ™å¼•æ“æ”¯æŒåç»­æ‰©å±•æ›´å¤šè§„åˆ™ï¼Œå¦‚è½¬åœºã€ç‰¹æ•ˆç­‰ã€‚
        """
        # é‡ç½®åºåˆ—å¤„ç†å™¨çŠ¶æ€
        sequence_processor.reset()
        
        # æ„å»ºè§„åˆ™å¼•æ“ä¸Šä¸‹æ–‡åˆ—è¡¨
        contexts = []
        for seg in segments:
            context = SegmentContext(
                segment_id=seg.id,
                duration_ms=seg.duration,
                text=seg.text,
                has_face=seg.has_face,
                face_center_x=seg.face_center_x,
                face_center_y=seg.face_center_y,
                face_ratio=seg.face_ratio,
                emotion=seg.emotion,
                importance=seg.importance,
                keywords=seg.keywords,
                is_breath=seg.is_breath,
                metadata=seg.metadata or {},
            )
            contexts.append(context)
        
        # ä½¿ç”¨è§„åˆ™å¼•æ“æ‰¹é‡å¤„ç†
        params_list = [transform_engine.process(ctx) for ctx in contexts]
        
        # åºåˆ—æ„ŸçŸ¥åå¤„ç†ï¼šç¡®ä¿è¿é•œå¤šæ ·æ€§
        params_contexts = list(zip(params_list, contexts))
        processed_params = sequence_processor.process_batch(params_contexts)
        
        # è½¬æ¢ä¸ºå…ƒä¿¡æ¯å¹¶èµ‹å€¼ï¼ˆå…³é”®å¸§ç”±è°ƒç”¨æ–¹ç”Ÿæˆå¹¶å­˜å…¥ keyframes è¡¨ï¼‰
        for seg, params in zip(segments, processed_params):
            seg.transform = params.get_meta()  # å…ƒä¿¡æ¯ dictï¼ˆå‘åå…¼å®¹ï¼‰
            seg.transform_params = params  # å®Œæ•´çš„ TransformParams å¯¹è±¡ï¼ˆç”¨äºç”Ÿæˆå…³é”®å¸§ï¼‰
        
        # æ‰“å°è¿é•œå†³ç­–ç»Ÿè®¡
        rule_counts = {}
        strategy_counts = {"keyframe": 0, "instant": 0, "static": 0}
        
        for seg in segments:
            rule = seg.transform.get('_rule_applied', 'unknown') if seg.transform else 'none'
            rule_name = rule.split(':')[0]  # å–è§„åˆ™å
            rule_counts[rule_name] = rule_counts.get(rule_name, 0) + 1
            
            # ç»Ÿè®¡ç­–ç•¥ç±»å‹
            strategy = seg.transform.get('_strategy', 'unknown') if seg.transform else 'none'
            if 'keyframe' in strategy:
                strategy_counts["keyframe"] += 1
            elif 'instant' in strategy:
                strategy_counts["instant"] += 1
            else:
                strategy_counts["static"] += 1
        
        logger.info(f"   ğŸ“Š è§„åˆ™åº”ç”¨ç»Ÿè®¡: {rule_counts}")
        logger.info(f"   ğŸ¬ è¿é•œç­–ç•¥åˆ†å¸ƒ: keyframe={strategy_counts['keyframe']}, instant={strategy_counts['instant']}, static={strategy_counts['static']}")
        
        # æ‰“å°éƒ¨åˆ†è¿é•œå†³ç­–è¯¦æƒ…
        logger.info(f"   â†’ è¿é•œå†³ç­–è¯¦æƒ… (å‰5æ¡):")
        for seg in segments[:5]:
            if seg.transform:
                rule = seg.transform.get('_rule_applied', 'unknown')
                strategy = seg.transform.get('_strategy', 'unknown')
                text_preview = seg.text[:20] + '...' if len(seg.text) > 20 else (seg.text or '[æ¢æ°”]')
                logger.info(f"      [{rule:25}] strategy={strategy:15} | \"{text_preview}\"")
        
        return segments
    
    def _generate_subtitles(self, segments: List[SmartSegment]) -> List[Dict]:
        """ç”Ÿæˆå­—å¹•æ•°æ®"""
        return [
            {
                "id": seg.id,
                "text": seg.text,
                "start": seg.start,
                "end": seg.end,
                "style": "default"
            }
            for seg in segments
        ]


# å•ä¾‹å¯¼å‡º
ai_video_creator = AIVideoCreatorService()
