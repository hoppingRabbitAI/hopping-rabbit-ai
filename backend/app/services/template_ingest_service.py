"""
Template Ingest Service
è´Ÿè´£å°†çˆ†æ¬¾è§†é¢‘/å›¾ç‰‡æ‹†è§£ä¸ºæ¨¡æ¿å¹¶å†™å…¥æ•°æ®åº“
"""
from __future__ import annotations

import asyncio
import io
import logging
import os
import re
import tempfile
import uuid
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union

import httpx
from PIL import Image

from app.services.supabase_client import get_supabase

logger = logging.getLogger(__name__)

TEMPLATE_BUCKET = "templates"
TEMPLATE_PREFIX = "visual-backgrounds/"
DEFAULT_THUMB_MAX = 512


@dataclass
class TemplateAsset:
    template_id: str
    name: str
    category: str
    type: str
    storage_path: str
    thumbnail_path: Optional[str]
    url: str
    thumbnail_url: str


class TemplateIngestService:
    def __init__(self) -> None:
        self._tasks: Dict[str, asyncio.Task] = {}

    def enqueue(self, job_id: str) -> None:
        """å¼‚æ­¥å¤„ç†æ¨¡æ¿é‡‡é›†ä»»åŠ¡"""
        if job_id in self._tasks and not self._tasks[job_id].done():
            return
        self._tasks[job_id] = asyncio.create_task(self._process_job(job_id))

    async def _process_job(self, job_id: str) -> None:
        supabase = get_supabase()
        now = datetime.utcnow().isoformat()
        supabase.table("template_ingest_jobs").update({
            "status": "processing",
            "progress": 0.05,
            "started_at": now,
            "updated_at": now,
        }).eq("id", job_id).execute()

        try:
            job = supabase.table("template_ingest_jobs").select("*").eq("id", job_id).single().execute().data
            if not job:
                raise RuntimeError("Ingest job not found")

            source_type = job.get("source_type", "video")
            ingest_output: Union[List[TemplateAsset], Tuple[List[TemplateAsset], Dict[str, Any]]]

            if source_type == "image":
                ingest_output = await self._ingest_image(job)
            elif source_type == "zip":
                ingest_output = await self._ingest_zip(job)
            else:
                ingest_output = await self._ingest_video(job)

            pack_summary: Optional[Dict[str, Any]] = None
            if isinstance(ingest_output, tuple):
                templates, pack_summary = ingest_output
            else:
                templates = ingest_output

            result_payload: Dict[str, Any] = {
                "templates": [
                    {
                        "template_id": t.template_id,
                        "name": t.name,
                        "category": t.category,
                        "type": t.type,
                        "storage_path": t.storage_path,
                        "thumbnail_path": t.thumbnail_path,
                        "url": t.url,
                        "thumbnail_url": t.thumbnail_url,
                    }
                    for t in templates
                ]
            }
            if pack_summary:
                result_payload.update(pack_summary)

            supabase.table("template_ingest_jobs").update({
                "status": "succeeded",
                "progress": 1,
                "result": result_payload,
                "completed_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat(),
            }).eq("id", job_id).execute()
        except Exception as exc:
            logger.error("[TemplateIngest] å¤„ç†å¤±è´¥: %s", exc, exc_info=True)
            supabase.table("template_ingest_jobs").update({
                "status": "failed",
                "progress": 1,
                "error_code": "INGEST_FAILED",
                "error_message": str(exc),
                "completed_at": datetime.utcnow().isoformat(),
                "updated_at": datetime.utcnow().isoformat(),
            }).eq("id", job_id).execute()

    async def _ingest_image(self, job: Dict[str, Any]) -> List[TemplateAsset]:
        image_bytes = await self._download_bytes(job["source_url"])
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        return [await self._create_template_from_image(image, job, index=0)]

    async def _ingest_zip(self, job: Dict[str, Any]) -> List[TemplateAsset]:
        import zipfile

        data = await self._download_bytes(job["source_url"])
        templates: List[TemplateAsset] = []

        with zipfile.ZipFile(io.BytesIO(data)) as archive:
            image_files = [f for f in archive.namelist() if f.lower().endswith((".jpg", ".jpeg", ".png", ".webp"))]
            for idx, file_name in enumerate(image_files):
                with archive.open(file_name) as file_obj:
                    image = Image.open(file_obj).convert("RGB")
                templates.append(await self._create_template_from_image(image, job, index=idx))

        if not templates:
            raise RuntimeError("Zip ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return templates

    async def _ingest_video(
        self,
        job: Dict[str, Any],
    ) -> Union[List[TemplateAsset], Tuple[List[TemplateAsset], Dict[str, Any]]]:
        """ç»Ÿä¸€èµ°è‡ªåŠ¨åˆ†é•œæ£€æµ‹ç®¡çº¿ï¼Œä¸å†åŒºåˆ† ad/transition"""
        job_id = job.get("id", "unknown")
        source_url = job.get("source_url", "")
        logger.info(f"[TemplateIngest] === å¼€å§‹è§†é¢‘å…¥åº“ï¼ˆè‡ªåŠ¨åˆ†é•œï¼‰ === job_id={job_id}, url={source_url[:100]}...")
        return await self._ingest_transition_video(job)

    async def _ingest_transition_video(self, job: Dict[str, Any]) -> Tuple[List[TemplateAsset], Dict[str, Any]]:
        job_id = job.get("id", "unknown")
        # è½¬åœºæ¨¡å¼ï¼šä¸ç”¨ extract_frames ç¡¬ç¼–ç æ•°é‡ï¼Œç”± scene detection è‡ªåŠ¨å†³å®š
        # extract_frames ä»…ä½œä¸ºå®‰å…¨ä¸Šé™ï¼ˆé˜²æ­¢ç”Ÿæˆè¿‡å¤šæ¨¡æ¿ï¼‰
        max_cap = max(1, min(int(job.get("extract_frames") or 32), 64))
        transition_duration_ms = self._parse_transition_duration_ms(job)
        clip_ranges = job.get("clip_ranges") or []
        video_url = str(job.get("source_url") or "")
        if not video_url:
            raise RuntimeError("ç¼ºå°‘ source_url")
        
        logger.info(f"[TemplateIngest] === å¼€å§‹è½¬åœºè§†é¢‘å…¥åº“(æ™ºèƒ½æ£€æµ‹) === job_id={job_id}, max_cap={max_cap}, duration_ms={transition_duration_ms}")
        logger.info(f"[TemplateIngest] è½¬åœºè§†é¢‘URL: {video_url}")

        pack_id = f"pack-{uuid.uuid4().hex[:10]}"
        # ç”¨ä¸€ä¸ªå¤§ä¸Šé™è®© scene detection å……åˆ†æ£€æµ‹æ‰€æœ‰è½¬åœº
        detected_ranges, detection_debug = await self._detect_transition_ranges(
            video_url=video_url,
            max_ranges=max_cap,
            clip_ranges=clip_ranges,
            transition_duration_ms=transition_duration_ms,
        )
        detected_segments = len(detected_ranges)
        logger.info(f"[TemplateIngest] æ£€æµ‹åˆ° {detected_segments} ä¸ªè½¬åœºèŒƒå›´: {detected_ranges[:5]}...")

        deduped_ranges = self._dedupe_transition_ranges(detected_ranges)
        deduped_templates = max(detected_segments - len(deduped_ranges), 0)
        # ä¸å†ç¡¬æˆªæ–­åˆ° extract_framesï¼Œè€Œæ˜¯å–å»é‡åçš„å…¨éƒ¨æœ‰æ•ˆè½¬åœºï¼ˆæœ€å¤š max_cap ä¸ªï¼‰
        selected_ranges = deduped_ranges[:max_cap] if len(deduped_ranges) > max_cap else deduped_ranges
        auto_detected_count = len(selected_ranges)
        logger.info(f"[TemplateIngest] æ™ºèƒ½æ£€æµ‹: å»é‡å {len(deduped_ranges)} ä¸ªè½¬åœºï¼Œé€‰å– {auto_detected_count} ä¸ª")
        detection_debug["deduped_range_count"] = len(deduped_ranges)
        detection_debug["auto_detected_count"] = auto_detected_count
        detection_debug["published_ranges"] = [
            {"start": round(start, 3), "end": round(end, 3)} for start, end in selected_ranges
        ]

        # åŠ¨æ€è®¡ç®—æ¯ä¸ªè½¬åœºçš„ A/B å¸§åç§»é‡ï¼š
        #   Aå¸§: è½¬åœºåŒºåŸŸå‰çš„æ¸…æ™°é™æ€å¸§ï¼ˆåç§»é‡æ ¹æ®åˆ°å‰ä¸€ä¸ªè½¬åœºçš„é—´è·åŠ¨æ€è®¡ç®—ï¼‰
        #   Midå¸§: è½¬åœºä¸­å¿ƒï¼ˆä»…ç”¨äº LLM åˆ†æï¼Œä¸ç”¨äºå±•ç¤ºï¼‰
        #   Bå¸§: è½¬åœºåŒºåŸŸåçš„æ¸…æ™°é™æ€å¸§
        total_dur = detection_debug.get("duration_sec", 999)
        all_timestamps: List[float] = []
        for i, (start, end) in enumerate(selected_ranges):
            # è®¡ç®—åˆ°å‰åé‚»å±…è½¬åœºçš„é—´è·
            gap_before = start if i == 0 else start - selected_ranges[i - 1][1]
            gap_after = (total_dur - end) if i == len(selected_ranges) - 1 else selected_ranges[i + 1][0] - end
            # A/B åç§»: å–é—´è·çš„40%ï¼Œé™åˆ¶åœ¨ [0.2s, 1.5s]
            offset_a = max(0.2, min(1.5, gap_before * 0.4))
            offset_b = max(0.2, min(1.5, gap_after * 0.4))
            ts_a = max(0.0, start - offset_a)
            ts_mid = (start + end) / 2.0
            ts_b = min(total_dur, end + offset_b)
            all_timestamps.extend([ts_a, ts_mid, ts_b])
        logger.info(f"[TemplateIngest] æå–å¸§æ—¶é—´æˆ³ (A/Mid/B x{len(selected_ranges)}): {[round(t,3) for t in all_timestamps]}")
        all_frames = await self._extract_frames_at_timestamps(video_url=video_url, timestamps=all_timestamps)
        logger.info(f"[TemplateIngest] æˆåŠŸæå– {len(all_frames)} å¸§")

        # ---------- å‡†å¤‡æ¯ä¸ªè½¬åœºçš„å¸§æ•°æ® ----------
        transition_items: List[Dict[str, Any]] = []
        for idx, (start, end) in enumerate(selected_ranges):
            base = idx * 3
            frame_a = all_frames[base] if base < len(all_frames) else None
            frame_mid = all_frames[base + 1] if base + 1 < len(all_frames) else None
            frame_b = all_frames[base + 2] if base + 2 < len(all_frames) else None

            # å±•ç¤ºå¸§ç”¨æ¸…æ™°çš„ A å¸§ï¼ˆè½¬åœºå‰ï¼‰ï¼Œä¸ç”¨æ¨¡ç³Šçš„ Mid å¸§
            display_frame = frame_a or frame_b or frame_mid
            if display_frame is None:
                logger.warning(f"[TemplateIngest] è½¬åœº {idx} æ— æ³•æå–å¸§ï¼Œè·³è¿‡")
                continue
            transition_items.append({
                "idx": idx, "start": start, "end": end,
                "frame_a": frame_a, "frame_mid": frame_mid, "frame_b": frame_b,
                "display_frame": display_frame,
            })

        # ---------- å¹¶å‘è§†é¢‘ç†è§£åˆ†æï¼ˆä¸é™çº§ï¼‰ ----------
        import asyncio
        logger.info(f"[TemplateIngest] å¯åŠ¨ {len(transition_items)} ä¸ªè½¬åœºçš„å¹¶å‘è§†é¢‘ç†è§£åˆ†æ...")
        analysis_tasks = [
            self._analyze_transition_frames(
                frame_a=item["frame_a"],
                frame_mid=item["frame_mid"],
                frame_b=item["frame_b"],
                index=item["idx"],
                video_url=video_url,
                start_sec=item["start"],
                end_sec=item["end"],
            )
            for item in transition_items
        ]
        analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)

        # ---------- æ£€æŸ¥åˆ†æé”™è¯¯ï¼ˆä¸å†é™é»˜é™çº§ï¼‰ ----------
        for item, analysis in zip(transition_items, analysis_results):
            if isinstance(analysis, Exception):
                idx = item["idx"]
                raise RuntimeError(
                    f"è½¬åœº {idx} è§†é¢‘åˆ†æå¤±è´¥: {analysis}"
                ) from analysis

        # ---------- ç»„è£…æ¨¡æ¿ ----------
        templates: List[TemplateAsset] = []
        for item, analysis in zip(transition_items, analysis_results):
            idx = item["idx"]
            logger.info(f"[TemplateIngest] è½¬åœº {idx} åˆ†æç»“æœ: {analysis}")
            transition_spec = self._build_transition_spec(
                start_sec=item["start"],
                end_sec=item["end"],
                index=idx,
                job=job,
                analysis=analysis,
            )
            metadata_extra = {
                "transition_spec": transition_spec,
                "transition_pack": {
                    "pack_id": pack_id,
                    "source_video_url": video_url,
                    "detected_segments": detected_segments,
                    "cluster_method": "range_overlap_v1",
                },
            }
            source_timecode = f"{item['start']:.3f}-{item['end']:.3f}"
            templates.append(
                await self._create_template_from_image(
                    item["display_frame"],
                    job,
                    index=idx,
                    source_timecode=source_timecode,
                    metadata_extra=metadata_extra,
                )
            )

        if not templates:
            raise RuntimeError("è½¬åœºè§†é¢‘æœªæå–åˆ°æœ‰æ•ˆæ¨¡æ¿")

        published_templates = len(templates)
        summary = {
            "pack_id": pack_id,
            "detected_segments": detected_segments,
            "auto_detected_count": auto_detected_count,
            "published_templates": published_templates,
            "deduped_templates": deduped_templates,
            "detection_debug": detection_debug,
        }
        return templates, summary

    @staticmethod
    def _parse_transition_duration_ms(job: Dict[str, Any]) -> int:
        params = job.get("params") or {}
        metadata = params.get("metadata") if isinstance(params, dict) else {}
        duration_raw = None
        if isinstance(metadata, dict):
            duration_raw = metadata.get("transition_duration_ms")
        try:
            duration_ms = int(duration_raw) if duration_raw is not None else 1200
        except (TypeError, ValueError):
            duration_ms = 1200
        return max(200, min(duration_ms, 2000))

    async def _detect_transition_ranges(
        self,
        video_url: str,
        max_ranges: int,
        clip_ranges: List[Dict[str, Any]],
        transition_duration_ms: int,
    ) -> Tuple[List[Tuple[float, float]], Dict[str, Any]]:
        tmp_path = await self._ensure_local_video(video_url)
        total_duration_sec = await self._probe_video_duration(tmp_path)
        logger.info(f"[TemplateIngest] è§†é¢‘æ€»æ—¶é•¿: {total_duration_sec}s")

        detection_debug: Dict[str, Any] = {
            "duration_sec": round(total_duration_sec, 3),
            "transition_duration_ms": transition_duration_ms,
        }

        normalized_clip_ranges = self._normalize_clip_ranges(clip_ranges, total_duration_sec)
        if normalized_clip_ranges:
            selected_ranges = self._select_evenly_spaced_ranges(normalized_clip_ranges, max_ranges)
            logger.info(f"[TemplateIngest] ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ clip_ranges: {selected_ranges}")
            detection_debug.update({
                "ranges_source": "clip_ranges",
                "scene_event_count": 0,
                "selected_peak_count": 0,
                "selected_ranges": [
                    {"start": round(start, 3), "end": round(end, 3)} for start, end in selected_ranges
                ],
            })
            return selected_ranges, detection_debug

        # æä½é˜ˆå€¼æ”¶é›†æ‰€æœ‰åœºæ™¯å˜åŒ–ï¼Œè®©åç»­èšç±»æ¥åˆ†ç¦»å™ªå£°
        detect_cmd = [
            "ffmpeg",
            "-hide_banner",
            "-i",
            tmp_path,
            "-filter_complex",
            "select=gt(scene\,0.02),metadata=print",
            "-an",
            "-f",
            "null",
            "-",
        ]
        process = await asyncio.create_subprocess_exec(
            *detect_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        stdout, stderr = await process.communicate()
        if process.returncode != 0:
            logger.warning("[TemplateIngest] åœºæ™¯æ£€æµ‹å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç =%s", process.returncode)

        scene_output = (stdout or b"").decode(errors="ignore") + "\n" + (stderr or b"").decode(errors="ignore")
        scene_events = self._extract_scene_events(scene_output, total_duration_sec)
        detection_debug["scene_event_count"] = len(scene_events)
        detection_debug["top_scene_events"] = [
            {"ts": round(ts, 3), "score": round(score, 4)}
            for ts, score in sorted(scene_events, key=lambda item: item[1], reverse=True)[:20]
        ]

        # ========== åŠ¨æ€è½¬åœºåŒºåŸŸæ£€æµ‹ï¼ˆæ›¿ä»£å›ºå®šçª—å£ï¼‰ ==========
        min_zone_width = max(0.1, float(transition_duration_ms) / 4000.0)  # æœ€å°åŒºåŸŸå®½åº¦
        zones = self._cluster_into_transition_zones(
            scene_events=scene_events,
            total_duration_sec=total_duration_sec,
            min_zone_width_sec=min_zone_width,
        )
        logger.info("[TemplateIngest] åŠ¨æ€æ£€æµ‹åˆ° %s ä¸ªè½¬åœºåŒºåŸŸ: %s", len(zones), zones[:8])

        # ä»…è¿‡æ»¤æå¼±çš„å™ªå£°åŒºåŸŸï¼ˆpeak_score < 0.03ï¼‰ï¼Œä¸å†ç”¨ä¸­ä½æ•°è¿‡æ»¤
        # æ²»æœ¬ï¼šè®©æ‰€æœ‰çœŸå®è½¬åœºéƒ½æœ‰æœºä¼šä¿ç•™ï¼Œç”± max_cap æ§åˆ¶ä¸Šé™
        if zones:
            before_count = len(zones)
            zones = [z for z in zones if z["peak_score"] >= 0.03]
            logger.info("[TemplateIngest] å™ªå£°è¿‡æ»¤: %s -> %s ä¸ªåŒºåŸŸ", before_count, len(zones))

        # å°†åŒºåŸŸè½¬ä¸º ranges
        ranges: List[Tuple[float, float]] = [(z["start"], z["end"]) for z in zones]

        detection_debug.update({
            "ranges_source": "dynamic_zones" if ranges else "uniform_fallback",
            "zone_count": len(zones),
            "zones": zones[:16],
        })

        if not ranges:
            logger.info("[TemplateIngest] åœºæ™¯æ£€æµ‹æ— æœ‰æ•ˆåŒºåŸŸï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ")
            ranges = self._build_uniform_transition_ranges(total_duration_sec, max_ranges, transition_duration_ms)
            detection_debug["ranges_source"] = "uniform_fallback"

        # å¦‚æœåŒºåŸŸå¤ªå¤šï¼ŒæŒ‰ peak_score ç•™å‰ max_ranges ä¸ª
        if len(ranges) > max_ranges:
            scored = sorted(zip(zones, ranges), key=lambda x: x[0]["peak_score"], reverse=True)
            ranges = sorted([r for _, r in scored[:max_ranges]], key=lambda x: x[0])

        detection_debug["selected_ranges"] = [
            {"start": round(start, 3), "end": round(end, 3)} for start, end in ranges
        ]
        logger.info(f"[TemplateIngest] æœ€ç»ˆè½¬åœºèŒƒå›´: {ranges}")

        return ranges, detection_debug

    @staticmethod
    def _cluster_into_transition_zones(
        scene_events: List[Tuple[float, float]],
        total_duration_sec: float,
        gap_threshold_sec: float = 0.45,
        min_zone_width_sec: float = 0.12,
    ) -> List[Dict[str, Any]]:
        """
        å°†è¿ç»­çš„ scene events æŒ‰æ—¶é—´é—´éš”èšç±»ä¸ºè½¬åœºåŒºåŸŸã€‚
        æ¯ä¸ªåŒºåŸŸæœ‰è‡ªç„¶çš„èµ·æ­¢æ—¶é—´ï¼Œè€Œä¸æ˜¯å›ºå®šå®½åº¦çª—å£ã€‚

        åŸç†ï¼šä¸€ä¸ªè½¬åœºä¼šåœ¨çŸ­æ—¶é—´å†…äº§ç”Ÿä¸€ä¸²è¿ç»­çš„é«˜ scene score äº‹ä»¶ï¼Œ
        äº‹ä»¶ä¹‹é—´çš„æ—¶é—´é—´éš”å¾ˆå°ï¼ˆ<gap_threshold_secï¼‰ï¼Œ
        è€Œä¸¤ä¸ªä¸åŒè½¬åœºä¹‹é—´çš„é—´éš”åˆ™è¾ƒå¤§ã€‚

        è¿”å›:
            [{"start": 0.3, "end": 0.7, "peak_ts": 0.5, "peak_score": 0.22, "event_count": 5}, ...]
        """
        if not scene_events:
            return []

        sorted_events = sorted(scene_events, key=lambda x: x[0])

        zones: List[Dict[str, Any]] = []
        cluster: List[Tuple[float, float]] = [sorted_events[0]]

        for event in sorted_events[1:]:
            if event[0] - cluster[-1][0] <= gap_threshold_sec:
                cluster.append(event)
            else:
                zones.append(TemplateIngestService._finalize_transition_zone(
                    cluster, total_duration_sec, min_zone_width_sec,
                ))
                cluster = [event]

        if cluster:
            zones.append(TemplateIngestService._finalize_transition_zone(
                cluster, total_duration_sec, min_zone_width_sec,
            ))

        return zones

    @staticmethod
    def _finalize_transition_zone(
        cluster: List[Tuple[float, float]],
        total_duration_sec: float,
        min_zone_width_sec: float,
    ) -> Dict[str, Any]:
        """ å°†ä¸€ç±»äº‹ä»¶èšåˆä¸ºä¸€ä¸ªè½¬åœºåŒºåŸŸï¼Œç¡®ä¿æœ€å°å®½åº¦ """
        peak = max(cluster, key=lambda x: x[1])
        start = cluster[0][0]
        end = cluster[-1][0]
        if end - start < min_zone_width_sec:
            center = (start + end) / 2.0
            start = max(0.0, center - min_zone_width_sec / 2.0)
            end = min(total_duration_sec, center + min_zone_width_sec / 2.0)
        return {
            "start": round(start, 3),
            "end": round(end, 3),
            "peak_ts": round(peak[0], 3),
            "peak_score": round(peak[1], 4),
            "event_count": len(cluster),
        }

    @staticmethod
    def _extract_scene_events(
        scene_output: str,
        total_duration_sec: float,
    ) -> List[Tuple[float, float]]:
        events: List[Tuple[float, float]] = []
        pending_ts: Optional[float] = None

        for line in scene_output.splitlines():
            ts_match = re.search(r"pts_time:([0-9]+(?:\.[0-9]+)?)", line)
            if ts_match:
                try:
                    pending_ts = float(ts_match.group(1))
                except (TypeError, ValueError):
                    pending_ts = None
                continue

            score_match = re.search(r"lavfi\.scene_score=([0-9]+(?:\.[0-9]+)?)", line)
            if score_match and pending_ts is not None:
                try:
                    score = float(score_match.group(1))
                except (TypeError, ValueError):
                    pending_ts = None
                    continue
                if 0 < pending_ts <= total_duration_sec:
                    events.append((round(pending_ts, 3), score))
                pending_ts = None

        if not events:
            for match in re.finditer(r"pts_time:([0-9]+(?:\.[0-9]+)?)", scene_output):
                try:
                    ts = float(match.group(1))
                except (TypeError, ValueError):
                    continue
                if 0 < ts <= total_duration_sec:
                    events.append((round(ts, 3), 0.0))

        if not events:
            return []

        score_by_ts: Dict[float, float] = {}
        for ts, score in events:
            score_by_ts[ts] = max(score_by_ts.get(ts, 0.0), score)
        return sorted(score_by_ts.items(), key=lambda item: item[0])

    @staticmethod
    def _select_scene_peaks(
        scene_events: List[Tuple[float, float]],
        total_duration_sec: float,
        max_ranges: int,
    ) -> Tuple[List[Tuple[float, float]], float, float]:
        if not scene_events:
            return [], 0.0, 0.4

        min_peak_spacing_sec = max(0.35, min(0.75, total_duration_sec / 12.0))
        score_levels = [0.22, 0.16, 0.12, 0.08, 0.05, 0.0]
        target_min_peaks = 2 if total_duration_sec >= 3.0 else 1
        peak_cap = max(1, min(max_ranges * 2 if max_ranges > 0 else 32, 64))

        fallback_peaks: List[Tuple[float, float]] = []
        fallback_level = 0.0

        for level in score_levels:
            candidates = [item for item in scene_events if item[1] >= level]
            if not candidates:
                continue

            peaks = TemplateIngestService._collapse_scene_events(candidates, min_peak_spacing_sec)
            if len(peaks) > peak_cap:
                peaks = sorted(peaks, key=lambda item: item[1], reverse=True)[:peak_cap]
                peaks = sorted(peaks, key=lambda item: item[0])

            if len(peaks) > len(fallback_peaks):
                fallback_peaks = peaks
                fallback_level = level

            if len(peaks) >= target_min_peaks:
                return peaks, level, min_peak_spacing_sec

        return fallback_peaks, fallback_level, min_peak_spacing_sec

    @staticmethod
    def _collapse_scene_events(
        scene_events: List[Tuple[float, float]],
        min_spacing_sec: float,
    ) -> List[Tuple[float, float]]:
        if not scene_events:
            return []

        sorted_events = sorted(scene_events, key=lambda item: item[0])
        peaks: List[Tuple[float, float]] = []
        cluster: List[Tuple[float, float]] = [sorted_events[0]]

        for event in sorted_events[1:]:
            prev_ts = cluster[-1][0]
            if event[0] - prev_ts <= min_spacing_sec:
                cluster.append(event)
                continue

            peaks.append(max(cluster, key=lambda item: item[1]))
            cluster = [event]

        if cluster:
            peaks.append(max(cluster, key=lambda item: item[1]))

        return sorted(peaks, key=lambda item: item[0])

    @staticmethod
    def _build_ranges_from_peaks(
        peaks: List[Tuple[float, float]],
        total_duration_sec: float,
        transition_duration_ms: int,
    ) -> List[Tuple[float, float]]:
        ranges: List[Tuple[float, float]] = []
        for ts, _score in peaks:
            transition_range = TemplateIngestService._build_transition_range(
                center_ts=ts,
                total_duration_sec=total_duration_sec,
                transition_duration_ms=transition_duration_ms,
            )
            if transition_range is not None:
                ranges.append(transition_range)
        return ranges

    @staticmethod
    def _build_transition_range(
        center_ts: float,
        total_duration_sec: float,
        transition_duration_ms: int,
    ) -> Optional[Tuple[float, float]]:
        half_window = max(0.1, min(float(transition_duration_ms) / 2000.0, 1.0))
        start = max(0.0, center_ts - half_window)
        end = min(total_duration_sec, center_ts + half_window)
        if end - start < 0.2:
            return None
        return (start, end)

    @staticmethod
    def _build_uniform_transition_ranges(
        duration_sec: float,
        max_ranges: int,
        transition_duration_ms: int,
    ) -> List[Tuple[float, float]]:
        if duration_sec <= 0:
            return []
        count = max(1, min(max_ranges, 8))
        half_window = max(0.1, min(float(transition_duration_ms) / 2000.0, 1.0))
        ranges: List[Tuple[float, float]] = []
        step = duration_sec / (count + 1)
        for i in range(count):
            center = step * (i + 1)
            start = max(0.0, center - half_window)
            end = min(duration_sec, center + half_window)
            if end - start >= 0.2:
                ranges.append((start, end))
        return ranges

    @staticmethod
    def _dedupe_transition_ranges(ranges: List[Tuple[float, float]]) -> List[Tuple[float, float]]:
        if not ranges:
            return []
        sorted_ranges = sorted(ranges, key=lambda item: item[0])
        kept: List[Tuple[float, float]] = []
        for current_start, current_end in sorted_ranges:
            if current_end <= current_start:
                continue
            if not kept:
                kept.append((current_start, current_end))
                continue

            prev_start, prev_end = kept[-1]
            overlap = min(prev_end, current_end) - max(prev_start, current_start)
            if overlap <= 0:
                kept.append((current_start, current_end))
                continue

            prev_span = prev_end - prev_start
            curr_span = current_end - current_start
            ratio = overlap / max(min(prev_span, curr_span), 1e-6)
            center_gap = abs(((prev_start + prev_end) / 2) - ((current_start + current_end) / 2))

            # æ˜æ˜¾é‡å¤çš„è½¬åœºçª—å£ç›´æ¥è·³è¿‡
            if ratio >= 0.5 or center_gap <= 0.08:
                continue
            kept.append((current_start, current_end))

        return kept

    @staticmethod
    def _select_evenly_spaced_ranges(
        ranges: List[Tuple[float, float]],
        limit: int,
    ) -> List[Tuple[float, float]]:
        if limit <= 0 or not ranges:
            return []
        if len(ranges) <= limit:
            return sorted(ranges, key=lambda item: item[0])

        ordered = sorted(ranges, key=lambda item: item[0])
        selected: List[Tuple[float, float]] = []
        total = len(ordered)
        for i in range(limit):
            idx = int(round((total - 1) * (i / max(limit - 1, 1))))
            selected.append(ordered[idx])
        return selected

    def _build_transition_spec(
        self,
        start_sec: float,
        end_sec: float,
        index: int,
        job: Dict[str, Any],
        analysis: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        duration_ms = int(round(max(200.0, min((end_sec - start_sec) * 1000.0, 2000.0))))

        # â”€â”€ ä» LLM åˆ†æç»“æœä¸­æå–åˆ†ç±» â”€â”€
        analysis = analysis or {}
        family = str(analysis.get("transition_type") or "whip_pan").strip().lower()
        valid_families = {"whip_pan", "zoom_blur", "flash_cut", "glitch", "spin",
                         "luma_wipe", "dolly_zoom", "morph", "occlusion"}
        if family not in valid_families:
            family = "whip_pan"

        # å…è®¸ tag è¦†ç›–ï¼ˆå‘åå…¼å®¹ï¼‰
        tags = [str(tag).lower() for tag in (job.get("tags_hint") or [])]
        if any("é—ª" in tag or "flash" in tag for tag in tags):
            family = "flash_cut"
        elif any("glitch" in tag or "æ•…éšœ" in tag for tag in tags):
            family = "glitch"
        elif any("ç¼©æ”¾" in tag or "zoom" in tag for tag in tags):
            family = "zoom_blur"

        return {
            "version": "v2",
            "family": family,
            "duration_ms": duration_ms,
            "fps": 30,
            # â”€â”€ LLM åˆ†æç»“æœï¼ˆæ ¸å¿ƒæ–°å¢ï¼‰ â”€â”€
            "transition_category": analysis.get("transition_category") or "unknown",
            "transition_description": analysis.get("transition_description") or "",
            "motion_pattern": analysis.get("motion_pattern") or "",
            "camera_movement": analysis.get("camera_movement") or "",
            "scene_a_description": analysis.get("scene_a_description") or "",
            "scene_b_description": analysis.get("scene_b_description") or "",
            "recommended_prompt": analysis.get("recommended_prompt") or "",
            "motion_prompt": analysis.get("motion_prompt") or "",
            # â”€â”€ å¤šå±‚è¿åŠ¨åˆ†æï¼ˆè§†é¢‘ç†è§£å¢å¼ºï¼‰ â”€â”€
            "camera_compound": analysis.get("camera_compound") or "",
            "background_motion": analysis.get("background_motion") or "",
            "subject_motion": analysis.get("subject_motion") or "",
            "transition_duration_sec": analysis.get("transition_duration_sec") or 0.0,
            "_analysis_method": analysis.get("_analysis_method") or "unknown",
            # â”€â”€ å¤šç»´åº¦è¯„åˆ† â”€â”€
            "dimension_scores": analysis.get("dimension_scores") or {
                "outfit_change": 0.0, "subject_preserve": 0.0, "scene_shift": 0.0,
            },
            "recommended_focus_modes": analysis.get("recommended_focus_modes") or [],
            # â”€â”€ 5ç»´æŠ€æœ¯è§£å‰–ï¼ˆv3: è§£å‰–é©±åŠ¨åˆ†ç±»ï¼‰ â”€â”€
            "technical_dissection": analysis.get("technical_dissection") or {},
            # â”€â”€ æ¨¡å‹è‡ªæŠ¥çš„è½¬åœºç²¾ç¡®æ—¶é—´çª—å£ â”€â”€
            "transition_window": analysis.get("transition_window") or {},
            "quality_tier": "template_match",
        }

    async def _analyze_transition_frames(
        self,
        frame_a: Optional[Image.Image],
        frame_mid: Optional[Image.Image],
        frame_b: Optional[Image.Image],
        index: int,
        video_url: Optional[str] = None,
        start_sec: Optional[float] = None,
        end_sec: Optional[float] = None,
    ) -> Dict[str, Any]:
        """
        ç”¨ LLM è§†è§‰åˆ†æè½¬åœºç±»å‹ï¼ˆä»…è§†é¢‘ç†è§£ï¼Œæ— å¸§åˆ†æé™çº§ï¼‰ã€‚
        æµç¨‹ï¼šæå–è½¬åœºè§†é¢‘ç‰‡æ®µ â†’ ä¸Šä¼  Ark â†’ Responses API è§†é¢‘ç†è§£
        ä»»ä½•æ­¥éª¤å¤±è´¥ç›´æ¥æŠ›å¼‚å¸¸ï¼Œæš´éœ²é”™è¯¯ä»¥ä¾¿ä¿®å¤ã€‚
        """
        default_result: Dict[str, Any] = {
            "transition_category": "unknown",
            "transition_type": "whip_pan",
            "transition_description": "",
            "motion_pattern": "",
            "camera_movement": "static",
            "scene_a_description": "",
            "scene_b_description": "",
            "recommended_prompt": "",
            "motion_prompt": "",
            # â”€â”€ å¤šç»´åº¦è¯„åˆ†ï¼šæ¯ä¸ªè½¬åœºåŒæ—¶æ¶‰åŠçš„ç»´åº¦ (0.0-1.0) â”€â”€
            "dimension_scores": {
                "outfit_change": 0.0,
                "subject_preserve": 0.0,
                "scene_shift": 0.0,
            },
            "recommended_focus_modes": [],
            # â”€â”€ å¤šå±‚è¿åŠ¨åˆ†æï¼ˆè§†é¢‘ç†è§£å¢å¼ºï¼‰ â”€â”€
            "background_motion": "",
            "subject_motion": "",
            "camera_compound": "",
            "transition_duration_sec": 0.0,
        }

        # è®¡ç®—è½¬åœºæ—¶é•¿ï¼ˆç”¨äº prompt å’Œ fps å†³ç­–ï¼‰
        transition_duration = 0.0
        if start_sec is not None and end_sec is not None:
            transition_duration = round(end_sec - start_sec, 3)
            default_result["transition_duration_sec"] = transition_duration

        frames_for_analysis = [f for f in [frame_a, frame_mid, frame_b] if f is not None]
        if not frames_for_analysis:
            return default_result

        from app.utils.image_utils import pil_image_to_base64

        # â”€â”€ å‡†å¤‡ A/B è¾¹ç•Œå¸§çš„ base64 â”€â”€
        boundary_images_b64 = []
        if frame_a is not None:
            boundary_images_b64.append(pil_image_to_base64(frame_a, format="JPEG"))
        if frame_b is not None:
            boundary_images_b64.append(pil_image_to_base64(frame_b, format="JPEG"))

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        #  è§†é¢‘ç‰‡æ®µç†è§£ï¼ˆä¸é™çº§åˆ°å¸§åˆ†æï¼Œæš´éœ²é”™è¯¯ä»¥ä¾¿ä¿®å¤ï¼‰
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if not video_url or start_sec is None or end_sec is None:
            raise RuntimeError(
                f"è½¬åœº {index}: è§†é¢‘æ¨¡æ¿åˆ†æå¿…é¡»æä¾› video_url/start_sec/end_secï¼Œ"
                "ä¸å†æ”¯æŒå¸§åˆ†æé™çº§"
            )

        # â”€â”€ æ…·æ…¨ paddingï¼šä¸çŒœé˜ˆå€¼ï¼Œç»™æ¨¡å‹è¶³å¤Ÿä¸Šä¸‹æ–‡è®©å®ƒè‡ªå·±åˆ¤æ–­ â”€â”€
        # å›ºå®š 2s paddingï¼šç¡®ä¿ä»»ä½•ç±»å‹çš„è½¬åœºï¼ˆç¬åˆ‡/ç”©é•œ/å¸ŒåŒºæŸ¯å…‹å˜ç„¦/èºæ—‹ï¼‰
        # çš„å»ºç«‹â†’é«˜æ½®â†’æ¶ˆæ•£è¿‡ç¨‹éƒ½è¢«å®Œæ•´æ•è·ã€‚æ¨¡å‹ä¼šåœ¨è¾“å‡ºä¸­è‡ªè¡ŒæŠ¥å‘Šç²¾ç¡®è¾¹ç•Œã€‚
        CLIP_PADDING = 2.0
        logger.info(
            "[TemplateIngest] è½¬åœº %d: è§†é¢‘ç†è§£ (zone=%.2fs-%.2fs, duration=%.2fs, padding=%.1fs)",
            index, start_sec, end_sec, transition_duration, CLIP_PADDING,
        )
        clip_path = await self._extract_transition_clip(
            video_url=video_url,
            start_sec=start_sec,
            end_sec=end_sec,
            padding=CLIP_PADDING,
        )
        try:
            file_id = await self._upload_clip_to_ark(clip_path)

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            #  ä¸¤é˜¶æ®µåˆ†æï¼šæŠŠã€Œçœ‹ã€å’Œã€Œåˆ¤æ–­ã€æ‹†å¼€
            #  Stage 1: è§†é¢‘ â†’ çº¯è§‚å¯Ÿç¬”è®°ï¼ˆåªæè¿°çœ‹åˆ°äº†ä»€ä¹ˆï¼Œä¸åˆ†ç±»ï¼‰
            #  Stage 2: è§‚å¯Ÿç¬”è®° â†’ åˆ†ç±» + motion_promptï¼ˆçº¯æ–‡æœ¬æ¨ç†ï¼‰
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            # â”€â”€ Stage 1: çº¯è§†è§‰è§‚å¯Ÿ â”€â”€
            stage1_system = self._build_stage1_observation_prompt()
            boundary_desc = ""
            if frame_a is not None and frame_b is not None:
                boundary_desc = "åŒæ—¶æä¾›äº†è½¬åœºå‰ï¼ˆAå¸§ï¼‰å’Œè½¬åœºåï¼ˆBå¸§ï¼‰çš„æ¸…æ™°é™æ€æˆªå›¾ä½œä¸ºå‚è€ƒã€‚"
            elif frame_a is not None:
                boundary_desc = "åŒæ—¶æä¾›äº†è½¬åœºå‰ï¼ˆAå¸§ï¼‰çš„æ¸…æ™°é™æ€æˆªå›¾ä½œä¸ºå‚è€ƒã€‚"
            elif frame_b is not None:
                boundary_desc = "åŒæ—¶æä¾›äº†è½¬åœºåï¼ˆBå¸§ï¼‰çš„æ¸…æ™°é™æ€æˆªå›¾ä½œä¸ºå‚è€ƒã€‚"

            stage1_user = (
                "è¿™æ˜¯ä¸€æ®µåŒ…å«è½¬åœºç‰¹æ•ˆçš„è§†é¢‘ç‰‡æ®µï¼Œå‰åå„æœ‰çº¦2ç§’é™æ€ä¸Šä¸‹æ–‡ã€‚"
                f"{boundary_desc}\n"
                "è¯·æŒ‰5ä¸ªç»´åº¦ï¼ˆSubject Anchoring / Trigger Mechanism / "
                "Spatial Perspective / Asset Replacement / Motion Dynamicsï¼‰"
                "é€ä¸€è§‚å¯Ÿï¼Œè®°å½•ä½ çœ‹åˆ°çš„ç‰©ç†ç°è±¡ã€‚\n"
                "âš ï¸ åªæè¿°ä½ çœ‹åˆ°äº†ä»€ä¹ˆï¼Œä¸è¦ç»™å‡ºä»»ä½•åˆ†ç±»åç§°ã€‚"
            )

            logger.info(
                "[TemplateIngest] è½¬åœº %d: ğŸ”¬ é˜¶æ®µä¸€ï¼ˆçº¯è§‚å¯Ÿï¼‰å¼€å§‹",
                index,
            )
            stage1_result = await self._call_llm_with_video(
                file_id=file_id,
                system_prompt=stage1_system,
                user_prompt=stage1_user,
                boundary_images_b64=boundary_images_b64 if boundary_images_b64 else None,
            )

            # æå–è§‚å¯Ÿæ–‡æœ¬ â€” å…¼å®¹ dict å’Œ str
            import json as _json
            if isinstance(stage1_result, dict):
                observation = stage1_result.get("observation", {})
                observation_text = _json.dumps(stage1_result, ensure_ascii=False, indent=2)
            else:
                observation = {}
                observation_text = str(stage1_result)

            logger.info(
                "[TemplateIngest] è½¬åœº %d: ğŸ”¬ é˜¶æ®µä¸€å®Œæˆ | è§‚å¯Ÿæ‘˜è¦: %s",
                index, observation_text[:300],
            )

            # â”€â”€ é˜¶æ®µé—´å†·å´ï¼šé¿å…è¿ç»­è¯·æ±‚è§¦å‘ Ark 429 é™æµ â”€â”€
            await asyncio.sleep(3)

            # â”€â”€ Stage 2: çº¯æ–‡æœ¬æ¨ç†ï¼ˆä¸å†å‘é€è§†é¢‘ï¼‰ â”€â”€
            stage2_system = self._build_stage2_reasoning_prompt()

            # æŠŠ stage1 çš„åœºæ™¯æè¿°ä¹Ÿä¼ ç»™ stage2
            scene_a = stage1_result.get("scene_a_description", "") if isinstance(stage1_result, dict) else ""
            scene_b = stage1_result.get("scene_b_description", "") if isinstance(stage1_result, dict) else ""
            tw_stage1 = stage1_result.get("transition_window", {}) if isinstance(stage1_result, dict) else {}

            stage2_user = (
                "ä»¥ä¸‹æ˜¯å¦ä¸€ä½åˆ†æå¸ˆè§‚çœ‹è§†é¢‘åå†™ä¸‹çš„çº¯å®¢è§‚è§‚å¯Ÿè®°å½•ã€‚\n"
                "è¯· 100%% åŸºäºè¿™äº›è§‚å¯Ÿæ¥æ¨å¯¼åˆ†ç±»å’Œç¼–èˆè“å›¾ã€‚\n\n"
                f"## è§‚å¯Ÿè®°å½•\n{observation_text}\n\n"
                f"## åœºæ™¯ä¿¡æ¯\n"
                f"- è½¬åœºå‰ (Scene A): {scene_a}\n"
                f"- è½¬åœºå (Scene B): {scene_b}\n\n"
                f"## è½¬åœºçª—å£\n"
                f"- effect_start_sec: {tw_stage1.get('effect_start_sec', 'æœªçŸ¥')}\n"
                f"- effect_end_sec: {tw_stage1.get('effect_end_sec', 'æœªçŸ¥')}\n"
                f"- effect_duration_sec: {tw_stage1.get('effect_duration_sec', 'æœªçŸ¥')}\n\n"
                "è¯·è¾“å‡ºå®Œæ•´ JSONã€‚"
            )

            logger.info(
                "[TemplateIngest] è½¬åœº %d: ğŸ§  é˜¶æ®µäºŒï¼ˆæ–‡æœ¬æ¨ç†ï¼‰å¼€å§‹",
                index,
            )
            video_result = await self._call_llm_text_only(
                system_prompt=stage2_system,
                user_prompt=stage2_user,
            )

            logger.info(
                "[TemplateIngest] è½¬åœº %d: ğŸ§  é˜¶æ®µäºŒå®Œæˆ | type=%s",
                index,
                video_result.get("transition_type", "?") if isinstance(video_result, dict) else "?",
            )

            # åˆå¹¶ Stage 1 çš„ä¿¡æ¯åˆ°æœ€ç»ˆç»“æœ
            if isinstance(video_result, dict):
                # scene descriptions ä¼˜å…ˆç”¨ Stage 1 çš„ï¼ˆç›´æ¥çœ‹è§†é¢‘çš„æ›´å‡†ï¼‰
                if scene_a and not video_result.get("scene_a_description"):
                    video_result["scene_a_description"] = scene_a
                if scene_b and not video_result.get("scene_b_description"):
                    video_result["scene_b_description"] = scene_b
                # transition_window ä¼˜å…ˆç”¨ Stage 1 çš„ï¼ˆç›´æ¥çœ‹è§†é¢‘çš„æ›´å‡†ï¼‰
                if tw_stage1 and not video_result.get("transition_window"):
                    video_result["transition_window"] = tw_stage1
                # ä¿å­˜åŸå§‹è§‚å¯Ÿè®°å½•ç”¨äºè°ƒè¯•
                video_result["_stage1_observation"] = observation

            # å¼‚æ­¥æ¸…ç†ä¸Šä¼ çš„æ–‡ä»¶
            asyncio.create_task(self._cleanup_ark_file(file_id))
        finally:
            # æ— è®ºæˆåŠŸå¤±è´¥éƒ½æ¸…ç†æœ¬åœ°ä¸´æ—¶ç‰‡æ®µ
            try:
                os.remove(clip_path)
            except Exception:
                pass

        if not isinstance(video_result, dict):
            raise RuntimeError(
                f"è½¬åœº {index}: è§†é¢‘ç†è§£ LLM è¿”å›é dict ç»“æœ: {type(video_result)}"
            )

        video_result["_analysis_method"] = "video_clip"
        video_result["transition_duration_sec"] = transition_duration

        # â”€â”€ åå¤„ç†ï¼šæ¸…æ´— motion_prompt ä¸­çš„å†…å®¹æ³„æ¼ â”€â”€
        video_result = self._sanitize_analysis_content_leak(video_result)

        for key in default_result:
            if key not in video_result or not video_result[key]:
                video_result[key] = default_result[key]

        # è®°å½•æ¨¡å‹è‡ªæŠ¥çš„è½¬åœºçª—å£ï¼ˆæ ¸å¿ƒ debug ä¿¡æ¯ï¼‰
        tw = video_result.get("transition_window") or {}
        logger.info(
            "[TemplateIngest] è½¬åœº %d: âœ… è§†é¢‘ç†è§£æˆåŠŸ | type=%s | camera=%s | "
            "model_window=[%.2fs-%.2fs](%.2fs, conf=%.2f)",
            index,
            video_result.get("transition_type", "?"),
            video_result.get("camera_movement", "?"),
            float(tw.get("effect_start_sec", 0)),
            float(tw.get("effect_end_sec", 0)),
            float(tw.get("effect_duration_sec", 0)),
            float(tw.get("confidence", 0)),
        )
        return video_result

    # â”€â”€ åå¤„ç†ï¼šæ¸…æ´—å†…å®¹æ³„æ¼ â”€â”€

    @staticmethod
    def _sanitize_analysis_content_leak(result: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´— LLM åˆ†æç»“æœä¸­çš„å†…å®¹æ³„æ¼ã€‚

        motion_prompt / background_motion / subject_motion æ˜¯çº¯æœºæ¢°è“å›¾ï¼Œ
        ä¸åº”åŒ…å«å…·ä½“åœºæ™¯/æœè£…/äººç‰©æè¿°è¯æ±‡ã€‚
        ç”¨æ³›åŒ–æ›¿æ¢è¯æ›¿ä»£ï¼Œä¿ç•™åŠ›å­¦æè¿°ã€‚
        """
        import re

        # éœ€è¦æ¸…æ´—çš„å­—æ®µï¼ˆçº¯åŠ›å­¦ï¼Œä¸åº”æœ‰å†…å®¹è¯æ±‡ï¼‰
        fields_to_sanitize = ["motion_prompt", "background_motion", "subject_motion"]

        # æ›¿æ¢è§„åˆ™ï¼š(pattern, replacement)
        # é¡ºåºå¾ˆé‡è¦ï¼šå…ˆæ›¿æ¢å¤šè¯çŸ­è¯­ï¼Œå†æ›¿æ¢å•è¯
        replacements = [
            # åœºæ™¯ç±»å‹ â†’ the background / the scene
            # å…ˆåŒ¹é…é•¿çŸ­è¯­ï¼ŒååŒ¹é…å•è¯
            (r'\bopen\s+outdoor\s+space\b', 'the new background'),
            (r'\bstatic\s+indoor\s+scene\b', 'static initial background'),
            (r'\bindoor\s+scene\b', 'initial background'),
            (r'\boutdoor\s+scene\b', 'new background'),
            (r'\bindoor\s+(?:space|room|environment|setting)\b', 'initial background'),
            (r'\boutdoor\s+(?:space|area|environment|setting)\b', 'new background'),
            (r'\bfrom\s+indoor\s+to\s+outdoor\b', 'from initial scene to new scene'),
            (r'\bfrom\s+outdoor\s+to\s+indoor\b', 'from initial scene to new scene'),
            (r'\bindoor\b', 'initial'),
            (r'\boutdoor\b', 'new'),
            (r'\b(?:living\s+)?room\b', 'background'),
            (r'\bpark\b', 'background'),
            (r'\bstreet\b', 'background'),
            (r'\boffice\b', 'background'),
            (r'\bbedroom\b', 'background'),
            (r'\bkitchen\b', 'background'),
            # äººç‰©æè¿° â†’ the subject
            (r'\b(?:girl|woman|man|boy|lady|guy|person)\b', 'the subject'),
            # æœè£…å…·ä½“æè¿° â†’ outfit
            (r'\b(?:black|white|red|blue|green|pink|yellow)\s+(?:coat|dress|shirt|jacket|top|pants|skirt)\b', 'outfit'),
            (r'\b(?:coat|dress|shirt|jacket|sweater|hoodie|blouse)\b(?!\s+(?:transformation|swap|change|replacement))', 'outfit'),
        ]

        for field in fields_to_sanitize:
            val = result.get(field)
            if not val or not isinstance(val, str):
                continue
            original = val
            for pattern, repl in replacements:
                val = re.sub(pattern, repl, val, flags=re.IGNORECASE)
            if val != original:
                result[field] = val
                logger.info(
                    "[TemplateIngest] ğŸ§¹ å†…å®¹æ³„æ¼æ¸…æ´— [%s]: '%s' â†’ '%s'",
                    field, original[:80], val[:80],
                )

        return result

    # â”€â”€ ç³»ç»Ÿ Prompt æ„å»ºï¼šä¸¤é˜¶æ®µï¼ˆè§‚å¯Ÿ + æ¨ç†ï¼‰ â”€â”€

    @staticmethod
    def _build_stage1_observation_prompt() -> str:
        """é˜¶æ®µä¸€ï¼šçº¯è§†è§‰è§‚å¯Ÿ promptã€‚

        è®¾è®¡åŸåˆ™ï¼š
        - åªåšã€Œçœ‹åˆ°äº†ä»€ä¹ˆã€çš„å®¢è§‚æè¿°ï¼Œ**ç»å¯¹ä¸åšåˆ†ç±»/å‘½å/åˆ¤æ–­**
        - 5 ä¸ªæŠ€æœ¯ç»´åº¦é€å¸§è§‚å¯Ÿï¼Œè¾“å‡ºç»“æ„åŒ–çš„è§‚å¯Ÿç¬”è®°
        - æ¨¡å‹è¢«è¿«å…ˆå†™ä¸‹å…·ä½“è§‚å¯Ÿï¼ˆå¦‚"ä¸»ä½“å¤§å°ä¸å˜ã€èƒŒæ™¯çºµæ·±æ‹‰ä¼¸"ï¼‰ï¼Œ
          è¿™äº›è§‚å¯Ÿä¸€æ—¦å†™ä¸‹å°±æˆä¸º"å·²æ‰¿è¯ºäº‹å®"ï¼Œåç»­æ¨ç†é˜¶æ®µæ— æ³•æ¨ç¿»
        """
        return (
            "ä½ æ˜¯ä¸€ä¸ªç²¾å¯†çš„è§†é¢‘åˆ†æä»ªå™¨ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯**é€å¸§è§‚å¯Ÿå¹¶è®°å½•äº‹å®**ã€‚\n\n"

            "## ç»å¯¹ç¦æ­¢\n"
            "âŒ ä¸è¦ç»™å‡ºä»»ä½•åˆ†ç±»åç§°ï¼ˆå¦‚ whip_panã€dolly_zoomã€spin ç­‰ï¼‰\n"
            "âŒ ä¸è¦åšä»»ä½•åˆ¤æ–­æˆ–æ¨ç†\n"
            "âŒ ä¸è¦å†™ motion_prompt\n"
            "âŒ ä¸è¦å†™ recommended_prompt\n"
            "ä½ åªæ˜¯ä¸€å°æ‘„åƒæœºå›æ”¾åˆ†æä»ªï¼Œåªè¾“å‡ºä½ **çœ‹åˆ°çš„ç‰©ç†ç°è±¡**ã€‚\n\n"

            "## è§†é¢‘ä¸Šä¸‹æ–‡\n"
            "è§†é¢‘ç‰‡æ®µåœ¨è½¬åœºå‰åå„ç•™äº†çº¦ 2 ç§’é™æ€ç”»é¢ã€‚\n"
            "è¯·åˆ¤æ–­è½¬åœºç‰¹æ•ˆçš„ç²¾ç¡®èµ·æ­¢ç§’æ•°ã€‚\n\n"

            "## è§‚å¯Ÿç»´åº¦ï¼ˆé€ä¸€å¡«å†™ï¼Œæ¯ä¸ªç»´åº¦ 2-4 å¥è‹±æ–‡ï¼‰\n\n"

            "### 1. Subject Anchoringï¼ˆä¸»ä½“é”šå®šï¼‰\n"
            "è§‚å¯Ÿå¹¶è®°å½•ï¼š\n"
            "- è½¬åœºå‰ä¸»ä½“ï¼ˆäººç‰©å¤´éƒ¨/äº”å®˜ï¼‰åœ¨ç”»é¢ä¸­çš„ä½ç½®åæ ‡ï¼ˆå¦‚ï¼šä¸­å¿ƒåå·¦ã€å ç”»é¢ 40%%ï¼‰\n"
            "- è½¬åœºåä¸»ä½“çš„ä½ç½®åæ ‡å’Œå ç”»é¢æ¯”ä¾‹\n"
            "- ä¸»ä½“ä½ç½®/æ¯”ä¾‹æ˜¯å¦å‘ç”Ÿäº†å˜åŒ–ï¼Ÿå˜åŒ–äº†å¤šå°‘ï¼Ÿ\n"
            "- ä¸»ä½“æ˜¯å¦å§‹ç»ˆä¿æŒæ¸…æ™°ï¼Ÿè¿˜æ˜¯æœ‰ä¸€æ®µæ¨¡ç³ŠæœŸï¼Ÿ\n\n"

            "### 2. Trigger Mechanismï¼ˆè§¦å‘æœºåˆ¶ï¼‰\n"
            "è§‚å¯Ÿå¹¶è®°å½•ï¼š\n"
            "- è½¬åœºå¼€å§‹æ—¶ç”»é¢å‘ç”Ÿäº†ä»€ä¹ˆç‰©ç†å˜åŒ–ï¼Ÿï¼ˆå¦‚ï¼šç”»é¢å¼€å§‹æ¨¡ç³Šã€æœ‰ç‰©ä½“ä»å·¦ä¾§åˆ’å…¥ã€é—ªç™½ï¼‰\n"
            "- æ˜¯å¦æœ‰ä»»ä½•ç‰©ä½“/æ‰‹/å¤´å‘ç­‰é®æŒ¡äº†ç”»é¢ï¼Ÿé®æŒ¡äº†ç™¾åˆ†ä¹‹å‡ ï¼Ÿ\n"
            "- åœºæ™¯ A åˆ°åœºæ™¯ B çš„åˆ‡æ¢æ˜¯åœ¨å“ªä¸ªæ—¶åˆ»å®Œæˆçš„ï¼Ÿ\n"
            "- åˆ‡æ¢æ˜¯ç¬é—´å®Œæˆè¿˜æ˜¯æœ‰è¿‡æ¸¡æœŸï¼Ÿè¿‡æ¸¡æœŸå¤šé•¿ï¼Ÿ\n\n"

            "### 3. Spatial Perspectiveï¼ˆç©ºé—´é€è§†ï¼‰\n"
            "è¿™æ˜¯æœ€å…³é”®çš„ç»´åº¦ï¼Œè¯·ç‰¹åˆ«ä»”ç»†è§‚å¯Ÿï¼š\n"
            "- è½¬åœºå‰èƒŒæ™¯çš„çºµæ·±æ„Ÿå¦‚ä½•ï¼Ÿï¼ˆå¦‚ï¼šèƒŒæ™¯è¾ƒè¿‘/èƒŒæ™¯æœ‰çºµæ·±èµ°å»Šæ„Ÿ/èƒŒæ™¯æ¨¡ç³Šï¼‰\n"
            "- è½¬åœºåèƒŒæ™¯çš„çºµæ·±æ„Ÿå¦‚ä½•ï¼Ÿ\n"
            "- èƒŒæ™¯åœ¨è½¬åœºè¿‡ç¨‹ä¸­æ˜¯å¦å‡ºç°äº†**æ‹‰ä¼¸æˆ–å‹ç¼©**æ•ˆæœï¼Ÿ\n"
            "  ï¼ˆå³èƒŒæ™¯ç‰©ä½“ä¹‹é—´çš„è·ç¦»æ„Ÿæ˜¯å¦åœ¨å˜åŒ–ï¼Ÿï¼‰\n"
            "- ä¸»ä½“å¤§å°ä¸å˜ä½†èƒŒæ™¯çºµæ·±å‰§å˜ï¼Ÿè¿˜æ˜¯å…¨ç”»é¢ä¸€èµ·ç¼©æ”¾ï¼Ÿè¿˜æ˜¯å…¨ç”»é¢ä¸€èµ·å¹³ç§»ï¼Ÿ\n"
            "- èƒŒæ™¯ç­ç‚¹ï¼ˆæ¶ˆå¤±ç‚¹ï¼‰æ˜¯å¦å‘ç”Ÿäº†ä½ç§»ï¼Ÿå‘å“ªä¸ªæ–¹å‘ï¼Ÿ\n\n"

            "### 4. Asset Replacementï¼ˆèµ„äº§æ›¿æ¢ï¼‰\n"
            "è§‚å¯Ÿå¹¶è®°å½•ï¼š\n"
            "- è½¬åœºå‰åå…‰çº¿æœ‰ä»€ä¹ˆå˜åŒ–ï¼Ÿï¼ˆè‰²æ¸©ã€äº®åº¦ã€æ–¹å‘ï¼‰\n"
            "- è½¬åœºå‰åæœè£…/æè´¨æ˜¯å¦ä¸åŒï¼Ÿ\n"
            "- æ›¿æ¢å‘ç”Ÿåœ¨å“ªä¸€å¸§ï¼Ÿæ˜¯ç¡¬åˆ‡è¿˜æ˜¯èåˆï¼Ÿ\n"
            "- è½¬åœºå‰åç¯å¢ƒ/èƒŒæ™¯æ˜¯å¦ä¸åŒï¼Ÿ\n\n"

            "### 5. Motion Dynamicsï¼ˆè¿åŠ¨åŠ›å­¦ï¼‰\n"
            "è§‚å¯Ÿå¹¶è®°å½•ï¼š\n"
            "- ç”»é¢ä¸­è¿åŠ¨æ¨¡ç³Šçš„æ–¹å‘ï¼šæ°´å¹³å‘å·¦/å³ï¼Ÿä»ä¸­å¿ƒå‘å¤–è¾å°„ï¼Ÿæ—‹è½¬ï¼Ÿæ— ç»Ÿä¸€æ–¹å‘ï¼Ÿ\n"
            "- å‰æ™¯ï¼ˆä¸»ä½“ï¼‰å’ŒèƒŒæ™¯çš„è¿åŠ¨æ˜¯å¦ä¸€è‡´ï¼Ÿ\n"
            "  Â· å¦‚æœä¸€è‡´ï¼šå…¨ç”»é¢ä¸€èµ·å‘æŸæ–¹å‘è¿åŠ¨\n"
            "  Â· å¦‚æœä¸ä¸€è‡´ï¼šå…·ä½“æè¿°å„è‡ªæ€ä¹ˆåŠ¨ï¼ˆå¦‚ 'ä¸»ä½“ä¸åŠ¨ï¼ŒèƒŒæ™¯å‘åæ‹‰ä¼¸'ï¼‰\n"
            "- è¿åŠ¨é€Ÿåº¦æ›²çº¿ï¼šåŒ€é€Ÿï¼Ÿå…ˆæ…¢åå¿«ï¼Ÿå…ˆå¿«åæ…¢ï¼Ÿçªç„¶åŠ é€Ÿï¼Ÿ\n"
            "- æ¨¡ç³Šå³°å€¼å‡ºç°åœ¨è½¬åœºçš„å¤§çº¦ç™¾åˆ†ä¹‹å‡ å¤„ï¼Ÿ\n\n"

            "## è¾“å‡º JSONï¼ˆä¸¥æ ¼ï¼Œä¸åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼‰\n"
            "{\n"
            '  "observation": {\n'
            '    "subject_anchoring": "2-4 sentences in English, pure observation, NO classification terms",\n'
            '    "trigger_mechanism": "2-4 sentences in English, pure observation",\n'
            '    "spatial_perspective": "3-5 sentences in English, VERY detailed, this is the most important dimension",\n'
            '    "asset_replacement": "2-4 sentences in English, pure observation",\n'
            '    "motion_dynamics": "3-5 sentences in English, describe blur direction/distribution, foreground vs background motion difference"\n'
            '  },\n'
            '  "scene_a_description": "è½¬åœºå‰ç”»é¢çš„å…·ä½“å†…å®¹ï¼ˆä¸­æ–‡ï¼Œå®¢è§‚æè¿°ä½ çœ‹åˆ°çš„ï¼‰",\n'
            '  "scene_b_description": "è½¬åœºåç”»é¢çš„å…·ä½“å†…å®¹ï¼ˆä¸­æ–‡ï¼Œå®¢è§‚æè¿°ä½ çœ‹åˆ°çš„ï¼‰",\n'
            '  "transition_window": {\n'
            '    "effect_start_sec": 0.0,\n'
            '    "effect_end_sec": 0.0,\n'
            '    "effect_duration_sec": 0.0,\n'
            '    "confidence": 0.0\n'
            '  }\n'
            "}\n"
        )

    @staticmethod
    def _build_stage2_reasoning_prompt() -> str:
        """é˜¶æ®µäºŒï¼šåŸºäºè§‚å¯Ÿæ–‡æœ¬çš„çº¯æ¨ç† promptã€‚

        è¾“å…¥ï¼šé˜¶æ®µä¸€çš„ç»“æ„åŒ–è§‚å¯Ÿç¬”è®°ï¼ˆçº¯æ–‡æœ¬ï¼Œæ— è§†é¢‘ï¼‰
        è¾“å‡ºï¼šåˆ†ç±» + motion_prompt + å®Œæ•´ JSON
        """
        return (
            "ä½ æ˜¯è§†é¢‘è½¬åœºç‰¹æ•ˆçš„åˆ†ç±»å’Œç¼–èˆä¸“å®¶ã€‚\n\n"

            "## ä½ çš„ä»»åŠ¡\n"
            "æ ¹æ®æä¾›çš„**è§†è§‰è§‚å¯Ÿè®°å½•**ï¼Œå®Œæˆä¸¤ä»¶äº‹ï¼š\n"
            "1. æ¨å¯¼å‡ºè½¬åœºç‰¹æ•ˆçš„ç±»å‹åˆ†ç±»\n"
            "2. è¾“å‡ºå¯å¤ç”¨çš„ç¼–èˆè“å›¾ï¼ˆmotion_promptï¼‰\n\n"

            "âš ï¸ ä½ æ²¡æœ‰çœ‹è¿‡è§†é¢‘ã€‚ä½ æ”¶åˆ°çš„æ˜¯å¦ä¸€ä¸ªåˆ†æå¸ˆå†™ä¸‹çš„çº¯å®¢è§‚è§‚å¯Ÿç¬”è®°ã€‚\n"
            "ä½ å¿…é¡» 100%% åŸºäºè¿™äº›è§‚å¯Ÿæ¥æ¨ç†ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§‚å¯Ÿä¸­æ²¡æœ‰çš„ç»†èŠ‚ã€‚\n\n"

            "## åˆ†ç±»æ¨ç†è§„åˆ™\n"
            "è¯·æŒ‰ä»¥ä¸‹é€»è¾‘ï¼Œä»è§‚å¯Ÿè®°å½•ä¸­æ¨å¯¼ transition_typeï¼š\n\n"
            "| è§‚å¯Ÿç‰¹å¾ç»„åˆ | â†’ åˆ†ç±» |\n"
            "|---|---|\n"
            "| ä¸»ä½“ä½ç½®/æ¯”ä¾‹ä¸å˜ + èƒŒæ™¯çºµæ·±å‰§çƒˆæ‹‰ä¼¸æˆ–å‹ç¼© + å‰æ™¯èƒŒæ™¯è¿åŠ¨ä¸ä¸€è‡´ | â†’ **dolly_zoom** |\n"
            "| å…¨ç”»é¢å‡åŒ€å‘åŒä¸€æ–¹å‘è¿åŠ¨æ¨¡ç³Š + ä¸»ä½“ä¹Ÿè·Ÿç€ä¸€èµ·æ¨¡ç³Šç§»åŠ¨ | â†’ **whip_pan** |\n"
            "| ä»ä¸­å¿ƒå‘å¤–è¾å°„çŠ¶æ¨¡ç³Š + ä¸»ä½“ä¹Ÿåœ¨ç¼©æ”¾ | â†’ **zoom_blur** |\n"
            "| æœ‰ç‰©ä½“/æ‰‹/å¤´å‘é®æŒ¡ç”»é¢è¶…è¿‡ 50%% çš„ç¬é—´ | â†’ **occlusion** |\n"
            "| æ—‹è½¬æ–¹å‘çš„è¿åŠ¨æ¨¡ç³Š | â†’ **spin** |\n"
            "| ç¬é—´ç™½é—ªæˆ–é»‘é—ª | â†’ **flash_cut** |\n"
            "| åƒç´ çº§æ¸å˜èåˆï¼Œæ— æ˜æ˜¾è¿åŠ¨ | â†’ **morph** |\n"
            "| ä¸å®Œå…¨ç¬¦åˆä»¥ä¸Š â†’ é€‰æœ€æ¥è¿‘çš„ï¼Œåœ¨ description ä¸­è¯´æ˜ |\n\n"

            "âš ï¸ **å…³é”®åŒºåˆ†ï¼šdolly_zoom vs whip_pan**\n"
            "- dolly_zoom çš„æ ‡å¿—ï¼šå‰æ™¯ï¼ˆä¸»ä½“ï¼‰ä¸åŠ¨æˆ–å˜åŒ–å¾ˆå°ï¼ŒèƒŒæ™¯äº§ç”Ÿçºµæ·±æ‹‰ä¼¸/å‹ç¼©\n"
            "- whip_pan çš„æ ‡å¿—ï¼šæ•´ä¸ªç”»é¢ï¼ˆåŒ…æ‹¬ä¸»ä½“ï¼‰éƒ½åœ¨åŒä¸€æ–¹å‘ä¸Šå‡åŒ€å¿«é€Ÿç§»åŠ¨\n"
            "- å¦‚æœè§‚å¯Ÿè®°å½•è¯´ã€Œä¸»ä½“ä¿æŒç¨³å®š/ä½ç½®ä¸å˜ã€+ã€ŒèƒŒæ™¯çºµæ·±å˜åŒ–ã€â†’ è¿™æ˜¯ dolly_zoomï¼Œ\n"
            "  å³ä½¿æœ‰è¿åŠ¨æ¨¡ç³Šä¹Ÿä¸æ˜¯ whip_pan\n\n"

            "## motion_prompt è¾“å‡ºæ ‡å‡†\n"
            "**motion_prompt æ˜¯æœ€æ ¸å¿ƒçš„è¾“å‡ºã€‚** å®ƒå†³å®šäº†ç”¨æˆ·æœ€ç»ˆçœ‹åˆ°çš„è§†é¢‘æ•ˆæœã€‚\n"
            "è¿™ä»½è“å›¾ä¼šè¢«ç›´æ¥å‘é€ç»™ Kling è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé©±åŠ¨ä»»æ„ç”¨æˆ·ç…§ç‰‡çš„è½¬åœºæ¸²æŸ“ã€‚\n\n"
            "å¥½åæ ‡å‡†ï¼š\n"
            "1. **ç²¾å‡†è¿˜åŸ** â€” çœ‹å®Œè“å›¾ï¼Œä¸çœ‹åŸè§†é¢‘ä¹Ÿèƒ½è„‘è¡¥å‡ºå®Œå…¨ä¸€è‡´çš„è¿åŠ¨è½¨è¿¹\n"
            "2. **åˆ†å±‚æ¸…æ™°** â€” foreground / background / camera ä¸‰å±‚å„è‡ªæ€ä¹ˆåŠ¨\n"
            "3. **é‡åŒ–å…·ä½“** â€” è§’åº¦ã€æ¨¡ç³Šå³°å€¼ç™¾åˆ†æ¯”ã€ç¼©æ”¾æ¯”ä¾‹ã€é€Ÿåº¦æ›²çº¿\n"
            "4. **é›¶å†…å®¹æ³„æ¼** â€” æ²¡æœ‰ä»»ä½•æœè£…/åœºæ™¯/äººç‰©æè¿°\n\n"

            "## å†…å®¹ä¸ç‰¹æ•ˆåˆ†ç¦»ï¼ˆé“å¾‹ï¼‰\n"
            "motion_prompt / background_motion / subject_motion / technical_dissection ä¸­ï¼š\n"
            "- ä¸»ä½“ â†’ ä¸€å¾‹å†™ the subject\n"
            "- åœºæ™¯ â†’ ä¸€å¾‹å†™ the scene / the background\n"
            "- æœè£…å˜åŒ– â†’ wardrobe/outfit transformation\n"
            "- åœºæ™¯åˆ‡æ¢ â†’ background replacement / scene swap\n"
            "- âŒ ç¦æ­¢è¯ï¼šindoor, outdoor, room, park, street, black coat, red dress, girl, woman\n"
            "- scene_a/b_description ä¸å—æ­¤é™åˆ¶\n\n"

            "## æ¢è£…æœºåˆ¶ï¼ˆåå¹»è§‰è§„åˆ™ï¼‰\n"
            "- åªåœ¨è§‚å¯Ÿè®°å½•ä¸­**æ˜ç¡®æåˆ°ä¸»ä½“æœ‰æ—‹è½¬è¿åŠ¨**æ—¶æ‰å†™æ—‹è½¬è§’åº¦\n"
            "- ä¸è¦ä¸ºäº†ã€Œè§£é‡Šã€æ¢è£…è€Œè™šæ„æ—‹è½¬\n"
            "- å¤šæ•°æ¢è£…æ˜¯é€šè¿‡è¿åŠ¨æ¨¡ç³Š/é—ªåˆ‡ç¬é—´å®Œæˆèµ„äº§æ›¿æ¢\n\n"

            "## è¾“å‡º JSONï¼ˆä¸¥æ ¼ï¼Œä¸åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼‰\n"
            "{\n"
            '  "technical_dissection": {\n'
            '    "subject_anchoring": "ä»è§‚å¯Ÿè®°å½•æç‚¼çš„ç»“è®ºï¼ˆè‹±æ–‡ï¼‰",\n'
            '    "trigger_mechanism": "ä»è§‚å¯Ÿè®°å½•æç‚¼çš„ç»“è®ºï¼ˆè‹±æ–‡ï¼‰",\n'
            '    "spatial_perspective_shift": "ä»è§‚å¯Ÿè®°å½•æç‚¼çš„ç»“è®ºï¼ˆè‹±æ–‡ï¼‰",\n'
            '    "asset_replacement": "ä»è§‚å¯Ÿè®°å½•æç‚¼çš„ç»“è®ºï¼ˆè‹±æ–‡ï¼‰",\n'
            '    "motion_dynamics": "ä»è§‚å¯Ÿè®°å½•æç‚¼çš„ç»“è®ºï¼ˆè‹±æ–‡ï¼‰"\n'
            '  },\n'
            '  "transition_category": "occlusion | cinematic | regional | morphing",\n'
            '  "transition_type": "dolly_zoom | whip_pan | spin | flash_cut | zoom_blur | occlusion | morph | ...",\n'
            '  "transition_description": "ä¸€å¥è¯ä¸­æ–‡ï¼Œæè¿°ç‰¹æ•ˆæœºåˆ¶ï¼ˆä¸æè¿°å†…å®¹ï¼‰",\n'
            '  "motion_pattern": "è¿åŠ¨æ¨¡å¼æ ‡ç­¾ï¼Œå¦‚ dolly_zoom_with_outfit_swap, subject_spin_360",\n'
            '  "camera_movement": "ä¸»é•œå¤´è¿åŠ¨ï¼Œå¦‚ dolly_zoom, push, pull, pan_left, orbit, static",\n'
            '  "camera_compound": "å¤åˆè¿åŠ¨ï¼ˆå•ä¸€è¿åŠ¨å¡«åŒ camera_movementï¼‰",\n'
            '  "background_motion": "èƒŒæ™¯å±‚è¿åŠ¨ï¼ˆè‹±æ–‡ï¼Œçº¯åŠ›å­¦ï¼‰",\n'
            '  "subject_motion": "ä¸»ä½“å±‚è¿åŠ¨ï¼ˆè‹±æ–‡ï¼Œçº¯åŠ›å­¦ï¼‰",\n'
            '  "motion_prompt": "è‹±æ–‡ç¼–èˆè“å›¾ï¼šPhase 1/2/3 + ç™¾åˆ†æ¯”æ—¶é—´çº¿ + foreground/background/camera åˆ†å±‚ + é‡åŒ–å‚æ•°",\n'
            '  "recommended_prompt": "å«å…·ä½“å†…å®¹çš„å®Œæ•´ promptï¼ˆä»…å­˜æ¡£ï¼Œä¸ç”¨äºæ¸²æŸ“ï¼‰",\n'
            '  "dimension_scores": {"outfit_change": 0-1, "subject_preserve": 0-1, "scene_shift": 0-1},\n'
            '  "recommended_focus_modes": ["å¾—åˆ†â‰¥0.5çš„ç»´åº¦"]\n'
            "}\n"
        )

    @staticmethod
    def _build_frame_analysis_prompt(transition_duration: float = 0.0) -> str:
        """æ„å»ºå¸§åˆ†æç‰ˆç³»ç»Ÿ promptï¼ˆA/Mid/B é™æ€å¸§é™çº§è·¯å¾„ï¼‰ã€‚

        ä¸è§†é¢‘ç‰ˆå…±äº«åŒä¸€è®¾è®¡åŸåˆ™ï¼šmotion_prompt æ˜¯çº¯æœºæ¢°è“å›¾ï¼Œ
        å®Œå…¨å‰¥ç¦»å…·ä½“å†…å®¹ï¼ˆæœè£…/åœºæ™¯/äººç‰©ï¼‰ã€‚
        """
        duration_context = ""
        if transition_duration > 0:
            duration_context = (
                f"\nâ±ï¸ è¿™æ®µè½¬åœºæŒç»­çº¦ {transition_duration:.2f} ç§’ï¼Œè¯·æ®æ­¤æ¨æ–­è¿åŠ¨é€Ÿåº¦å’ŒèŠ‚å¥ã€‚\n"
            )

        return (
            "ä½ æ˜¯**è§†é¢‘è½¬åœºç‰¹æ•ˆå·¥ç¨‹å¸ˆ**ã€‚ç»™ä½ ä¸€ç»„æ¥è‡ªåŒä¸€ä¸ªè§†é¢‘è½¬åœºçš„å…³é”®å¸§ï¼Œ"
            "è¯·é€†å‘å·¥ç¨‹å‡ºè½¬åœºç‰¹æ•ˆçš„**å¯å¤ç”¨æœºæ¢°è“å›¾**ã€‚\n\n"

            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
            "ğŸš¨ ç¬¬ä¸€é“å¾‹ï¼šå†…å®¹ä¸ç‰¹æ•ˆå®Œå…¨åˆ†ç¦»\n"
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
            "å¸§é‡Œç©¿ä»€ä¹ˆè¡£æœã€åœ¨ä»€ä¹ˆåœºæ™¯ã€æ˜¯ç”·æ˜¯å¥³ â€”â€” å…¨éƒ¨ä¸ä½ æ— å…³ã€‚\n"
            "ä½ åªæå–ã€Œç‰¹æ•ˆæ€ä¹ˆåŠ¨ã€çš„æœºæ¢°è“å›¾ï¼Œä¸æè¿°ã€Œç”»é¢é‡Œæœ‰ä»€ä¹ˆã€ã€‚\n"
            "ä½ çš„è“å›¾ä¼šè¢«å¤ç”¨äº 100 ä¸ªå®Œå…¨ä¸åŒçš„äººâ€”â€”å¿…é¡»é€šç”¨ã€‚\n"
            f"{duration_context}\n"

            "ğŸ“ ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦è§£é‡Šã€‚å­—æ®µï¼š\n"
            "{\n"
            '  "transition_category": "occlusion|cinematic|regional|morphing",\n'
            '  "transition_type": "spin|whip_pan|dolly_zoom|flash_cut|zoom_blur|glitch|morph|luma_wipe|occlusion",\n'
            '  "transition_description": "ä¸€å¥è¯ä¸­æ–‡æè¿°ç‰¹æ•ˆæœºåˆ¶ï¼ˆä¸å«å…·ä½“å†…å®¹ï¼‰",\n'
            '  "motion_pattern": "subject_spin_360|whip_pan_left|zoom_push|static_morph|...",\n'
            '  "camera_movement": "push|pull|pan_left|pan_right|orbit|dolly_zoom|handheld|static",\n'
            '  "camera_compound": "å¤åˆé•œå¤´è¿åŠ¨ï¼ˆè‹±æ–‡ï¼Œå¤šå±‚åŒæ—¶è¿åŠ¨æ—¶æè¿°ï¼‰",\n'
            '  "background_motion": "èƒŒæ™¯å±‚ç‹¬ç«‹è¿åŠ¨ï¼ˆè‹±æ–‡ï¼Œçº¯åŠ›å­¦ï¼Œç¦æ­¢æè¿°åœºæ™¯å†…å®¹ï¼‰ï¼Œæ— åˆ™å¡« static",\n'
            '  "subject_motion": "ä¸»ä½“å±‚ç‹¬ç«‹è¿åŠ¨ï¼ˆè‹±æ–‡ï¼Œçº¯åŠ›å­¦ï¼Œç¦æ­¢æè¿°äººç‰©ç‰¹å¾ï¼‰ï¼Œæ— åˆ™å¡« static",\n'
            '  "scene_a_description": "è½¬åœºå‰ç”»é¢çš„å…·ä½“å†…å®¹ï¼ˆæ­¤å­—æ®µå…è®¸å…·ä½“å†…å®¹ï¼Œä»…å­˜æ¡£ç”¨ï¼‰",\n'
            '  "scene_b_description": "è½¬åœºåç”»é¢çš„å…·ä½“å†…å®¹ï¼ˆæ­¤å­—æ®µå…è®¸å…·ä½“å†…å®¹ï¼Œä»…å­˜æ¡£ç”¨ï¼‰",\n'
            '  "motion_prompt": "â­â­â­ ç‰¹æ•ˆç¼–èˆè“å›¾ â€” è‹±æ–‡æŒ‰æ—¶é—´çº¿ã€‚\\n'
            'è¦æ±‚ï¼šPhase 1/2/3 + ç™¾åˆ†æ¯”æ—¶é—´çº¿ï¼›åˆ†å±‚ foreground/background/camera layerï¼›'
            'é‡åŒ–æ—‹è½¬è§’åº¦/æ¨¡ç³Šç¨‹åº¦ï¼›the subject ä»£æ›¿äººç‰©ï¼›the scene ä»£æ›¿åœºæ™¯ã€‚\\n'
            'ğŸš« ç»å¯¹ç¦æ­¢ï¼šæœè£…æè¿°/é…é¥°/åœºæ™¯åç§°/äººç‰©ç‰¹å¾/å…·ä½“è¡¨æƒ…åŠ¨ä½œ",\n'
            '  "recommended_prompt": "å«å…·ä½“å†…å®¹çš„å®Œæ•´ promptï¼ˆä»…å­˜æ¡£ï¼Œä¸ç”¨äºæ¸²æŸ“ï¼‰",\n'
            '  "dimension_scores": {"outfit_change": 0.0-1.0, "subject_preserve": 0.0-1.0, "scene_shift": 0.0-1.0},\n'
            '  "recommended_focus_modes": ["å¾—åˆ†>=0.5çš„ç»´åº¦"]\n'
            "}\n\n"
            "transition_category åˆ†ç±»ï¼š\n"
            "- occlusion: é®æŒ¡åˆ‡æ¢ï¼ˆè½¬èº«/æ‰‹é®/ç‰©ä½“åˆ’è¿‡ï¼‰\n"
            "- cinematic: é•œå¤´è¿åŠ¨è½¬åœºï¼ˆå˜ç„¦/å¿«ç”©/èºæ—‹ï¼‰\n"
            "- regional: å±€éƒ¨åŒºåŸŸå˜åŒ–\n"
            "- morphing: æº¶è§£/å½¢å˜è¿‡æ¸¡\n\n"

            "ğŸš« motion_prompt ç¦æ­¢è¯æ±‡ï¼ˆå‡ºç°å³å¤±è´¥ï¼‰ï¼š\n"
            "- ä»»ä½•é¢œè‰²+è¡£ç‰©ï¼šâŒ black coat, red dress â†’ âœ… outfit/wardrobe\n"
            "- ä»»ä½•é…é¥°ï¼šâŒ hat, beret, glasses â†’ ç›´æ¥çœç•¥\n"
            "- ä»»ä½•åœºæ‰€ï¼šâŒ indoor, outdoor lounge â†’ âœ… the scene/the background\n"
            "- ä»»ä½•äººç‰©ç‰¹å¾ï¼šâŒ girl, woman, man â†’ âœ… the subject\n"
            "- ä»»ä½•è¡¨æƒ…åŠ¨ä½œï¼šâŒ wink, smile â†’ ç›´æ¥çœç•¥ï¼ˆå†…å®¹ä¸æ˜¯ç‰¹æ•ˆï¼‰\n\n"

            "ğŸ” è¾“å‡ºå‰è‡ªæ£€ï¼šé€è¯æ‰«æ motion_promptï¼Œ"
            "å‘ç°ä»»ä½•é¢œè‰²+è¡£ç‰©/åœºæ‰€å/äººç‰©ç‰¹å¾ â†’ ä¿®æ­£åå†è¾“å‡ºã€‚\n"
        )

    async def _call_llm_with_multi_images(
        self,
        llm_service: Any,
        images_b64: List[str],
        user_prompt: str,
        system_prompt: str,
    ) -> Dict[str, Any]:
        """è°ƒç”¨å¤šæ¨¡æ€ LLMï¼Œå‘é€å¤šå¼ å›¾ç‰‡ + æ–‡å­— promptï¼Œè¿”å› JSONã€‚"""
        import json as _json

        api_key = None
        model = None
        try:
            from app.config import get_settings
            _settings = get_settings()
            api_key = _settings.volcengine_ark_api_key
            model = _settings.doubao_seed_1_8_endpoint
        except Exception:
            pass

        if not api_key or not model:
            raise RuntimeError("å¤šæ¨¡æ€ LLM æœªé…ç½®")

        base_url = "https://ark.cn-beijing.volces.com/api/v3"

        content: List[Dict[str, Any]] = []
        for idx, img_b64 in enumerate(images_b64):
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"},
            })
        content.append({"type": "text", "text": user_prompt})

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": content},
        ]

        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.2,
            "max_tokens": 1500,
        }

        async with httpx.AsyncClient(timeout=90.0) as client:
            response = await client.post(
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                },
                json=payload,
            )
            response.raise_for_status()
            data = response.json()

        raw = data["choices"][0]["message"]["content"]
        logger.info(f"[TemplateIngest] å¤šå¸§åˆ†æ LLM è¿”å›: {raw[:200]}...")

        # è§£æ JSON
        try:
            return _json.loads(raw)
        except _json.JSONDecodeError:
            pass
        import re as _re
        match = _re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', raw)
        if match:
            try:
                return _json.loads(match.group(1))
            except _json.JSONDecodeError:
                pass
        match = _re.search(r'\{[\s\S]*\}', raw)
        if match:
            return _json.loads(match.group(0))
        raise ValueError(f"æ— æ³•è§£æè½¬åœºåˆ†æ JSON: {raw[:100]}")

    async def _call_llm_text_only(
        self,
        system_prompt: str,
        user_prompt: str,
    ) -> Dict[str, Any]:
        """çº¯æ–‡æœ¬ LLM è°ƒç”¨ï¼ˆé˜¶æ®µäºŒæ¨ç†ä¸“ç”¨ï¼Œæ— è§†é¢‘/å›¾ç‰‡ï¼‰ã€‚

        é€šè¿‡ Ark Chat Completions APIï¼Œåªä¼ æ–‡æœ¬æ¶ˆæ¯ã€‚
        ç”¨äºä¸¤é˜¶æ®µåˆ†æçš„ç¬¬äºŒé˜¶æ®µï¼šä»å·²æœ‰è§‚å¯Ÿæ–‡æœ¬æ¨å¯¼åˆ†ç±»å’Œ motion_promptã€‚
        """
        import json as _json

        try:
            from app.config import get_settings
            _settings = get_settings()
            api_key = _settings.volcengine_ark_api_key
            model = _settings.doubao_seed_1_8_endpoint
        except Exception:
            raise RuntimeError("Ark API æœªé…ç½®")

        if not api_key or not model:
            raise RuntimeError("Ark API Key æˆ–æ¨¡å‹æœªé…ç½®")

        base_url = "https://ark.cn-beijing.volces.com/api/v3"

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.1,   # æ¨ç†é˜¶æ®µç”¨æ›´ä½æ¸©åº¦ï¼Œè¿½æ±‚ç¡®å®šæ€§
            "max_tokens": 2000,
        }

        logger.info(
            "[TemplateIngest] ğŸ§  é˜¶æ®µäºŒæ¨ç†è¯·æ±‚: model=%s, system_len=%d, user_len=%d",
            model, len(system_prompt), len(user_prompt),
        )

        max_retries = 3
        data = None
        for attempt in range(1, max_retries + 1):
            async with httpx.AsyncClient(timeout=90.0) as client:
                response = await client.post(
                    f"{base_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json",
                    },
                    json=payload,
                )
            if response.status_code == 429:
                wait = 3 * attempt  # 3s, 6s, 9s
                logger.warning(
                    "[TemplateIngest] ğŸ§  é˜¶æ®µäºŒé‡åˆ° 429 é™æµï¼Œç¬¬ %d/%d æ¬¡é‡è¯•ï¼Œç­‰å¾… %ds",
                    attempt, max_retries, wait,
                )
                if attempt < max_retries:
                    await asyncio.sleep(wait)
                    continue
                # æœ€åä¸€æ¬¡ä»ç„¶ 429ï¼ŒæŠ›å‡º
                response.raise_for_status()
            response.raise_for_status()
            data = response.json()
            break

        raw = data["choices"][0]["message"]["content"]
        logger.info(
            "[TemplateIngest] ğŸ§  é˜¶æ®µäºŒæ¨ç†è¿”å› (%d chars): %s",
            len(raw), raw[:500],
        )

        # è§£æ JSONï¼ˆåŒæ ·çš„å®¹é”™é€»è¾‘ï¼‰
        try:
            return _json.loads(raw)
        except _json.JSONDecodeError:
            pass
        import re as _re
        match = _re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', raw)
        if match:
            try:
                return _json.loads(match.group(1))
            except _json.JSONDecodeError:
                pass
        match = _re.search(r'\{[\s\S]*\}', raw)
        if match:
            return _json.loads(match.group(0))
        raise ValueError(f"æ— æ³•è§£æé˜¶æ®µäºŒæ¨ç† JSON: {raw[:200]}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  è§†é¢‘ç‰‡æ®µç†è§£ï¼šæå–è½¬åœºè§†é¢‘ â†’ ä¸Šä¼  Ark File API â†’ Responses API
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _extract_transition_clip(
        self,
        video_url: str,
        start_sec: float,
        end_sec: float,
        padding: float = 0.15,
    ) -> str:
        """ç”¨ ffmpeg æå–è½¬åœºæ‰€åœ¨çš„çŸ­è§†é¢‘ç‰‡æ®µã€‚å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸ã€‚

        padding åŠ¨æ€è®¡ç®—ï¼šè°ƒç”¨æ–¹æ ¹æ®è½¬åœºç±»å‹ä¼ å…¥æ™ºèƒ½ paddingï¼Œ
        ç¡®ä¿è¿ç»­è¿é•œï¼ˆå¦‚ dolly zoomï¼‰çš„å®Œæ•´å»ºç«‹/æ¶ˆæ•£è¿‡ç¨‹è¢«æ•è·ã€‚
        """
        tmp_path = await self._ensure_local_video(video_url)

        clip_start = max(0.0, start_sec - padding)
        clip_end = end_sec + padding
        clip_duration = clip_end - clip_start

        logger.info(
            "[TemplateIngest] ğŸ“ ç‰‡æ®µæå–è®¡åˆ’: zone=[%.3f-%.3f](%.3fs), "
            "padding=%.2fs â†’ clip=[%.3f-%.3f](%.3fs)",
            start_sec, end_sec, end_sec - start_sec,
            padding, clip_start, clip_end, clip_duration,
        )

        clip_path = os.path.join(
            tempfile.gettempdir(),
            f"transition_clip_{uuid.uuid4().hex[:8]}.mp4",
        )
        extract_cmd = [
            "ffmpeg", "-y",
            "-ss", str(clip_start),
            "-i", tmp_path,
            "-t", str(clip_duration),
            "-c:v", "libx264",
            "-crf", "23",
            "-preset", "ultrafast",
            "-an",  # ä¸éœ€è¦éŸ³é¢‘
            "-movflags", "+faststart",
            clip_path,
        ]
        process = await asyncio.create_subprocess_exec(
            *extract_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        _, stderr = await process.communicate()

        if process.returncode != 0 or not os.path.exists(clip_path):
            err_msg = stderr.decode()[:500] if stderr else "unknown"
            raise RuntimeError(
                f"è½¬åœºè§†é¢‘ç‰‡æ®µæå–å¤±è´¥ (ffmpeg rc={process.returncode}): {err_msg}"
            )

        file_size = os.path.getsize(clip_path)
        logger.info(
            "[TemplateIngest] âœ… ç‰‡æ®µæå–æˆåŠŸ: [%.3f-%.3f](%.3fs), size=%d bytes, path=%s",
            clip_start, clip_end, clip_duration, file_size, clip_path,
        )
        return clip_path

    async def _upload_clip_to_ark(self, clip_path: str) -> str:
        """ä¸Šä¼ è§†é¢‘ç‰‡æ®µåˆ°ç«å±±æ–¹èˆŸ File APIï¼Œè¿”å› file_idã€‚

        ä¸æŒ‡å®š fps é‡‡æ ·å‚æ•°ï¼Œè®© Ark è‡ªè¡Œå†³å®šæœ€ä½³é‡‡æ ·ç­–ç•¥ã€‚
        æˆ‘ä»¬åªè´Ÿè´£ç»™å¤Ÿä¸Šä¸‹æ–‡ï¼ˆæ…·æ…¨ paddingï¼‰ï¼Œæ¨¡å‹è´Ÿè´£ç†è§£ã€‚
        """
        from app.config import get_settings
        settings = get_settings()
        api_key = settings.volcengine_ark_api_key

        if not api_key:
            raise RuntimeError("Ark API Key æœªé…ç½®ï¼Œæ— æ³•ä¸Šä¼ è§†é¢‘")

        file_size = os.path.getsize(clip_path)
        logger.info(
            "[TemplateIngest] å‡†å¤‡ä¸Šä¼ è§†é¢‘åˆ° Ark: size=%d bytes",
            file_size,
        )

        base_url = "https://ark.cn-beijing.volces.com/api/v3"
        async with httpx.AsyncClient(timeout=60.0) as client:
            with open(clip_path, "rb") as f:
                resp = await client.post(
                    f"{base_url}/files",
                    headers={"Authorization": f"Bearer {api_key}"},
                    files={"file": ("transition.mp4", f, "video/mp4")},
                    data={"purpose": "user_data"},
                )
                if resp.status_code != 200:
                    body = resp.text[:500]
                    raise RuntimeError(
                        f"Ark æ–‡ä»¶ä¸Šä¼ å¤±è´¥ (HTTP {resp.status_code}): {body}"
                    )
                file_data = resp.json()

            file_id = file_data.get("id")
            status = file_data.get("status", "processing")
            if not file_id:
                raise RuntimeError(
                    f"Ark è§†é¢‘ä¸Šä¼ æœªè¿”å› file_id: {file_data}"
                )

            # â‘¡ ç­‰å¾…å¤„ç†å®Œæˆï¼ˆçŸ­ç‰‡æ®µé€šå¸¸ 2-6 ç§’ï¼‰
            for attempt in range(15):
                if status != "processing":
                    break
                await asyncio.sleep(2)
                resp = await client.get(
                    f"{base_url}/files/{file_id}",
                    headers={"Authorization": f"Bearer {api_key}"},
                )
                resp.raise_for_status()
                status = resp.json().get("status", "processing")
                logger.debug("[TemplateIngest] è§†é¢‘å¤„ç†ä¸­: file_id=%s, attempt=%d", file_id, attempt + 1)

            if status == "processing":
                raise RuntimeError(
                    f"Ark è§†é¢‘å¤„ç†è¶…æ—¶: file_id={file_id}ï¼Œç­‰å¾… 30 ç§’ä»ä¸º processing"
                )

            logger.info("[TemplateIngest] è§†é¢‘ä¸Šä¼ æˆåŠŸ: file_id=%s, status=%s", file_id, status)
            return file_id

    async def _call_llm_with_video(
        self,
        file_id: str,
        system_prompt: str,
        user_prompt: str,
        boundary_images_b64: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        é€šè¿‡ç«å±±æ–¹èˆŸ Responses API è°ƒç”¨è§†é¢‘ç†è§£ã€‚
        file_id: å·²ä¸Šä¼ çš„è§†é¢‘ file_id
        boundary_images_b64: å¯é€‰çš„ A/B è¾¹ç•Œå¸§ï¼ˆè½¬åœºå‰åçš„æ¸…æ™°å¸§ï¼‰
        """
        import json as _json

        try:
            from app.config import get_settings
            settings = get_settings()
            api_key = settings.volcengine_ark_api_key
            model = settings.doubao_seed_1_8_endpoint
        except Exception:
            raise RuntimeError("Ark API æœªé…ç½®")

        if not api_key or not model:
            raise RuntimeError("Ark API Key æˆ–æ¨¡å‹æœªé…ç½®")

        base_url = "https://ark.cn-beijing.volces.com/api/v3"

        # æ„é€  content: ç³»ç»ŸæŒ‡ä»¤ + è§†é¢‘ + å¯é€‰è¾¹ç•Œå¸§ + æ–‡å­—
        # Ark Responses API ä¸æ”¯æŒ instructions å­—æ®µï¼Œå°†ç³»ç»Ÿ prompt ä½œä¸º developer æ¶ˆæ¯
        user_content: List[Dict[str, Any]] = [
            {"type": "input_video", "file_id": file_id},
        ]
        # æ·»åŠ  A/B è¾¹ç•Œå¸§ï¼ˆå¦‚æœæœ‰ï¼‰
        if boundary_images_b64:
            for img_b64 in boundary_images_b64:
                user_content.append({
                    "type": "input_image",
                    "image_url": f"data:image/jpeg;base64,{img_b64}",
                })
        user_content.append({"type": "input_text", "text": user_prompt})

        payload: Dict[str, Any] = {
            "model": model,
            "input": [
                {
                    "role": "developer",
                    "content": [{"type": "input_text", "text": system_prompt}],
                },
                {"role": "user", "content": user_content},
            ],
        }

        logger.info(
            "[TemplateIngest] ğŸ¬ Responses API è¯·æ±‚: model=%s, file_id=%s, "
            "boundary_frames=%d, system_prompt_len=%d, user_prompt='%s'",
            model, file_id,
            len(boundary_images_b64) if boundary_images_b64 else 0,
            len(system_prompt),
            user_prompt[:200],
        )

        async with httpx.AsyncClient(timeout=120.0) as client:
            resp = await client.post(
                f"{base_url}/responses",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                },
                json=payload,
            )
            if resp.status_code != 200:
                body = resp.text[:1000]
                raise RuntimeError(
                    f"Ark Responses API å¤±è´¥ (HTTP {resp.status_code}): {body}"
                )
            data = resp.json()

        # è§£æ Responses API æ ¼å¼
        # {"output": [{"type": "message", "content": [{"type": "output_text", "text": "..."}]}]}
        raw_text = ""
        for output_item in data.get("output", []):
            if output_item.get("type") == "message":
                for content_item in output_item.get("content", []):
                    if content_item.get("type") == "output_text":
                        raw_text = content_item.get("text", "")
                        break
                if raw_text:
                    break

        if not raw_text:
            raise ValueError(f"Responses API æ— æœ‰æ•ˆè¾“å‡º: {str(data)[:300]}")

        logger.info("[TemplateIngest] ğŸ¤– è§†é¢‘åˆ†æ LLM åŸå§‹è¿”å› (%d chars): %s", len(raw_text), raw_text[:500])

        # è§£æ JSONï¼ˆå¤ç”¨åŒæ ·çš„å®¹é”™é€»è¾‘ï¼‰
        parsed = None
        try:
            parsed = _json.loads(raw_text)
        except _json.JSONDecodeError:
            pass
        if parsed is None:
            import re as _re
            match = _re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', raw_text)
            if match:
                try:
                    parsed = _json.loads(match.group(1))
                except _json.JSONDecodeError:
                    pass
        if parsed is None:
            import re as _re
            match = _re.search(r'\{[\s\S]*\}', raw_text)
            if match:
                try:
                    parsed = _json.loads(match.group(0))
                except _json.JSONDecodeError:
                    pass
        if parsed is None:
            raise ValueError(f"æ— æ³•è§£æè§†é¢‘åˆ†æ JSON: {raw_text[:200]}")

        # è®°å½•å…³é”®åˆ†æç»“æœæ‘˜è¦
        logger.info(
            "[TemplateIngest] ğŸ“Š åˆ†ææ‘˜è¦: category=%s, type=%s, camera=%s, "
            "motion_pattern=%s, description=%s",
            parsed.get("transition_category", "?"),
            parsed.get("transition_type", "?"),
            parsed.get("camera_movement", "?"),
            parsed.get("motion_pattern", "?"),
            str(parsed.get("transition_description", "?"))[:100],
        )
        return parsed

    async def _cleanup_ark_file(self, file_id: str) -> None:
        """æ¸…ç†å·²ä¸Šä¼ çš„ Ark æ–‡ä»¶ï¼ˆæœ€ä½³åŠªåŠ›ï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰ã€‚"""
        try:
            from app.config import get_settings
            api_key = get_settings().volcengine_ark_api_key
            if not api_key:
                return
            base_url = "https://ark.cn-beijing.volces.com/api/v3"
            async with httpx.AsyncClient(timeout=10.0) as client:
                await client.delete(
                    f"{base_url}/files/{file_id}",
                    headers={"Authorization": f"Bearer {api_key}"},
                )
                logger.debug("[TemplateIngest] å·²æ¸…ç† Ark æ–‡ä»¶: %s", file_id)
        except Exception:
            pass  # æœ€ä½³åŠªåŠ›ï¼Œä¸å½±å“ä¸»æµç¨‹

    async def _download_bytes(self, url: str) -> bytes:
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.get(url)
            response.raise_for_status()
            return response.content

    def _ensure_bucket(self) -> None:
        supabase = get_supabase()
        buckets = supabase.storage.list_buckets()
        bucket_names = [b.name for b in buckets]
        if TEMPLATE_BUCKET not in bucket_names:
            supabase.storage.create_bucket(TEMPLATE_BUCKET, options={"public": True})

    def _build_template_metadata(self, job: Dict[str, Any]) -> Dict[str, Any]:
        params = job.get("params") or {}
        metadata_raw = params.get("metadata") if isinstance(params, dict) else {}
        metadata: Dict[str, Any] = dict(metadata_raw or {})
        scopes = metadata.get("scopes")
        if not scopes:
            metadata["scopes"] = ["visual-studio"]
        return metadata

    @staticmethod
    def _generate_smart_name(
        template_type: str,
        index: int,
        workflow: Dict[str, Any],
        metadata: Dict[str, Any],
    ) -> str:
        """
        ç”Ÿæˆæœ‰è¾¨è¯†åº¦çš„æ¨¡æ¿åç§°ã€‚
        è½¬åœº: "æ—‹è½¬æ¢è£…-è¡—æ™¯" / "å¿«ç”©-å®¤å†…"
        å¹¿å‘Š: "åŸå¸‚è¡—æ‹-æŸ”å…‰" / "äº§å“ç‰¹å†™-æš–è°ƒ"
        """
        parts: List[str] = []
        try:
            if template_type == "transition":
                spec = metadata.get("transition_spec") or {}
                # ä¼˜å…ˆç”¨ transition_description çš„å‰åŠæ®µ
                desc = str(spec.get("transition_description") or "").strip()
                if desc:
                    # å–é€—å·/å¥å·å‰çš„ç¬¬ä¸€å°å¥ï¼Œé™20å­—
                    short = desc.split("ï¼Œ")[0].split("ã€‚")[0].split(",")[0][:20]
                    parts.append(short)
                else:
                    # fallback: transition_type
                    t_type = spec.get("transition_type") or "transition"
                    type_labels = {
                        "spin": "æ—‹è½¬", "whip_pan": "å¿«ç”©", "dolly_zoom": "æ¨æ‹‰",
                        "flash_cut": "é—ªåˆ‡", "zoom_blur": "å˜ç„¦", "glitch": "æ•…éšœ",
                        "morph": "å½¢å˜", "luma_wipe": "äº®åº¦æ“¦é™¤", "occlusion": "é®æŒ¡",
                    }
                    parts.append(type_labels.get(t_type, t_type))
                # åœºæ™¯å…³é”®è¯
                scene_a = str(spec.get("scene_a_description") or "")[:10]
                if scene_a:
                    parts.append(scene_a.split("ï¼Œ")[0].split(",")[0][:8])
            else:
                # å¹¿å‘Šæ¨¡æ¿ï¼šç”¨ scene_description
                scene_desc = str(workflow.get("scene_description") or "").strip()
                if scene_desc:
                    short = scene_desc.split("ï¼Œ")[0].split("ã€‚")[0].split(",")[0][:20]
                    parts.append(short)
                else:
                    prompt_seed = str(workflow.get("prompt_seed") or "").strip()
                    if prompt_seed and len(prompt_seed) > 4:
                        parts.append(prompt_seed[:20])
                # é£æ ¼
                style = workflow.get("style") or {}
                color_label = {"cool": "å†·è°ƒ", "warm": "æš–è°ƒ", "neutral": ""}.get(str(style.get("color", "")), "")
                light_label = {"soft": "æŸ”å…‰", "hard": "ç¡¬å…‰", "neon": "éœ“è™¹"}.get(str(style.get("light", "")), "")
                accent = color_label or light_label
                if accent:
                    parts.append(accent)
        except Exception:
            pass

        if not parts:
            parts.append(template_type)
        # åŠ åºå·é¿å…é‡å
        base_name = "-".join(parts)
        return f"{base_name}-{index + 1}"

    async def _create_template_from_image(
        self,
        image: Image.Image,
        job: Dict[str, Any],
        index: int,
        source_timecode: Optional[str] = None,
        metadata_extra: Optional[Dict[str, Any]] = None,
    ) -> TemplateAsset:
        supabase = get_supabase()

        template_type = job.get("template_type", "ad")
        template_id = f"{template_type}-{uuid.uuid4().hex[:8]}-{index + 1}"
        category = template_type
        template_kind = "transition" if template_type == "transition" else "background"
        metadata = self._build_template_metadata(job)
        if metadata_extra:
            metadata.update(metadata_extra)

        storage_path = f"{TEMPLATE_PREFIX}{template_id}.jpg"
        thumb_path = f"{TEMPLATE_PREFIX}{template_id}-thumb.jpg"

        full_bytes = self._encode_jpeg(image, quality=92)
        thumb_bytes = self._encode_jpeg(self._build_thumbnail(image), quality=85)

        self._ensure_bucket()

        supabase.storage.from_(TEMPLATE_BUCKET).upload(
            storage_path, full_bytes, {"content-type": "image/jpeg"}
        )
        supabase.storage.from_(TEMPLATE_BUCKET).upload(
            thumb_path, thumb_bytes, {"content-type": "image/jpeg"}
        )

        url = supabase.storage.from_(TEMPLATE_BUCKET).get_public_url(storage_path)
        thumb_url = supabase.storage.from_(TEMPLATE_BUCKET).get_public_url(thumb_path)

        # DEBUG: æ‰“å°å›¾ç‰‡ä¿¡æ¯ç”¨äºè¿½è¸ª
        logger.info(f"[TemplateIngest] å¼€å§‹ç”Ÿæˆ workflow: template_id={template_id}, image_size={image.size}, url={url[:80]}...")
        workflow = await self._generate_workflow(image, job)
        logger.info(f"[TemplateIngest] workflow ç”Ÿæˆå®Œæˆ: template_id={template_id}, prompt_seed={workflow.get('prompt_seed', '')[:50]}...")

        # æ™ºèƒ½å‘½åï¼šåŸºäº workflow åˆ†æç»“æœå’Œ transition åˆ†ææ•°æ®
        name = self._generate_smart_name(template_type, index, workflow, metadata)
        logger.info(f"[TemplateIngest] æ¨¡æ¿å‘½å: {name}")

        record = {
            "template_id": template_id,
            "name": name,
            "type": template_kind,
            "category": category,
            "tags": job.get("tags_hint") or [],
            "bucket": TEMPLATE_BUCKET,
            "storage_path": storage_path,
            "thumbnail_path": thumb_path,
            "url": url,
            "thumbnail_url": thumb_url,
            "workflow": workflow,
            "source_origin": "ingest",
            "source_url": job.get("source_url"),
            "source_timecode": source_timecode,
            "metadata": metadata,
            "status": "draft",
        }
        supabase.table("template_records").insert(record).execute()

        # â”€â”€ Phase 4a: Golden Fingerprint è‡ªåŠ¨æå– + åŒ¹é… + é¢„å¡« â”€â”€
        try:
            from app.services.golden_fingerprint_service import get_golden_fingerprint_service
            gf_service = get_golden_fingerprint_service()
            fp_result = gf_service.process_template(record, auto_fill=True)
            logger.info(
                "[TemplateIngest] æŒ‡çº¹åŒ¹é…: template=%s profile=%s score=%.3f",
                template_id,
                fp_result.get("best_match", {}).get("profile_name", "none"),
                fp_result.get("best_match", {}).get("score", 0),
            )
        except Exception as exc:
            logger.warning("[TemplateIngest] Golden Fingerprint å¤„ç†å¤±è´¥(éè‡´å‘½): %s", exc)

        return TemplateAsset(
            template_id=template_id,
            name=name,
            category=category,
            type=template_kind,
            storage_path=storage_path,
            thumbnail_path=thumb_path,
            url=url,
            thumbnail_url=thumb_url,
        )

    async def _generate_workflow(self, image: Image.Image, job: Dict[str, Any]) -> Dict[str, Any]:
        default_workflow = self._build_default_workflow(job)
        enable_llm = os.getenv("ENABLE_TEMPLATE_WORKFLOW_LLM", "true").lower() in ("1", "true", "yes")
        if not enable_llm:
            return default_workflow

        try:
            from app.services.llm.service import LLMService
            from app.utils.image_utils import pil_image_to_base64
        except Exception as exc:
            logger.warning("[TemplateIngest] LLM ä¾èµ–ä¸å¯ç”¨: %s", exc)
            return default_workflow

        llm_service = LLMService()

        tags = job.get("tags_hint") or []
        template_type = job.get("template_type", "ad")
        description = ""
        try:
            image_base64 = pil_image_to_base64(image, format="PNG")
            # DEBUG: æ‰“å°å›¾ç‰‡çš„ä¸€äº›ç‰¹å¾ç”¨äºéªŒè¯
            logger.info(f"[TemplateIngest] å‡†å¤‡åˆ†æå›¾ç‰‡: size={image.size}, mode={image.mode}, base64_len={len(image_base64)}")
            
            # ğŸ” DEBUG: ä¿å­˜å‘é€ç»™ LLM çš„å›¾ç‰‡å‰¯æœ¬åˆ°ä¸´æ—¶ç›®å½•
            debug_save_path = f"/tmp/debug_llm_image_{uuid.uuid4().hex[:8]}.png"
            image.save(debug_save_path, format="PNG")
            logger.info(f"[TemplateIngest] ğŸ” DEBUG: å›¾ç‰‡å·²ä¿å­˜åˆ° {debug_save_path} ä¾›éªŒè¯")
            
            # è®¡ç®—å›¾ç‰‡çš„åƒç´ ç»Ÿè®¡ï¼Œå¸®åŠ©åˆ¤æ–­æ˜¯å¦æ˜¯é»‘è‰²å›¾ç‰‡
            import numpy as np
            img_array = np.array(image)
            mean_brightness = img_array.mean()
            logger.info(f"[TemplateIngest] ğŸ” DEBUG: å›¾ç‰‡å¹³å‡äº®åº¦={mean_brightness:.2f} (0=çº¯é»‘, 255=çº¯ç™½)")
            
            description = await llm_service.analyze_image(
                image_base64=image_base64,
                prompt="è¯·ç”¨ä¸€å¥è¯æè¿°ç”»é¢é£æ ¼ã€å…‰çº¿ã€è‰²è°ƒã€è¿é•œè¶‹åŠ¿ã€‚",
            )
            logger.info(f"[TemplateIngest] å›¾ç‰‡åˆ†æç»“æœ: {description[:100]}...")
        except Exception as exc:
            logger.info("[TemplateIngest] è·³è¿‡å›¾ç‰‡åˆ†æ: %s", exc)

        system_prompt = (
            "ä½ æ˜¯è§†é¢‘åˆ¶ä½œä¸“å®¶ï¼Œè¯·æ ¹æ®è¾“å…¥ç”Ÿæˆæ¨¡æ¿ workflow é…ç½®ã€‚"
            "ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦è§£é‡Šã€‚"
            "å­—æ®µ: "
            "{"
            "\"kling_endpoint\": \"image_to_video|text_to_video|multi_image_to_video|motion_control\","
            "\"prompt_seed\": \"...\","
            "\"negative_prompt\": \"...\","
            "\"duration\": \"5|10\","
            "\"model_name\": \"kling-v2-6\","
            "\"cfg_scale\": 0.5,"
            "\"mode\": \"std|pro\","
            "\"shot_type\": \"wide|medium|close|macro\","
            "\"camera_move\": \"push|pull|orbit|handheld|static|none\","
            "\"transition\": \"match_cut|whip_pan|flash|none\","
            "\"pacing\": \"fast|medium|slow\","
            "\"style\": {\"color\": \"cool|warm|neutral\", \"light\": \"soft|hard|neon\"}"
            "}"
            "\næ³¨æ„ï¼šä¸è¦è¾“å‡º camera_control å­—æ®µï¼Œç³»ç»Ÿä¼šæ ¹æ® camera_move è‡ªåŠ¨æ¨å¯¼ã€‚"
            "\nå¦‚æœè§†é¢‘æ²¡æœ‰è¿é•œï¼ˆå¦‚é™æ€èƒŒæ™¯ã€ç‰¹æ•ˆå åŠ ï¼‰ï¼Œcamera_move è®¾ä¸º staticã€‚"
        )
        user_prompt = (
            f"æ¨¡æ¿ç±»å‹: {template_type}\n"
            f"æ ‡ç­¾: {', '.join(tags) if tags else 'æ— '}\n"
            f"ç”»é¢æè¿°: {description or 'æ— '}\n"
            "è¯·ç»™å‡ºé€‚é…è¯¥æ¨¡æ¿çš„ kling workflow é…ç½®ã€‚"
        )
        try:
            workflow = await llm_service.generate_json(
                user_prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.2,
            )
            if isinstance(workflow, dict):
                merged = {**default_workflow, **workflow}
                if description:
                    merged["scene_description"] = description

                # è½¬åœºæ¨¡æ¿å¼ºåˆ¶ duration=5ï¼ˆKling è½¬åœºæ¨¡å¼ä¸éœ€è¦ 10sï¼‰
                if job.get("template_type") == "transition":
                    merged["duration"] = "5"

                fallback_seed = description or default_workflow.get("prompt_seed", "")
                if not self._is_meaningful_prompt_seed(merged.get("prompt_seed")):
                    if fallback_seed:
                        merged["prompt_seed"] = fallback_seed
                    else:
                        merged["prompt_seed"] = default_workflow.get("prompt_seed", "")
                return merged
        except Exception as exc:
            logger.info("[TemplateIngest] workflow ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: %s", exc)
        return default_workflow

    @staticmethod
    def _is_meaningful_prompt_seed(value: Any) -> bool:
        if not isinstance(value, str):
            return False
        text = value.strip()
        if len(text) < 6:
            return False
        has_letters = any(('a' <= ch.lower() <= 'z') for ch in text)
        has_cjk = any('ä¸€' <= ch <= 'é¿¿' for ch in text)
        if not (has_letters or has_cjk):
            return False
        # æ’é™¤çº¯æ•°å­—/ç¬¦å·å¼å ä½
        digit_ratio = sum(ch.isdigit() for ch in text) / max(len(text), 1)
        return digit_ratio < 0.5

    @staticmethod
    def _build_default_workflow(job: Dict[str, Any]) -> Dict[str, Any]:
        template_type = job.get("template_type", "ad")
        if template_type == "transition":
            return {
                "kling_endpoint": "motion_control",
                "prompt_seed": "è½¬åœºæ¨¡æ¿ï¼ŒèŠ‚å¥ç´§å‡‘ï¼ŒåŠ¨æ„Ÿå¼º",
                "negative_prompt": "low quality, blurry, watermark",
                "duration": "5",
                "model_name": "kling-v2-6",
                "cfg_scale": 0.5,
                "mode": "std",
                "shot_type": "wide",
                "camera_move": "whip_pan",
                "transition": "whip_pan",
                "pacing": "fast",
                "style": {"color": "neutral", "light": "hard"},
            }
        return {
            "kling_endpoint": "image_to_video",
            "prompt_seed": "å¹¿å‘Šæ¨¡æ¿ï¼Œäº§å“è´¨æ„Ÿæ¸…æ™°ï¼Œå…‰çº¿é«˜çº§",
            "negative_prompt": "low quality, blurry, watermark",
            "duration": "5",
            "model_name": "kling-v2-6",
            "cfg_scale": 0.5,
            "mode": "std",
            "shot_type": "medium",
            "camera_move": "push",
            "transition": "none",
            "pacing": "medium",
            "style": {"color": "cool", "light": "soft"},
        }

    @staticmethod
    def _encode_jpeg(image: Image.Image, quality: int = 90) -> bytes:
        buffer = io.BytesIO()
        image.save(buffer, format="JPEG", quality=quality)
        return buffer.getvalue()

    @staticmethod
    def _build_thumbnail(image: Image.Image) -> Image.Image:
        thumb = image.copy()
        thumb.thumbnail((DEFAULT_THUMB_MAX, DEFAULT_THUMB_MAX), Image.Resampling.LANCZOS)
        return thumb

    @staticmethod
    def _normalize_clip_ranges(
        clip_ranges: List[Dict[str, Any]],
        total_duration_sec: float,
    ) -> List[Tuple[float, float]]:
        normalized: List[Tuple[float, float]] = []
        for raw in clip_ranges:
            if not isinstance(raw, dict):
                continue

            # å…¼å®¹ç§’çº§å­—æ®µä¸æ¯«ç§’å­—æ®µ
            start = raw.get("start")
            end = raw.get("end")
            if start is None:
                start = raw.get("start_sec")
            if end is None:
                end = raw.get("end_sec")

            start_ms = raw.get("start_ms")
            end_ms = raw.get("end_ms")
            if start_ms is not None:
                start = float(start_ms) / 1000
            if end_ms is not None:
                end = float(end_ms) / 1000

            try:
                start_sec = float(start)
                end_sec = float(end)
            except (TypeError, ValueError):
                continue

            start_sec = max(0.0, min(start_sec, total_duration_sec))
            end_sec = max(0.0, min(end_sec, total_duration_sec))
            if end_sec <= start_sec:
                continue

            normalized.append((start_sec, end_sec))

        normalized.sort(key=lambda item: item[0])
        return normalized

    @staticmethod
    def _allocate_frame_timestamps(
        ranges: List[Tuple[float, float]],
        extract_frames: int,
    ) -> List[float]:
        if extract_frames <= 0 or not ranges:
            return []

        durations = [max(0.0, end - start) for start, end in ranges]
        total = sum(durations)
        if total <= 0:
            return []

        exact_allocations = [(duration / total) * extract_frames for duration in durations]
        allocations = [int(value) for value in exact_allocations]
        remaining = extract_frames - sum(allocations)

        if remaining > 0:
            # æœ€å¤§ä½™æ•°æ³•ï¼Œä¿è¯æ€»æ•°ç²¾ç¡®å¯¹é½ extract_frames
            remainders = sorted(
                [(idx, exact_allocations[idx] - allocations[idx]) for idx in range(len(ranges))],
                key=lambda item: item[1],
                reverse=True,
            )
            for step in range(remaining):
                idx = remainders[step % len(remainders)][0]
                allocations[idx] += 1

        timestamps: List[float] = []
        for (start, end), frame_count in zip(ranges, allocations):
            if frame_count <= 0:
                continue
            span = end - start
            for i in range(frame_count):
                ts = start + span * ((i + 1) / (frame_count + 1))
                timestamps.append(ts)

        return timestamps

    async def _ensure_local_video(self, video_url: str) -> str:
        import hashlib

        url_hash = hashlib.md5(video_url.encode()).hexdigest()[:12]
        temp_dir = tempfile.gettempdir()
        tmp_path = os.path.join(temp_dir, f"tmpl_ingest_{url_hash}.mp4")

        # ğŸ”§ å…³é”®ä¿®å¤: æ¯æ¬¡éƒ½é‡æ–°ä¸‹è½½è§†é¢‘ï¼Œä¸ä½¿ç”¨ç¼“å­˜ï¼ˆé¿å…åˆ†æåˆ°æ—§è§†é¢‘ï¼‰
        # å› ä¸ºä¸åŒçš„ä»»åŠ¡å¯èƒ½å¤ç”¨åŒä¸€ä¸ªä¸´æ—¶æ–‡ä»¶åï¼ˆå¦‚æœ URL å“ˆå¸Œå†²çªï¼‰
        logger.info(f"[TemplateIngest] ä¸‹è½½è§†é¢‘: url_hash={url_hash}, tmp_path={tmp_path}, url={video_url[:80]}...")
        
        # åˆ é™¤æ—§ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(tmp_path):
            logger.info(f"[TemplateIngest] åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶: {tmp_path}")
            os.remove(tmp_path)
        
        download_process = await asyncio.create_subprocess_exec(
            "ffmpeg", "-y",
            "-i", video_url,
            "-c", "copy",
            "-movflags", "+faststart",
            tmp_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        _, stderr = await download_process.communicate()
        if download_process.returncode != 0:
            error_msg = stderr.decode()[:300] if stderr else "Unknown error"
            raise RuntimeError(f"è§†é¢‘ä¸‹è½½å¤±è´¥: {error_msg}")
        
        # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
        file_size = os.path.getsize(tmp_path) if os.path.exists(tmp_path) else 0
        logger.info(f"[TemplateIngest] è§†é¢‘ä¸‹è½½å®Œæˆ: file_size={file_size} bytes")
        
        return tmp_path

    async def _probe_video_duration(self, video_path: str) -> float:
        probe_cmd = [
            "ffprobe", "-v", "error",
            "-show_entries", "format=duration",
            "-of", "default=nw=1:nk=1",
            video_path,
        ]
        probe_process = await asyncio.create_subprocess_exec(
            *probe_cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )
        probe_stdout, _ = await probe_process.communicate()
        duration = float((probe_stdout.decode() or "0").strip() or 0)
        if duration <= 0:
            raise RuntimeError("æ— æ³•è§£æè§†é¢‘æ—¶é•¿")
        return duration

    async def _extract_frames_at_timestamps(
        self,
        video_url: str,
        timestamps: List[float],
    ) -> List[Image.Image]:
        logger.info(f"[TemplateIngest] è½¬åœºæ¨¡æ¿æå–å¸§: timestamps={timestamps}")
        tmp_path = await self._ensure_local_video(video_url)
        if not timestamps:
            return []

        frames: List[Image.Image] = []
        frames_dir = tempfile.mkdtemp(prefix="tmpl_ingest_frames_at_ts_")
        try:
            for idx, ts in enumerate(timestamps):
                frame_path = os.path.join(frames_dir, f"frame_{idx:03d}.jpg")
                extract_cmd = [
                    "ffmpeg", "-y",
                    "-ss", str(max(0.0, ts)),
                    "-i", tmp_path,
                    "-vframes", "1",
                    "-q:v", "2",
                    frame_path,
                ]
                extract_process = await asyncio.create_subprocess_exec(
                    *extract_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                await extract_process.communicate()
                if os.path.exists(frame_path):
                    frame_img = Image.open(frame_path).convert("RGB")
                    logger.info(f"[TemplateIngest] è½¬åœºå¸§ {idx}: ts={ts}s, size={frame_img.size}")
                    frames.append(frame_img)
                else:
                    logger.warning(f"[TemplateIngest] âš ï¸ è½¬åœºå¸§ {idx} æå–å¤±è´¥")
        finally:
            try:
                import shutil
                shutil.rmtree(frames_dir)
            except Exception:
                pass
        return frames

    async def _extract_frames(
        self,
        video_url: str,
        extract_frames: int,
        clip_ranges: Optional[List[Dict[str, Any]]] = None,
    ) -> List[Image.Image]:
        logger.info(f"[TemplateIngest] å¼€å§‹æå–å¸§: url={video_url[:80]}..., extract_frames={extract_frames}")
        tmp_path = await self._ensure_local_video(video_url)
        duration = await self._probe_video_duration(tmp_path)
        logger.info(f"[TemplateIngest] è§†é¢‘æ—¶é•¿: {duration}s, æœ¬åœ°è·¯å¾„: {tmp_path}")

        normalized_ranges = self._normalize_clip_ranges(clip_ranges or [], duration)
        timestamps = self._allocate_frame_timestamps(normalized_ranges, extract_frames)
        if not timestamps:
            interval = duration / (extract_frames + 1)
            timestamps = [interval * (i + 1) for i in range(extract_frames)]
        logger.info(f"[TemplateIngest] æå–æ—¶é—´æˆ³: {timestamps}")

        frames: List[Image.Image] = []

        frames_dir = tempfile.mkdtemp(prefix="tmpl_ingest_frames_")
        try:
            for idx, ts in enumerate(timestamps):
                frame_path = os.path.join(frames_dir, f"frame_{idx:03d}.jpg")
                extract_cmd = [
                    "ffmpeg", "-y",
                    "-ss", str(ts),
                    "-i", tmp_path,
                    "-vframes", "1",
                    "-q:v", "2",
                    frame_path,
                ]
                extract_process = await asyncio.create_subprocess_exec(
                    *extract_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                await extract_process.communicate()
                if os.path.exists(frame_path):
                    frame_img = Image.open(frame_path).convert("RGB")
                    logger.info(f"[TemplateIngest] æå–å¸§ {idx}: ts={ts}s, size={frame_img.size}")
                    frames.append(frame_img)
        finally:
            try:
                import shutil
                shutil.rmtree(frames_dir)
            except Exception:
                pass

        return frames


_template_ingest_service: Optional[TemplateIngestService] = None


def get_template_ingest_service() -> TemplateIngestService:
    global _template_ingest_service
    if _template_ingest_service is None:
        _template_ingest_service = TemplateIngestService()
    return _template_ingest_service
