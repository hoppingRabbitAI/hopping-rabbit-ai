"""
æ™ºèƒ½ä¸€é”®æˆç‰‡ V2 - SmartAnalyzer æœåŠ¡
ä¸€ç«™å¼ LLM æ™ºèƒ½åˆ†æï¼šè„šæœ¬å¯¹é½ã€åºŸè¯è¯†åˆ«ã€é‡å¤æ£€æµ‹ã€é£æ ¼åˆ†æ

æ ¸å¿ƒè®¾è®¡ç†å¿µ:
1. LLM ä¼˜å…ˆ - ä¸€æ¬¡è°ƒç”¨å®Œæˆæ‰€æœ‰åˆ†æä»»åŠ¡
2. ç”¨æˆ·æ„ŸçŸ¥åº¦ - æ¸…æ™°çš„é˜¶æ®µè¿›åº¦
3. æ•ˆç‡ä¸ºç‹ - æ¨èä¼˜å…ˆï¼Œä¸€é”®æ¥å—
"""

import json
import logging
from enum import Enum
from typing import Optional, Callable, List, Dict, Any
from uuid import uuid4
from datetime import datetime
from pydantic import BaseModel, Field

from .llm import llm_service
from .supabase_client import supabase

logger = logging.getLogger(__name__)


# ============================================
# æ•°æ®æ¨¡å‹
# ============================================

class ProcessingStage(str, Enum):
    """å¤„ç†é˜¶æ®µæšä¸¾"""
    PENDING = "pending"
    UPLOADING = "uploading"
    TRANSCRIBING = "transcribing"
    ANALYZING = "analyzing"
    GENERATING = "generating"
    COMPLETED = "completed"
    FAILED = "failed"
    
    @property
    def message(self) -> str:
        """è·å–é˜¶æ®µæè¿°"""
        messages = {
            "pending": "ç­‰å¾…å¤„ç†...",
            "uploading": "ğŸ“¤ ä¸Šä¼ ä¸­...",
            "transcribing": "ğŸ¤ è¯­éŸ³è½¬å†™ä¸­...",
            "analyzing": "ğŸ§  AI æ™ºèƒ½åˆ†æä¸­...",
            "generating": "âœ¨ ç”Ÿæˆæ¨èæ–¹æ¡ˆ...",
            "completed": "âœ… åˆ†æå®Œæˆï¼",
            "failed": "âŒ å¤„ç†å¤±è´¥"
        }
        return messages.get(self.value, "å¤„ç†ä¸­...")
    
    @property
    def progress(self) -> int:
        """è·å–é˜¶æ®µå¯¹åº”çš„è¿›åº¦ç™¾åˆ†æ¯”"""
        progress_map = {
            "pending": 0,
            "uploading": 10,
            "transcribing": 30,
            "analyzing": 60,
            "generating": 85,
            "completed": 100,
            "failed": 0
        }
        return progress_map.get(self.value, 0)


class QualityScores(BaseModel):
    """ç‰‡æ®µè´¨é‡è¯„åˆ†"""
    clarity: float = Field(default=0.8, ge=0, le=1, description="æ¸…æ™°åº¦")
    fluency: float = Field(default=0.8, ge=0, le=1, description="æµç•…åº¦")
    emotion: float = Field(default=0.8, ge=0, le=1, description="æƒ…æ„Ÿè¡¨è¾¾")
    speed: float = Field(default=0.8, ge=0, le=1, description="è¯­é€Ÿé€‚ä¸­ç¨‹åº¦")


# ============================================
# åˆ†ç±»æ˜ å°„ï¼ˆç»Ÿä¸€å¤„ç†ï¼‰
# ============================================

# æ ‡å‡†åˆ†ç±»åˆ—è¡¨
STANDARD_CLASSIFICATIONS = {
    "breath", "filler", "noise", "repeat", "dead_air", "hesitation", "long_pause",
    "matched", "deviation", "improvisation", "valuable", "uncertain", "delete"
}

# æ‰€æœ‰å¯èƒ½è¾“å…¥ -> æ ‡å‡†åˆ†ç±»ï¼ˆç»Ÿä¸€æ˜ å°„è¡¨ï¼Œå¤§å°å†™ä¸æ•æ„Ÿï¼‰
CLASSIFICATION_MAP: Dict[str, str] = {
    # === æ ‡å‡†åˆ†ç±»ï¼ˆç›´æ¥è¿”å›ï¼‰ ===
    "breath": "breath",
    "filler": "filler", 
    "noise": "noise",
    "repeat": "repeat",
    "dead_air": "dead_air",
    "hesitation": "hesitation",
    "long_pause": "long_pause",
    "matched": "matched",
    "deviation": "deviation",
    "improvisation": "improvisation",
    "valuable": "valuable",
    "uncertain": "uncertain",
    "delete": "delete",
    
    # === è‹±æ–‡å˜ä½“ ===
    "empty": "dead_air",
    "silence": "dead_air",
    "pause": "long_pause",
    "stutter": "hesitation",
    "irrelevant": "deviation",
    
    # === Action è¢«è¯¯ç”¨ä¸º classification ===
    "keep": "matched",
    "choose": "repeat",
    
    # === ä¸­æ–‡åˆ†ç±» ===
    "åºŸè¯": "filler",
    "å£ç™–": "filler",
    "å™ªéŸ³": "noise",
    "é™é»˜": "dead_air",
    "é‡å¤": "repeat",
    "çŠ¹è±«": "hesitation",
    "åœé¡¿": "long_pause",
    "æ¢æ°”": "breath",
    "åŒ¹é…": "matched",
    "æœ‰æ•ˆ": "matched",
    "åç¦»": "deviation",
    "å³å…´": "improvisation",
    "æœ‰ä»·å€¼": "valuable",
    "å¾…ç¡®è®¤": "uncertain",
    "åˆ é™¤": "delete",
}

def normalize_classification(raw_classification: str) -> str:
    """å°† LLM è¿”å›çš„åˆ†ç±»æ ‡å‡†åŒ–"""
    if not raw_classification:
        return "matched"
    
    # ç»Ÿä¸€æŸ¥è¡¨ï¼ˆå…ˆå°è¯•åŸå€¼ï¼Œå†å°è¯•å°å†™ï¼‰
    key = raw_classification.strip()
    if key in CLASSIFICATION_MAP:
        return CLASSIFICATION_MAP[key]
    
    lower_key = key.lower()
    if lower_key in CLASSIFICATION_MAP:
        return CLASSIFICATION_MAP[lower_key]
    
    # æœªçŸ¥åˆ†ç±»ï¼Œè®°å½•è­¦å‘Š
    logger.warning(f"æœªçŸ¥çš„åˆ†ç±»ç±»å‹: '{raw_classification}'ï¼Œå·²æ˜ å°„ä¸º 'matched'")
    return "matched"


class AnalyzedSegment(BaseModel):
    """åˆ†æåçš„ç‰‡æ®µ"""
    id: str
    start: float
    end: float
    text: str
    
    # åˆ†ç±» (LLM è¾“å‡º)
    action: str = Field(description="keep | delete | choose")
    classification: str = Field(description="matched | deviation | filler | repeat | improvisation")
    confidence: float = Field(default=0.9, ge=0, le=1)
    
    # å…³è”ä¿¡æ¯
    repeat_group_id: Optional[str] = None
    script_match: Optional[str] = None
    is_recommended: bool = False
    asset_id: Optional[str] = None  # æ¥æºç´ æ IDï¼ˆå¤šç´ æåœºæ™¯ï¼‰
    
    # åºŸè¯è¯
    filler_words: List[str] = []
    reason: Optional[str] = None
    
    # è´¨é‡è¯„åˆ†
    quality_score: float = Field(default=0.8, ge=0, le=1)
    quality_scores: Optional[QualityScores] = None
    quality_notes: Optional[str] = None


class RepeatGroup(BaseModel):
    """é‡å¤ç‰‡æ®µç»„"""
    id: str
    intent: str = Field(description="è¿™ç»„é‡å¤ç‰‡æ®µæƒ³è¡¨è¾¾çš„å†…å®¹")
    script_match: Optional[str] = None
    segment_ids: List[str]
    recommended_id: str
    recommend_reason: str


class ZoomRecommendation(BaseModel):
    """ç¼©æ”¾æ¨è"""
    rhythm: str = Field(description="punchy | smooth | minimal")
    scale_range: List[float] = Field(default=[1.0, 1.2])
    duration_ms: int = 500
    easing: str = "ease_in_out"
    triggers: List[str] = Field(default=["key_point", "new_topic"])


class StyleAnalysis(BaseModel):
    """é£æ ¼åˆ†æç»“æœ"""
    detected_style: str = Field(description="energetic_vlog | tutorial | storytelling | news_commentary")
    confidence: float = Field(default=0.8, ge=0, le=1)
    reasoning: str
    zoom_recommendation: ZoomRecommendation


class AnalysisSummary(BaseModel):
    """åˆ†æç»Ÿè®¡æ‘˜è¦"""
    total_segments: int = 0
    keep_count: int = 0
    delete_count: int = 0
    choose_count: int = 0
    repeat_groups_count: int = 0
    estimated_duration_after: float = 0.0
    reduction_percent: float = 0.0
    script_coverage: Optional[float] = None


class AnalysisResult(BaseModel):
    """å®Œæ•´åˆ†æç»“æœ"""
    segments: List[AnalyzedSegment]
    repeat_groups: List[RepeatGroup] = []
    style_analysis: Optional[StyleAnalysis] = None
    summary: AnalysisSummary


# ============================================
# Super Prompt
# ============================================

SUPER_ANALYSIS_PROMPT = """# è§’è‰²
ä½ æ˜¯ä¸“ä¸šçš„å£æ’­è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚ä½ éœ€è¦ä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰åˆ†æä»»åŠ¡ï¼Œè¾“å‡ºç»“æ„åŒ–çš„åˆ†æç»“æœã€‚

# è¾“å…¥æ•°æ®

## ASR è½¬å†™ç»“æœ (å¸¦æ—¶é—´æˆ³)
```json
{transcript_json}
```

## ç”¨æˆ·è„šæœ¬ (å¯é€‰ï¼Œå¦‚æœç”¨æˆ·æä¾›äº†)
{script_or_none}

## éŸ³é¢‘ç‰¹å¾
- è§†é¢‘æ—¶é•¿: {duration}ç§’
- å¹³å‡è¯­é€Ÿ: {speech_rate} å­—/åˆ†é’Ÿ
- åœé¡¿åˆ†å¸ƒ: {pause_info}

# ä½ çš„ä»»åŠ¡ (ä¸€æ¬¡æ€§å®Œæˆä»¥ä¸‹æ‰€æœ‰åˆ†æ)

## ä»»åŠ¡1: ç‰‡æ®µåˆ†ç±»
å¯¹è¾“å…¥ä¸­çš„**æ¯ä¸€ä¸ª** ASR ç‰‡æ®µè¿›è¡Œåˆ†ç±»ï¼ˆå¿…é¡»åŒ…å«æ‰€æœ‰ç‰‡æ®µ IDï¼Œä¸å¯é—æ¼ï¼‰ï¼š
- `keep` - æœ‰æ•ˆå†…å®¹ï¼Œç›´æ¥ä¿ç•™
- `delete` - åºŸè¯/å£ç™–ï¼Œå»ºè®®åˆ é™¤
- `choose` - éœ€è¦ç”¨æˆ·é€‰æ‹©ï¼ˆé€šå¸¸æ˜¯é‡å¤ç‰‡æ®µï¼‰

## ä»»åŠ¡2: åºŸè¯è¯†åˆ«
è¯†åˆ«ä»¥ä¸‹ç±»å‹çš„åºŸè¯ï¼š
- å£ç™–è¯ï¼šå—¯ã€å•Šã€é‚£ä¸ªã€å°±æ˜¯è¯´ã€å¯¹å§
- æ— æ„ä¹‰é‡å¤ï¼šåŒä¸€ä¸ªè¯è¿è¯´ä¸¤é
- ä¸­æ–­é‡å¯ï¼šè¯´åˆ°ä¸€åŠé‡æ–°è¯´
- è‡ªæˆ‘çº æ­£ï¼šå£è¯¯åçš„çº æ­£ï¼ˆä¿ç•™çº æ­£åçš„ç‰ˆæœ¬ï¼‰

## ä»»åŠ¡3: é‡å¤ç‰‡æ®µæ£€æµ‹
è¯†åˆ«ç”¨æˆ·å¯¹åŒä¸€å¥è¯å½•äº†å¤šéçš„æƒ…å†µï¼š
- æ ‡è®°ä¸ºåŒä¸€ä¸ª repeat_group
- æ¨èæœ€ä½³ç‰ˆæœ¬ï¼ˆè¯­é€Ÿè‡ªç„¶ã€æ— å£è¯¯ã€æƒ…ç»ªåˆ°ä½ï¼‰
- è¯´æ˜æ¨èç†ç”±

## ä»»åŠ¡4: è„šæœ¬å¯¹é½ (å¦‚æœæœ‰è„šæœ¬)
- æ‰¾å‡ºè½¬å†™å†…å®¹ä¸è„šæœ¬çš„å¯¹åº”å…³ç³»
- æ ‡è®°ï¼šmatched(åŒ¹é…) / deviation(åç¦») / improvisation(å³å…´)
- è®¡ç®—è„šæœ¬å®Œæˆåº¦

## ä»»åŠ¡5: é£æ ¼åˆ†æä¸ç¼©æ”¾æ¨è
åˆ¤æ–­è§†é¢‘é£æ ¼å¹¶æ¨èç¼©æ”¾å‚æ•°ï¼š
- energetic_vlog: æ´»åŠ›vlogï¼Œç¼©æ”¾å¿«é€Ÿæœ‰åŠ› (300ms, 1.0-1.4x)
- tutorial: æ•™ç¨‹è®²è§£ï¼Œç¼©æ”¾å¹³æ»‘ç¨³å®š (500ms, 1.0-1.2x)  
- storytelling: æ•…äº‹å™è¿°ï¼Œç¼©æ”¾ç¼“æ…¢æ²‰æµ¸ (800ms, 1.0-1.15x)
- news_commentary: æ–°é—»è¯„è®ºï¼Œç¼©æ”¾ä¸­ç­‰å¼ºè°ƒ (400ms, 1.0-1.25x)

# è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼JSON)
```json
{{
  "segments": [
    {{
      "id": "seg_001",
      "start": 0.0,
      "end": 3.2,
      "text": "å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯xxx",
      "action": "keep",
      "classification": "matched",
      "confidence": 0.95,
      "script_match": "å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯xxx",
      "repeat_group_id": null,
      "filler_words": [],
      "quality_score": 0.9
    }},
    {{
      "id": "seg_002",
      "start": 3.2,
      "end": 4.1,
      "text": "å—¯é‚£ä¸ª",
      "action": "delete",
      "classification": "filler",
      "confidence": 0.98,
      "filler_words": ["å—¯", "é‚£ä¸ª"],
      "reason": "çº¯å£ç™–è¯ï¼Œæ— å®é™…å†…å®¹"
    }},
    {{
      "id": "seg_003",
      "start": 4.1,
      "end": 8.5,
      "text": "ä»Šå¤©ç»™å¤§å®¶åˆ†äº«ä¸€ä¸ªæŠ€å·§",
      "action": "choose",
      "classification": "repeat",
      "confidence": 0.92,
      "repeat_group_id": "group_intro",
      "is_recommended": false,
      "quality_score": 0.75,
      "quality_notes": "è¯­é€Ÿåå¿«ï¼Œæœ‰è½»å¾®å£è¯¯"
    }}
  ],
  
  "repeat_groups": [
    {{
      "id": "group_intro",
      "intent": "å¼€åœºä»‹ç»ä»Šå¤©çš„ä¸»é¢˜",
      "script_match": "ä»Šå¤©ç»™å¤§å®¶åˆ†äº«ä¸€ä¸ªæŠ€å·§",
      "segment_ids": ["seg_003", "seg_006", "seg_009"],
      "recommended_id": "seg_006",
      "recommend_reason": "è¯­é€Ÿé€‚ä¸­ï¼Œè¡¨è¾¾æµç•…ï¼Œæƒ…ç»ªè‡ªç„¶"
    }}
  ],
  
  "style_analysis": {{
    "detected_style": "tutorial",
    "confidence": 0.88,
    "reasoning": "è¯­é€Ÿ180å­—/åˆ†é’Ÿé€‚ä¸­ï¼Œåœé¡¿è§„å¾‹ï¼Œå†…å®¹æœ‰é€»è¾‘ç»“æ„",
    "zoom_recommendation": {{
      "rhythm": "smooth",
      "scale_range": [1.0, 1.2],
      "duration_ms": 500,
      "easing": "ease_in_out",
      "triggers": ["key_point", "new_topic"]
    }}
  }},
  
  "summary": {{
    "total_segments": 25,
    "keep_count": 18,
    "delete_count": 5,
    "choose_count": 2,
    "repeat_groups_count": 1,
    "script_coverage": 0.92,
    "estimated_duration_after": 180,
    "reduction_percent": 28
  }}
}}
```

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""


# ============================================
# SmartAnalyzer æœåŠ¡
# ============================================

class SmartAnalyzer:
    """ä¸€ç«™å¼æ™ºèƒ½åˆ†æå™¨ - LLM ä¼˜å…ˆ"""
    
    def __init__(self):
        pass
    
    async def analyze(
        self,
        transcript_segments: List[Dict[str, Any]],
        script: Optional[str] = None,
        audio_features: Optional[Dict[str, Any]] = None,
        video_duration: float = 0
    ) -> AnalysisResult:
        """
        ä¸€æ¬¡ LLM è°ƒç”¨å®Œæˆæ‰€æœ‰åˆ†æ
        
        Args:
            transcript_segments: ASR è½¬å†™ç»“æœ
            script: ç”¨æˆ·è„šæœ¬ï¼ˆå¯é€‰ï¼‰
            audio_features: éŸ³é¢‘ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            video_duration: è§†é¢‘æ—¶é•¿
            
        Returns:
            AnalysisResult: å®Œæ•´åˆ†æç»“æœ
        """
        logger.info(f"ğŸ§  SmartAnalyzer: å¼€å§‹åˆ†æ {len(transcript_segments)} ä¸ªç‰‡æ®µ")
        
        # æ£€æŸ¥ LLM é…ç½®
        if not is_llm_configured():
            logger.warning("âš ï¸ LLM æœªé…ç½®ï¼Œè¿”å›é»˜è®¤åˆ†æç»“æœ")
            return self._generate_fallback_result(transcript_segments)
        
        # æ„å»ºè¾“å…¥
        # æ³¨æ„ï¼šASR segments æ—¶é—´æ˜¯æ¯«ç§’ï¼Œä½† LLM prompt ç¤ºä¾‹ä½¿ç”¨ç§’
        # ä¸ºäº†ä¸ç¤ºä¾‹ä¿æŒä¸€è‡´ï¼Œè½¬æ¢ä¸ºç§’ä¼ ç»™ LLM
        transcript_json = json.dumps([{
            "id": f"seg_{i:03d}",
            "start": round(seg.get("start", 0) / 1000, 3),  # æ¯«ç§’ -> ç§’
            "end": round(seg.get("end", 0) / 1000, 3),      # æ¯«ç§’ -> ç§’
            "text": seg.get("text", "")
        } for i, seg in enumerate(transcript_segments)], ensure_ascii=False, indent=2)
        
        script_or_none = f'"""\n{script}\n"""' if script else "æ— ï¼ˆç”¨æˆ·æœªæä¾›è„šæœ¬ï¼‰"
        
        speech_rate = audio_features.get("speech_rate", "æœªçŸ¥") if audio_features else "æœªçŸ¥"
        pause_info = audio_features.get("pause_summary", "æœªçŸ¥") if audio_features else "æœªçŸ¥"
        
        # æ„å»º Prompt
        prompt = SUPER_ANALYSIS_PROMPT.format(
            transcript_json=transcript_json,
            script_or_none=script_or_none,
            duration=video_duration,
            speech_rate=speech_rate,
            pause_info=pause_info
        )
        
        logger.info(f"ğŸ“ Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # æ£€æŸ¥ LLM æ˜¯å¦é…ç½®
        if not llm_service.is_configured():
            logger.warning("âš ï¸ LLM æœªé…ç½®ï¼Œè¿”å›é»˜è®¤ç»“æœ")
            return self._generate_fallback_result(transcript_segments)
        
        # ä¸€æ¬¡ LLM è°ƒç”¨
        response = await llm_service.call(
            prompt=prompt,
            system_prompt="ä½ æ˜¯ä¸“ä¸šçš„å£æ’­è§†é¢‘å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«åºŸè¯ã€é‡å¤ç‰‡æ®µå’Œåˆ†æè§†é¢‘é£æ ¼ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºã€‚",
        )
        
        if not response:
            logger.error("âŒ LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ")
            return self._generate_fallback_result(transcript_segments)
        
        # è§£æç»“æœ
        return self._parse_result(response, transcript_segments)
    
    def _parse_result(
        self, 
        response: str, 
        original_segments: List[Dict]
    ) -> AnalysisResult:
        """è§£æ LLM å“åº”"""
        try:
            # æå– JSON
            json_str = response
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0]
            
            data = json.loads(json_str.strip())
            
            # è§£æç‰‡æ®µ - ç¡®ä¿ä»¥åŸå§‹ç‰‡æ®µä¸ºåŸºå‡†ï¼Œé˜²æ­¢ LLM é—æ¼
            llm_segments_map = {s.get("id"): s for s in data.get("segments", []) if s.get("id")}
            segments = []
            
            for i, original_seg in enumerate(original_segments):
                seg_id = f"seg_{i:03d}"
                
                # é»˜è®¤å€¼ï¼ˆä»åŸå§‹ç‰‡æ®µè·å–ï¼‰
                text = original_seg.get("text", "")
                # è½¬æ¢ä¸ºç§’ï¼ˆASR è¿”å›çš„ start/end æ˜¯æ¯«ç§’ï¼‰
                start = round(original_seg.get("start", 0) / 1000, 3)
                end = round(original_seg.get("end", 0) / 1000, 3)
                # è·å–æ¥æºç´ æ IDï¼ˆå¤šç´ æåœºæ™¯ï¼‰
                asset_id = original_seg.get("_asset_id")
                
                # å°è¯•è·å– LLM åˆ†æç»“æœ
                llm_data = llm_segments_map.get(seg_id)
                
                # é»˜è®¤å€¼
                action = "keep"
                classification = "matched"
                confidence = 0.9
                repeat_group_id = None
                script_match = None
                is_recommended = False
                filler_words = []
                reason = None
                quality_score = 0.8
                quality_notes = None
                
                if llm_data:
                    # ä½¿ç”¨ LLM æ•°æ®è¦†ç›–
                    action = llm_data.get("action", "keep")
                    # â˜… å…³é”®ï¼šæ ‡å‡†åŒ–åˆ†ç±»ï¼ˆä¸­æ–‡ -> è‹±æ–‡ï¼‰
                    raw_classification = llm_data.get("classification", "matched")
                    classification = normalize_classification(raw_classification)
                    confidence = llm_data.get("confidence", 0.9)
                    repeat_group_id = llm_data.get("repeat_group_id")
                    script_match = llm_data.get("script_match")
                    is_recommended = llm_data.get("is_recommended", False)
                    filler_words = llm_data.get("filler_words", [])
                    reason = llm_data.get("reason")
                    quality_score = llm_data.get("quality_score", 0.8)
                    quality_notes = llm_data.get("quality_notes")
                
                try:
                    seg = AnalyzedSegment(
                        id=seg_id,
                        start=start,
                        end=end,
                        text=text,
                        action=action,
                        classification=classification,
                        confidence=confidence,
                        repeat_group_id=repeat_group_id,
                        script_match=script_match,
                        is_recommended=is_recommended,
                        asset_id=asset_id,
                        filler_words=filler_words,
                        reason=reason,
                        quality_score=quality_score,
                        quality_notes=quality_notes
                    )
                    segments.append(seg)
                except Exception as e:
                    logger.warning(f"æ„å»ºç‰‡æ®µå¤±è´¥: {e}, ID: {seg_id}")
            
            # è§£æé‡å¤ç»„
            repeat_groups = []
            for group_data in data.get("repeat_groups", []):
                try:
                    group = RepeatGroup(
                        id=group_data.get("id", f"group_{len(repeat_groups):03d}"),
                        intent=group_data.get("intent", ""),
                        script_match=group_data.get("script_match"),
                        segment_ids=group_data.get("segment_ids", []),
                        recommended_id=group_data.get("recommended_id", ""),
                        recommend_reason=group_data.get("recommend_reason", "")
                    )
                    repeat_groups.append(group)
                except Exception as e:
                    logger.warning(f"è§£æé‡å¤ç»„å¤±è´¥: {e}")
            
            # è§£æé£æ ¼åˆ†æ
            style_analysis = None
            style_data = data.get("style_analysis")
            if style_data and isinstance(style_data, dict):
                try:
                    zoom_data = style_data.get("zoom_recommendation") or {}
                    zoom_rec = ZoomRecommendation(
                        rhythm=zoom_data.get("rhythm") or "smooth",
                        scale_range=zoom_data.get("scale_range") or [1.0, 1.2],
                        duration_ms=zoom_data.get("duration_ms") or 500,
                        easing=zoom_data.get("easing") or "ease_in_out",
                        triggers=zoom_data.get("triggers") or ["key_point"]
                    )
                    style_analysis = StyleAnalysis(
                        detected_style=style_data.get("detected_style") or "tutorial",
                        confidence=style_data.get("confidence") or 0.8,
                        reasoning=style_data.get("reasoning") or "",
                        zoom_recommendation=zoom_rec
                    )
                except Exception as e:
                    logger.warning(f"è§£æé£æ ¼åˆ†æå¤±è´¥: {e}")
            
            # è§£ææ‘˜è¦ - ç¡®ä¿æ•°å€¼å­—æ®µä¸ä¸º None
            summary_data = data.get("summary") or {}
            summary = AnalysisSummary(
                total_segments=summary_data.get("total_segments") or len(segments),
                keep_count=summary_data.get("keep_count") or 0,
                delete_count=summary_data.get("delete_count") or 0,
                choose_count=summary_data.get("choose_count") or 0,
                repeat_groups_count=summary_data.get("repeat_groups_count") or len(repeat_groups),
                estimated_duration_after=float(summary_data.get("estimated_duration_after") or 0.0),
                reduction_percent=float(summary_data.get("reduction_percent") or 0.0),
                script_coverage=summary_data.get("script_coverage")
            )
            
            logger.info(f"âœ… åˆ†æå®Œæˆ: {len(segments)} ç‰‡æ®µ, {len(repeat_groups)} é‡å¤ç»„")
            
            return AnalysisResult(
                segments=segments,
                repeat_groups=repeat_groups,
                style_analysis=style_analysis,
                summary=summary
            )
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
            logger.error(f"åŸå§‹å“åº”: {response[:500]}...")
            return self._generate_fallback_result(original_segments)
        except Exception as e:
            logger.error(f"âŒ è§£æç»“æœå¤±è´¥: {e}")
            return self._generate_fallback_result(original_segments)
    
    def _generate_fallback_result(
        self, 
        transcript_segments: List[Dict]
    ) -> AnalysisResult:
        """ç”Ÿæˆé™çº§ç»“æœï¼ˆLLM ä¸å¯ç”¨æ—¶ï¼‰"""
        segments = []
        for i, seg in enumerate(transcript_segments):
            text = seg.get("text", "")
            
            # ç®€å•è§„åˆ™è¯†åˆ«åºŸè¯
            filler_words = []
            is_filler = False
            for word in ["å—¯", "å•Š", "é‚£ä¸ª", "å°±æ˜¯", "å¯¹å§", "ç„¶å"]:
                if word in text:
                    filler_words.append(word)
            
            # å¦‚æœæ•´å¥éƒ½æ˜¯è¯­æ°”è¯
            if len(text) < 5 and filler_words:
                is_filler = True
            
            segments.append(AnalyzedSegment(
                id=f"seg_{i:03d}",
                start=round(seg.get("start", 0) / 1000, 3),  # æ¯«ç§’ -> ç§’
                end=round(seg.get("end", 0) / 1000, 3),      # æ¯«ç§’ -> ç§’
                text=text,
                action="delete" if is_filler else "keep",
                classification="filler" if is_filler else "matched",
                confidence=0.7 if is_filler else 0.9,
                filler_words=filler_words,
                asset_id=seg.get("_asset_id")  # ä¼ é€’æ¥æºç´ æ ID
            ))
        
        keep_count = len([s for s in segments if s.action == "keep"])
        delete_count = len([s for s in segments if s.action == "delete"])
        
        # è®¡ç®—ä¿ç•™æ—¶é•¿
        kept_duration = sum(s.end - s.start for s in segments if s.action == "keep")
        total_duration = sum(s.end - s.start for s in segments)
        reduction_percent = ((total_duration - kept_duration) / total_duration * 100) if total_duration > 0 else 0.0
        
        return AnalysisResult(
            segments=segments,
            repeat_groups=[],
            style_analysis=None,
            summary=AnalysisSummary(
                total_segments=len(segments),
                keep_count=keep_count,
                delete_count=delete_count,
                choose_count=0,
                repeat_groups_count=0,
                estimated_duration_after=kept_duration,
                reduction_percent=reduction_percent
            )
        )


# ============================================
# è¿›åº¦ç®¡ç†
# ============================================

async def update_analysis_progress(
    analysis_id: str,
    stage: ProcessingStage,
    message: Optional[str] = None
) -> None:
    """æ›´æ–°åˆ†æè¿›åº¦"""
    try:
        update_data = {
            "processing_stage": stage.value,
            "processing_progress": stage.progress,
            "processing_message": message or stage.message,
            "updated_at": datetime.utcnow().isoformat()
        }
        
        if stage == ProcessingStage.COMPLETED:
            update_data["status"] = "completed"
            update_data["completed_at"] = datetime.utcnow().isoformat()
        elif stage == ProcessingStage.FAILED:
            update_data["status"] = "failed"
        elif stage != ProcessingStage.PENDING:
            update_data["status"] = "processing"
        
        supabase.table("content_analyses").update(update_data).eq("id", analysis_id).execute()
        
        logger.info(f"ğŸ“Š è¿›åº¦æ›´æ–°: {stage.value} ({stage.progress}%) - {message or stage.message}")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°è¿›åº¦å¤±è´¥: {e}")


async def get_analysis_progress(analysis_id: str, user_id: str = None) -> Optional[Dict]:
    """è·å–åˆ†æè¿›åº¦"""
    try:
        query = supabase.table("content_analyses").select(
            "id, processing_stage, processing_progress, processing_message, status"
        ).eq("id", analysis_id)
        
        # å¦‚æœæä¾›äº† user_idï¼Œæ·»åŠ æƒé™è¿‡æ»¤
        if user_id:
            query = query.eq("user_id", user_id)
        
        result = query.single().execute()
        
        if result.data:
            return {
                "id": result.data["id"],
                "stage": result.data["processing_stage"],
                "progress": result.data["processing_progress"],
                "message": result.data["processing_message"],
                "status": result.data["status"]
            }
        return None
    except Exception as e:
        logger.error(f"âŒ è·å–è¿›åº¦å¤±è´¥: {e}")
        return None


# ============================================
# åˆ›å»ºåˆ†æä»»åŠ¡
# ============================================

async def create_content_analysis(
    project_id: str,
    user_id: str,
    script: Optional[str] = None
) -> str:
    """åˆ›å»ºå†…å®¹åˆ†æè®°å½•"""
    analysis_id = str(uuid4())
    
    data = {
        "id": analysis_id,
        "project_id": project_id,
        "user_id": user_id,
        "mode": "with_script" if script else "without_script",
        "status": "pending",
        "processing_stage": ProcessingStage.PENDING.value,
        "processing_progress": 0,
        "processing_message": ProcessingStage.PENDING.message,
        "created_at": datetime.utcnow().isoformat(),
        "updated_at": datetime.utcnow().isoformat()
    }
    
    supabase.table("content_analyses").insert(data).execute()
    
    logger.info(f"âœ… åˆ›å»ºåˆ†æä»»åŠ¡: {analysis_id}")
    
    return analysis_id


async def save_analysis_result(
    analysis_id: str,
    result: AnalysisResult
) -> None:
    """ä¿å­˜åˆ†æç»“æœ"""
    try:
        update_data = {
            "segments": [seg.model_dump() for seg in result.segments],
            "repeat_groups": [g.model_dump() for g in result.repeat_groups],
            "style_analysis": result.style_analysis.model_dump() if result.style_analysis else None,
            "summary": result.summary.model_dump(),
            "status": "completed",
            "processing_stage": ProcessingStage.COMPLETED.value,
            "processing_progress": 100,
            "processing_message": ProcessingStage.COMPLETED.message,
            "completed_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat()
        }
        
        supabase.table("content_analyses").update(update_data).eq("id", analysis_id).execute()
        
        logger.info(f"âœ… ä¿å­˜åˆ†æç»“æœ: {analysis_id}")
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
        raise


# å¯¼å‡º
smart_analyzer = SmartAnalyzer()

__all__ = [
    "SmartAnalyzer",
    "smart_analyzer",
    "ProcessingStage",
    "AnalysisResult",
    "AnalyzedSegment",
    "RepeatGroup",
    "StyleAnalysis",
    "AnalysisSummary",
    "update_analysis_progress",
    "get_analysis_progress",
    "create_content_analysis",
    "save_analysis_result",
    # åˆ†ç±»æ˜ å°„
    "normalize_classification",
    "CLASSIFICATION_MAP",
    "STANDARD_CLASSIFICATIONS",
]
