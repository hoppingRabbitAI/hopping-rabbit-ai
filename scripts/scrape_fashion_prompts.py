#!/usr/bin/env python3
"""
Fashion Prompt é‡‡é›† & æ¸…æ´—è„šæœ¬

æ•°æ®æºï¼š
  1. Lexica.art API â€” 500ä¸‡+ SD promptï¼ŒæŒ‰å…³é”®è¯æœç´¢
  2. HuggingFace Datasets â€” Falah ç³»åˆ— fashion prompt æ•°æ®é›†

è¾“å‡ºï¼š
  scripts/output/fashion_prompts.json â€” å»é‡+åˆ†ç±»åçš„ prompt åº“

ç”¨æ³•ï¼š
  pip install requests datasets
  python scripts/scrape_fashion_prompts.py

"""

import json
import os
import re
import time
import hashlib
from pathlib import Path
from typing import Optional

# ============================================
# é…ç½®
# ============================================

OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_FILE = OUTPUT_DIR / "fashion_prompts.json"

# Lexica æœç´¢å…³é”®è¯ï¼ˆæ—¶å°šå‚ç±»ï¼‰
LEXICA_QUERIES = [
    "fashion photography portrait",
    "fashion editorial model",
    "outfit lookbook photography",
    "street style fashion",
    "fashion magazine cover",
    "clothing product photography white background",
    "fashion model runway",
    "fashion portrait golden hour",
    "neon fashion portrait cyberpunk",
    "vintage fashion film photography",
    "korean fashion minimalist",
    "french elegant fashion",
    "fashion flat lay outfit",
    "fashion studio lighting portrait",
    "luxury fashion editorial",
    "casual streetwear outfit",
    "fashion model walking",
    "haute couture evening gown",
    "fashion relight dramatic",
    "beauty skin retouching portrait",
    "virtual try-on outfit swap",
    "fashion video slow motion",
    "outfit transition smooth",
]

# æ¯ä¸ª query æœ€å¤šå–å¤šå°‘æ¡ï¼ˆLexica æ¯æ¬¡è¿”å› 50 æ¡ï¼‰
LEXICA_MAX_PER_QUERY = 150  # 3 é¡µ

# HuggingFace æ•°æ®é›†
HF_DATASETS = [
    {
        "name": "Falah/fashion_photography_prompts_SDXL",
        "split": "prompts",
        "text_col": "prompts",
        "max_rows": 5000,
    },
    {
        "name": "Falah/men_fashion_prompts_SDXL",
        "split": "prompts",
        "text_col": "prompts",
        "max_rows": 3000,
    },
    {
        "name": "Falah/fashion_moodboards_prompts",
        "split": "prompts",
        "text_col": "prompts",
        "max_rows": 1000,
    },
    {
        "name": "Geonmo/deepfashion-multimodal-descriptions",
        "split": "train",
        "text_col": "caption",
        "max_rows": 5000,
    },
]

# ============================================
# èƒ½åŠ›åˆ†ç±»è§„åˆ™
# ============================================

CAPABILITY_KEYWORDS = {
    "omni_image": {
        "keywords": [
            "fashion photo", "editorial", "portrait", "magazine", "cover",
            "product photo", "lookbook", "catalog", "studio shot", "flat lay",
            "white background", "clean background",
        ],
        "label": "å›¾åƒç”Ÿæˆ",
    },
    "face_swap": {
        "keywords": [
            "face swap", "face replace", "face blend", "face transfer",
            "face merge", "identity preserv",
        ],
        "label": "AI æ¢è„¸",
    },
    "skin_enhance": {
        "keywords": [
            "skin", "retouch", "beauty", "complexion", "smooth skin",
            "porcelain", "glow", "dewy", "flawless", "blemish",
        ],
        "label": "çš®è‚¤ç¾åŒ–",
    },
    "relight": {
        "keywords": [
            "lighting", "light", "golden hour", "sunset", "neon", "studio light",
            "dramatic light", "rim light", "backlight", "chiaroscuro",
            "soft light", "window light", "ring light",
        ],
        "label": "AI æ‰“å…‰",
    },
    "outfit_swap": {
        "keywords": [
            "outfit swap", "try-on", "virtual try", "clothing swap",
            "garment", "wearing", "dressed in", "outfit change",
        ],
        "label": "æ¢è£…",
    },
    "ai_stylist": {
        "keywords": [
            "styling", "style", "coordinate", "outfit recommend",
            "fashion advice", "wardrobe", "look", "ensemble",
            "french chic", "korean", "minimalist", "streetwear",
        ],
        "label": "AI ç©¿æ­å¸ˆ",
    },
    "outfit_shot": {
        "keywords": [
            "instagram", "xiaohongshu", "social media", "content",
            "flat lay", "ootd", "street snap", "lifestyle",
            "fashion post", "influencer",
        ],
        "label": "AI ç©¿æ­å†…å®¹",
    },
    "text_to_video": {
        "keywords": [
            "video", "runway", "walking", "catwalk", "slow motion",
            "cinematic", "transition", "animation",
        ],
        "label": "æ–‡ç”Ÿè§†é¢‘",
    },
    "image_to_video": {
        "keywords": [
            "animate", "motion", "breeze", "wind blow", "hair moving",
            "fabric flow", "gentle movement",
        ],
        "label": "å›¾ç”Ÿè§†é¢‘",
    },
}


# ============================================
# Source 1: Lexica.art
# ============================================

def fetch_lexica(query: str, offset: int = 0) -> list[dict]:
    """ä» Lexica API è·å–ä¸€é¡µç»“æœ"""
    import requests

    url = "https://lexica.art/api/v1/search"
    params = {"q": query, "offset": offset}
    try:
        resp = requests.get(url, params=params, timeout=15)
        resp.raise_for_status()
        data = resp.json()
        return data.get("images", [])
    except Exception as e:
        print(f"  âš  Lexica error for '{query}' offset={offset}: {e}")
        return []


def scrape_lexica() -> list[str]:
    """æ‰¹é‡æœç´¢ Lexica"""
    import requests  # noqa: F811 â€” ç¡®ä¿ import å¯ç”¨

    prompts = []
    for q in LEXICA_QUERIES:
        print(f"ğŸ” Lexica: '{q}'")
        offset = 0
        count = 0
        while count < LEXICA_MAX_PER_QUERY:
            images = fetch_lexica(q, offset)
            if not images:
                break
            for img in images:
                p = img.get("prompt", "").strip()
                if p and len(p) > 20:
                    prompts.append(p)
                    count += 1
            offset += len(images)
            time.sleep(0.5)  # ç¤¼è²Œå»¶è¿Ÿ
        print(f"  â†’ {count} prompts")
    print(f"\nğŸ“¦ Lexica total: {len(prompts)} raw prompts")
    return prompts


# ============================================
# Source 2: HuggingFace Datasets
# ============================================

def scrape_huggingface() -> list[str]:
    """ä» HuggingFace åŠ è½½ fashion prompt æ•°æ®é›†"""
    try:
        from datasets import load_dataset
    except ImportError:
        print("âš  `datasets` not installed. Run: pip install datasets")
        print("  Skipping HuggingFace source.")
        return []

    prompts = []
    for ds_config in HF_DATASETS:
        name = ds_config["name"]
        print(f"ğŸ“¥ HuggingFace: {name}")
        try:
            ds = load_dataset(name, split=ds_config["split"], streaming=True)
            count = 0
            for row in ds:
                text = row.get(ds_config["text_col"], "")
                if isinstance(text, str) and len(text.strip()) > 20:
                    prompts.append(text.strip())
                    count += 1
                if count >= ds_config["max_rows"]:
                    break
            print(f"  â†’ {count} prompts")
        except Exception as e:
            print(f"  âš  Failed to load {name}: {e}")

    print(f"\nğŸ“¦ HuggingFace total: {len(prompts)} raw prompts")
    return prompts


# ============================================
# æ¸…æ´— & å»é‡
# ============================================

def clean_prompt(text: str) -> Optional[str]:
    """æ¸…æ´—å•æ¡ prompt"""
    text = text.strip()

    # è¿‡æ»¤å¤ªçŸ­æˆ–å¤ªé•¿
    if len(text) < 30 or len(text) > 1500:
        return None

    # è¿‡æ»¤éè‹±æ–‡ï¼ˆæˆ‘ä»¬éœ€è¦è‹±æ–‡ promptï¼‰
    ascii_ratio = sum(1 for c in text if ord(c) < 128) / max(len(text), 1)
    if ascii_ratio < 0.7:
        return None

    # è¿‡æ»¤ NSFW å…³é”®è¯
    nsfw_words = ["nsfw", "nude", "naked", "sexy", "erotic", "seductive", "lingerie"]
    lower = text.lower()
    if any(w in lower for w in nsfw_words):
        return None

    # å»é™¤å¸¸è§çš„ SD æŠ€æœ¯æ ‡ç­¾å™ªéŸ³ï¼ˆä¿ç•™æœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼‰
    # ä¾‹å¦‚ "Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 7"
    text = re.sub(r"Steps:\s*\d+.*$", "", text, flags=re.IGNORECASE).strip()
    text = re.sub(r"Negative prompt:.*$", "", text, flags=re.IGNORECASE | re.DOTALL).strip()

    # å»å°¾éƒ¨é€—å·
    text = text.rstrip(",").strip()

    if len(text) < 30:
        return None

    return text


def deduplicate(prompts: list[str]) -> list[str]:
    """åŸºäºå†…å®¹å“ˆå¸Œå»é‡ + è¿‘ä¼¼å»é‡ï¼ˆå‰ 80 å­—ç¬¦ç›¸åŒè§†ä¸ºé‡å¤ï¼‰"""
    seen_hashes = set()
    seen_prefixes = set()
    result = []

    for p in prompts:
        h = hashlib.md5(p.encode()).hexdigest()
        if h in seen_hashes:
            continue
        seen_hashes.add(h)

        # è¿‘ä¼¼å»é‡ï¼šå‰ 80 å­—ç¬¦ç›¸åŒå°±è·³è¿‡
        prefix = p[:80].lower().strip()
        if prefix in seen_prefixes:
            continue
        seen_prefixes.add(prefix)

        result.append(p)

    return result


# ============================================
# åˆ†ç±»
# ============================================

def classify_prompt(text: str) -> list[str]:
    """å°† prompt åˆ†ç±»åˆ°ä¸€ä¸ªæˆ–å¤šä¸ªèƒ½åŠ›"""
    lower = text.lower()
    matched = []

    for cap_id, config in CAPABILITY_KEYWORDS.items():
        score = sum(1 for kw in config["keywords"] if kw in lower)
        if score >= 1:
            matched.append((cap_id, score))

    # æŒ‰åŒ¹é…åº¦æ’åºï¼Œå– top 3
    matched.sort(key=lambda x: -x[1])
    caps = [m[0] for m in matched[:3]]

    # æ²¡åŒ¹é…åˆ°ä»»ä½•èƒ½åŠ› â†’ é»˜è®¤ omni_image
    if not caps:
        caps = ["omni_image"]

    return caps


# ============================================
# ä¸»æµç¨‹
# ============================================

def main():
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("ğŸš€ Fashion Prompt é‡‡é›†å¼€å§‹")
    print("=" * 60)

    # 1. é‡‡é›†
    all_prompts = []

    print("\nâ”€â”€ Source 1: Lexica.art â”€â”€")
    lexica_prompts = scrape_lexica()
    all_prompts.extend(lexica_prompts)

    print("\nâ”€â”€ Source 2: HuggingFace â”€â”€")
    hf_prompts = scrape_huggingface()
    all_prompts.extend(hf_prompts)

    print(f"\nğŸ“Š Raw total: {len(all_prompts)}")

    # 2. æ¸…æ´—
    print("\nğŸ§¹ Cleaning...")
    cleaned = [p for p in (clean_prompt(t) for t in all_prompts) if p]
    print(f"  After clean: {len(cleaned)}")

    # 3. å»é‡
    print("ğŸ”„ Deduplicating...")
    unique = deduplicate(cleaned)
    print(f"  After dedup: {len(unique)}")

    # 4. åˆ†ç±»
    print("ğŸ·ï¸ Classifying by capability...")
    categorized: dict[str, list[str]] = {cap: [] for cap in CAPABILITY_KEYWORDS}

    for p in unique:
        caps = classify_prompt(p)
        for cap in caps:
            categorized[cap].append(p)

    # 5. è¾“å‡ºç»Ÿè®¡
    print("\nğŸ“‹ Results by capability:")
    total_entries = 0
    for cap_id, config in CAPABILITY_KEYWORDS.items():
        count = len(categorized[cap_id])
        total_entries += count
        print(f"  {config['label']:12s} ({cap_id:20s}): {count:5d} prompts")

    # 6. æ„å»ºè¾“å‡º JSON
    output = {
        "meta": {
            "total_unique_prompts": len(unique),
            "total_categorized_entries": total_entries,
            "sources": ["lexica.art", "huggingface"],
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        },
        "capabilities": {},
    }

    for cap_id, config in CAPABILITY_KEYWORDS.items():
        prompts_list = categorized[cap_id]
        output["capabilities"][cap_id] = {
            "label": config["label"],
            "count": len(prompts_list),
            "prompts": prompts_list[:500],  # æ¯ä¸ªèƒ½åŠ›æœ€å¤šä¿ç•™ 500 æ¡
        }

    # 7. å†™æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… Saved to {OUTPUT_FILE}")
    print(f"   File size: {OUTPUT_FILE.stat().st_size / 1024:.1f} KB")

    # 8. é¢å¤–è¾“å‡ºï¼šå‰ç«¯å¯ç›´æ¥ä½¿ç”¨çš„ç²¾ç®€ç‰ˆï¼ˆæ¯ä¸ªèƒ½åŠ› top 20ï¼‰
    slim_file = OUTPUT_DIR / "fashion_prompts_slim.json"
    slim = {}
    for cap_id in CAPABILITY_KEYWORDS:
        # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆä¿ç•™ä¿¡æ¯é‡å¤§çš„
        sorted_prompts = sorted(categorized[cap_id], key=len, reverse=True)
        slim[cap_id] = sorted_prompts[:20]
    with open(slim_file, "w", encoding="utf-8") as f:
        json.dump(slim, f, ensure_ascii=False, indent=2)
    print(f"   Slim version (top 20 per cap): {slim_file}")


if __name__ == "__main__":
    main()
